---
title: "Coding RUV as THETA in NONMEM"
description: "Have you ever been confused why some people use the $THETA block to code their RUV in NONMEM? You are not alone!"
author:
  - name: Marian Klose
    url: https://github.com/marianklose
    orcid: 0009-0005-1706-6289
date: 10-06-2024
categories: [RUV, Error, NONMEM] 
image: preview.jpg
draft: true 
echo: true
bibliography: references.bib
execute:
  echo: true
  message: false
  warning: false
format:
  html:
    toc: true
    code-fold: true
    code-tools: true
    number-sections: true
    embed-resources: true
---

```{r}
# load packages
library(ggplot2)
library(tibble)
library(dplyr)
library(kableExtra)
```

When I've started my PhD in pharmacometrics, a colleague of mine sent me a reference model which contained a yet unknown way of specifying the residual unexplained variability (RUV) in NONMEM. So far, I was always modeling the RUV in the *classical* way (if you can call it like this) by specifying it directly in the \$SIGMA block. Until this point, I have never thought about another way of doing so. The way I was used to model it looked like this:

``` r
$ERROR
Y = F + EPS(1)

$SIGMA
0.23
```

In this additive model the initial estimate for the *variance* of the RUV is 0.23. It is quite important, that it is the variance which we are specifying withing the \$SIGMA (and also \$OMEGA) block, because units are an important part of this post. The reference model from my colleague, however, looked like this:

``` r
$THETA
0.23        ; W 

$ERROR
Y = F + W * EPS(1)

$SIGMA
1 FIX
```

The colleague called it the *Uppsala way* of coding things. At the first glance I was quite confused and left with some questions:

-   Why would we fix the RUV in the \$SIGMA block to 1?
-   Why does it make sense to specify some sort of scaling parameter W in the \$THETA block?
-   How should we interpret the output from this? What's the unit of W?

Let's try to shed some light into the darkness.

## Some theory

Let's stick to the *Uppsala way* of coding the RUV and try to understand what is going on. Most of this theory can also be found in [@proost2017]. `EPS(1)` is a random variable drawn from a normal distribution with a mean of 0 and a variance of 1. You could write:

$$EPS(1) \sim \mathcal{N}(0,1)$$

Some people also call this a standard normal distribution. To me it helps to quickly visualize the distribution by sampling from a standard normal distribution (mean = 0, variance = 1):

```{r}
# sample from standard normal distribution
x <- rnorm(100000, mean = 0, sd = 1)
std_norm <- tibble(x = x, source = "unscaled")

# plot
std_norm |> 
  ggplot(aes(x = x, fill = source)) +
  geom_density(alpha=0.2)+
  labs(title = "Standard normal distribution", x = "EPS(1)", y = "Density")+
  scale_fill_manual(
    "Source",
    values = c(
      "unscaled" = "#003049"
    )
  ) +
  theme_bw()
```

But what happens now if we multiply this random variable with some scaling parameter W? Let's find out:

```{r}
# multiply with W
W <- 0.23
x_scaled <- x * W
std_norm_scaled <- tibble(x = x_scaled, source = "scaled")

# combine both
std_norm_combined <- bind_rows(std_norm, std_norm_scaled)

# plot
std_norm_combined |> 
  ggplot(aes(x = x, fill = source)) +
  geom_density(alpha = 0.2)+
  labs(title = "Normal distributions", x = "W * EPS(1)", y = "Density")+
  scale_fill_manual(
    "Source",
    values = c(
      "unscaled" = "#003049",  # Blue color for original
      "scaled" = "#c1121f"     # Orange color for scaled
    )
  ) +
  theme_bw()
```


Let's compare the standard deviation and variance of both distributions:

```{r}
# summarize data and calculate sd and variance
std_norm_combined |> 
  group_by(source) |>   
  summarize(
    sd = sd(x),
    var = var(x)
  ) |> 
  kbl() |> kable_styling()
```


