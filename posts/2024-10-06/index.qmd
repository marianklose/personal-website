---
title: "Coding RUV as THETA in NONMEM"
description: "Have you ever been confused why some people use the $THETA block to code their RUV in NONMEM? You are not alone!"
author:
  - name: Marian Klose
    url: https://github.com/marianklose
    orcid: 0009-0005-1706-6289
date: 10-06-2024
categories: [RUV, Error, NONMEM] 
image: preview.jpg
bibliography: G:/Mitarbeiter/Klose/Literature/zotero_library/everything.bib
draft: true 
echo: true
execute:
  echo: true
  message: false
  warning: false
format:
  html:
    number-sections: false
    toc: true
    code-fold: true
    code-tools: true
    embed-resources: true
---

```{r}
# load packages
library(ggplot2)
library(tibble)
library(dplyr)
library(kableExtra)
```

When I've started my PhD in pharmacometrics, I wanted to specify a combined error model in NONMEM for one of my projects. A colleague of mine was so kind to send me a reference model so I could implement it properly. To my surprise the model code contained a novel way of specifying the residual unexplained variability (RUV) and it was not quite clear why you would do that. Fixing the \$SIGMA block, modelling the RUV via \$THETA entries, specifying a W as scaling factor and then squaring something just to take the square root again. It all seemed a bit odd to me. Let's take a closer look at this.



## Different ways to specify the RUV

### The classical way 

Until this happend, I was always modeling the RUV in the *classical* way (if you can call it like this) by specifying it directly in the \$SIGMA block. I have also never thought about another way of doing so. The way I was used to model looked like this for an additive model:

``` r
$ERROR
IPRED = F
Y = IPRED + EPS(1)

$SIGMA
0.23
```

In this additive model the initial estimate for the *variance* of the RUV is 0.23. Now for a combined proportional and additive model I would've coded it like this:


``` r
$ERROR
IPRED = F
Y = IPRED + IPRED * EPS(1) + EPS(2)

$SIGMA
0.23
0.12
```

or a bit more elegant

``` r
$ERROR
IPRED = F
Y = IPRED * (1 + EPS(1)) + EPS(2)

$SIGMA
0.23
0.12
```

All these versions share that the RUV is directly specified in the \$SIGMA block. And it is quite straightforward to understand: We have random variables drawn from a normal distribution centered around a mean of 0 with a variance specified in the \$SIGMA block. The proportional part ``` IPRED * EPS(1) ``` is dependent on the predicted value, while the additive part ``` EPS(2) ``` is independent and just gets added to the resulting concentration. 

So far so good. Now what's the alternative way?


### The Uppsala way

The model code I received from my colleague looked like this for an additive model:

``` r
$THETA
0.23        ; RUV_ADD

$ERROR
IPRED = F
W_ADD = THETA(1)
Y = IPRED + W_ADD * EPS(1)

$SIGMA
1 FIX
```


and this for an combined proportional and additive model:

``` r
$THETA
0.23        ; RUV_PROP
0.12        ; RUV_ADD

$ERROR
IPRED = F
W_PROP = THETA(1)*IPRED
W_ADD = THETA(2)
W = SQRT(W_PROP**2 + W_ADD**2)
Y = IPRED + W * EPS(1)

$SIGMA
1 FIX
```

The colleague called it the *Uppsala way* of coding things. I was left with some questions:

-   Why would we fix the RUV in the \$SIGMA block to 1? And why do we just have one entry if we have two components - proportional and additive?
-   Why are we specifying some sort of scaling parameter W?
-   How should we interpret the output from this? What's the unit for THETA(1) and THETA(2)?

Let's try to shed some light into the darkness.

## About scaling factors and distributions

Let's stick to the *Uppsala way* of coding the RUV and try to understand what is going on. To keep it simple we will first tackle the additive error model. The key is to understand what happens if we perform this scaling operation:

``` r
W_ADD * EPS(1)
```

Most of this theory can also be found in the literature [@proostCombinedProportionalAdditive2017]. 


`EPS(1)` is a random variable drawn from a normal distribution with a mean of 0 and a variance of 1 (a standard normal distribution). You could write:

$$EPS(1) \sim \mathcal{N}(0,1)$$

To me it helps to quickly visualize the distribution by sampling from a standard normal distribution (mean = 0, variance = 1):

```{r}
# sample from standard normal distribution
x <- rnorm(100000, mean = 0, sd = 1)
std_norm <- tibble(x = x, source = "unscaled")

# plot
std_norm |> 
  ggplot(aes(x = x, fill = source)) +
  geom_density(alpha=0.2)+
  labs(title = "Standard normal distribution", x = "EPS(1)", y = "Density")+
  scale_fill_manual(
    "Source",
    values = c(
      "unscaled" = "#003049"
    )
  ) +
  theme_bw()
```

But what happens now if we multiply this random variable with some scaling parameter W? Let's find out:

```{r}
# multiply with W
W <- 0.23
x_scaled <- x * W
std_norm_scaled <- tibble(x = x_scaled, source = "scaled")

# combine both
std_norm_combined <- bind_rows(std_norm, std_norm_scaled)

# plot
std_norm_combined |> 
  ggplot(aes(x = x, fill = source)) +
  geom_density(alpha = 0.2)+
  labs(title = "Normal distributions", x = "W * EPS(1)", y = "Density")+
  scale_fill_manual(
    "Source",
    values = c(
      "unscaled" = "#003049",  # Blue color for original
      "scaled" = "#c1121f"     # Orange color for scaled
    )
  ) +
  theme_bw()
```

Let's compare the standard deviation and variance of both distributions:

```{r}
# summarize data and calculate sd and variance
std_norm_combined |> 
  group_by(source) |>   
  summarize(
    sd = sd(x),
    var = var(x)
  ) |> 
  kbl() |> kable_styling()
```

For the unscaled distribution we do not have any surprises: $\sigma^2 \approx 1$ and since $1^2 = 1$ we also say $\sigma^2 \approx \sigma$. For the scaled distribution we can see that the resulting standard distribution $\sigma$ is approximately equal to our scaling factor ```W_ADD```. This means that in the model code

``` r
W_ADD * EPS(1)
```

the ```W_ADD``` parameter (specified via \$THETA) was representing a standard deviation. 

