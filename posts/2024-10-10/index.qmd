---
title: "My attempt to understand NLME estimation algorithms in NONMEM"
description: "Trying to make sense of NLME estimation in NONMEM as a non-mathematician"
author:
  - name: Marian Klose
    url: https://github.com/marianklose
    orcid: 0009-0005-1706-6289
date: 10-10-2024
categories: [Estimation, NONMEM, Laplacian, FOCE, FO] 
image: preview.png
bibliography: C:/Users/mklose/Desktop/bibliography.bib
draft: true 
echo: true
execute:
  echo: true
  message: false
  warning: false
format:
  html:
    number-sections: false
    toc: true
    code-fold: true
    code-tools: true
---


# Theory

## Statistical model

In our little example we assume to have a (simple) hierarchical nonlinear mixed-effects (NLME) model, for which we want to conduct a parameter estimation step. To my understanding the hierarchical structure is given by having variability defined on a population (=parameter) level and an individual (=observation) level, where the individual level depends on the parameter level. 

### Population (parameter) level

The population level is represented by an inter-individual variability (IIV) term, which assumes a log-normal distribution around a typical parameter value. In this simplified example we only consider IIV on clearance and do not consider any other random effects. The population (or parameter) level can be defined as follows:

$$CL = \theta_{TVCL} \cdot e^{\eta_{i}},~~~~~\eta_{i} \sim N(0, \omega^2)$$

Here, the clearance ($CL$) is modeled as a random variable following a log-normal distribution centered around the typical clearance value ($\theta_{\text{TVCL}}$) with variance $\omega^2$. The random effect $\eta_i$ itself follows a normal distribution $N(0, \omega^2)$.

### Individual (observation) level

The individual level on the other hand is defined by the observed concentrations for each subject and the predicted concentrations. The predictions are based on the structural model and dependent on the individual parameters. If I am not mistaken, this dependency is a representation of the hierarchical and nested nature of our NLME model. The individual level also incorporates residual unexplained variability (RUV), which is an important piece since it enables us to define the likelihood function in the end. The individual level can be defined by:

$$Y_{ij} = f(x_{ij}; CL_i) + \epsilon_{ij},~~~~~\epsilon_{ij} \sim N(0, \sigma^2)$$
where we can note that:

- $Y_{ij}$ is the observed concentration for the $i^{th}$ individual at the $j^{th}$ time point, which would be one row (observation) of our dataset.
- $f(x_{ij}; CL_i)$ is the predicted concentration. 
  - It contains the function $f()$, which represents e.g., the set of ODEs that describe the PK model. 
  - The function $f()$ depends on the individual clearance ($CL_i$, can be seen as a realization of the random variable $CL$) and the variable $x_{ij}$. 
  - This $x_{ij}$ contains all the information about covariates (if we would have any), dosing and sampling events for the $i^{th}$ individual at the $j^{th}$ time point.
- $\epsilon_{ij}$ is the residual unexplained variability for the $i^{th}$ individual at the $j^{th}$ time point. It typically follows a normal distribution $N(0, \sigma^2)$

In our example we have two random variables, $Y_{ij}$ and $CL_i$, with parameters $\beta := (\theta_{TVCL}, \omega^2, \sigma^2)$. In our example we just want to estimate the typical clearance $\theta_{TVCL}$ and the IIV on clearance $\omega^2$. The residual unexplained variability $\sigma^2$ is assumed to be known and fixed. Why do we do that? Just that we can plot the surface of our objective function value in 3D and better undestand how it looks like.

## Maximum Likelihood estimation

In our simple case example (with fixed $V_D$ and fixed residual unexplained variability $\sigma^2$), we have only two parameters to estimate: $\theta_{TVCL}$ and $\omega^2$. The overall goal? To infer the parameters of interest $(\hat{\theta}_{TVCL}, \hat{\omega^2})$ from our observed data $y_{1:n}$ by maximizing the log-likelihood ($ln ~L$) function:

$$(\hat{\theta}_{TVCL}, \hat{\omega^2})_{ML} = \underset{\theta_{TVCL},~ \omega^2}{\mathrm{argmax}}~ln~L\left(\theta_{TVCL}, \omega^2| y_{1:n}, CL_{i:n}\right)$$

To align more with the notation in @wangDerivationVariousNONMEM2007, we can rather deal with the $\eta_i$ values instead of the realization in the parameter space. We can write the likelihood as:

$$(\hat{\theta}_{TVCL}, \hat{\omega^2})_{ML} = \underset{\theta_{TVCL},~ \omega^2}{\mathrm{argmax}}~ln~L\left(\theta_{TVCL}, \omega^2| y_{1:n}, \eta_{i:n}\right)$$


- However, the individual $\eta_i$ values are so called unobserved latent variables. We can only directly observe the $y_i$ values, not the $\eta_i$ values. Therefore, we cannot easily compute the joint likelihood.
- Ansatz: We want to get rid of the dependence on $\eta_i$ by working with the marginal likelihood.
- The marginal likelihood is calculated by integrating out the individual parameters $\eta_i$ ("marginalizing out $\eta_i$"):

$$(\hat{\theta}_{TVCL}, \hat{\omega^2})_{ML} = \underset{\theta_{TVCL}, \omega^2}{\mathrm{argmax}}~ln~L\left(\theta_{TVCL}, \omega^2| y_{1:n}\right)$$

with

$$L\left(\theta_{TVCL}, \omega^2| y_{1:n}\right) = p(y_{1:n}; \theta_{TVCL}, \omega^2) = \prod_{i=1}^n p(y_{i}; \theta_{TVCL},~ \omega^2)$$

$$L\left(\theta_{TVCL}, \omega^2| y_{1:n}\right) = \prod_{i=1}^n \int p(y_{i}, \eta_i; \theta_{TVCL}, \omega^2) \cdot d\eta_i $$

- By integrating over all possible values of $\eta_i$ we got rid of the dependence and are now left with teh marginal likelihood.
- We can further split the marginal likelihood equation by using the chain rule of probability:

$$L\left(\theta_{TVCL}, \omega^2| y_{1:n}\right) = \prod_{i=1}^n \int p(y_{i}| \eta_i; \theta_{TVCL}, \omega^2) \cdot p(\eta_i | \theta_{TVCL}, \omega^2) \cdot d\eta_i$$
As $p(\eta_i | \theta_{TVCL}, \omega^2)$ does actually not depend on $\theta_{TVCL}$, and $p(y_{i}| \eta_i; \theta_{TVCL}, \omega^2)$ does not actually depend on $\omega^2$, we can simplify the equation to:


$$L\left(\theta_{TVCL}, \omega^2| y_{1:n}\right) = \prod_{i=1}^n \int p(y_{i}| \eta_i; \theta_{TVCL}) \cdot p(\eta_i | \omega^2) \cdot d\eta_i$$

- The integral now contains two parts: The individual level $p(y_{i}| \eta_i; \theta_{TVCL})$ and the population level $p(\eta_i | \omega^2)$. The intuition behind this can be seen as follows: For a given $\eta_i$ within the integral, the population term $p(\eta_i |\omega^2)$ tells us how likely it is to observe this $\eta_i$ value in the population. The individual term $p(y_{i}| \eta_i; \theta_{TVCL})$ tells us how likely it is to observe the $y_i$ value given that particular $\eta_i$ value. The Likelihood will be maximal when the product of both terms is maximal.
- However, solving the marginal likelihood is much harder due to the integral. The question to answer is: "How can we maximize the marginal log likelihood function?"


# Epilogue

::: {.callout-tip collapse="true" appearance="simple"}
## Session Information

```{r}
# display session info
sessionInfo()
```
:::



