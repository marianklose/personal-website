{
  "hash": "9cdb031f492a8063e8a47a6a7a874a80",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Coding RUV as THETA in NONMEM\"\ndescription: \"Have you ever been confused why some people use the $THETA block to code their RUV in NONMEM? You are not alone!\"\nauthor:\n  - name: Marian Klose\n    url: https://github.com/marianklose\n    orcid: 0009-0005-1706-6289\ndate: 10-06-2024\ncategories: [RUV, Error, NONMEM] \nimage: preview.jpg\nbibliography: G:/Mitarbeiter/Klose/Literature/zotero_library/everything.bib\ndraft: true \necho: true\nexecute:\n  echo: true\n  message: false\n  warning: false\nformat:\n  html:\n    number-sections: false\n    toc: true\n    code-fold: true\n    code-tools: true\n    embed-resources: true\n---\n\n::: {.cell}\n\n```{.r .cell-code}\n# load packages\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(kableExtra)\n```\n:::\n\n\n\nWhen I've started my PhD in pharmacometrics, I wanted to specify a combined error model in NONMEM for one of my projects. A colleague of mine was so kind to send me a reference model so I could implement it properly. To my surprise the model code contained a novel way of specifying the residual unexplained variability (RUV) and it was not quite clear why you would do that. Fixing the \\$SIGMA block, modelling the RUV via \\$THETA entries, specifying a W as scaling factor and then squaring something just to take the square root again. It all seemed a bit odd to me. Let's take a closer look at this.\n\n\n\n## Different ways to specify the RUV\n\n### The classical way \n\nUntil this happend, I was always modeling the RUV in the *classical* way (if you can call it like this) by specifying it directly in the \\$SIGMA block. I have also never thought about another way of doing so. The way I was used to model looked like this for an additive model:\n\n``` r\n$ERROR\nIPRED = F\nY = IPRED + EPS(1)\n\n$SIGMA\n0.23\n```\n\nIn this additive model the initial estimate for the *variance* of the RUV is 0.23. Now for a combined proportional and additive model I would've coded it like this:\n\n\n``` r\n$ERROR\nIPRED = F\nY = IPRED + IPRED * EPS(1) + EPS(2)\n\n$SIGMA\n0.23\n0.12\n```\n\nor a bit more elegant\n\n``` r\n$ERROR\nIPRED = F\nY = IPRED * (1 + EPS(1)) + EPS(2)\n\n$SIGMA\n0.23\n0.12\n```\n\nAll these versions share that the RUV is directly specified in the \\$SIGMA block. And it is quite straightforward to understand: We have random variables drawn from a normal distribution centered around a mean of 0 with a variance specified in the \\$SIGMA block. The proportional part ``` IPRED * EPS(1) ``` is dependent on the predicted value, while the additive part ``` EPS(2) ``` is independent and just gets added to the resulting concentration. \n\nSo far so good. Now what's the alternative way?\n\n\n### The Uppsala way\n\nThe model code I received from my colleague looked like this for an additive model:\n\n``` r\n$THETA\n0.23        ; RUV_ADD\n\n$ERROR\nIPRED = F\nW_ADD = THETA(1)\nY = IPRED + W_ADD * EPS(1)\n\n$SIGMA\n1 FIX\n```\n\n\nand this for an combined proportional and additive model:\n\n``` r\n$THETA\n0.23        ; RUV_PROP\n0.12        ; RUV_ADD\n\n$ERROR\nIPRED = F\nW_PROP = THETA(1)*IPRED\nW_ADD = THETA(2)\nW = SQRT(W_PROP**2 + W_ADD**2)\nY = IPRED + W * EPS(1)\n\n$SIGMA\n1 FIX\n```\n\nThe colleague called it the *Uppsala way* of coding things. I was left with some questions:\n\n-   Why would we fix the RUV in the \\$SIGMA block to 1? And why do we just have one entry if we have two components - proportional and additive?\n-   Why are we specifying some sort of scaling parameter W?\n-   How should we interpret the output from this? What's the unit for THETA(1) and THETA(2)?\n\nLet's try to shed some light into the darkness.\n\n## About scaling factors and distributions\n\nLet's stick to the *Uppsala way* of coding the RUV and try to understand what is going on. To keep it simple we will first tackle the additive error model. The key is to understand what happens if we perform this scaling operation:\n\n``` r\nW_ADD * EPS(1)\n```\n\nMost of this theory can also be found in the literature [@proostCombinedProportionalAdditive2017]. \n\n\n`EPS(1)` is a random variable drawn from a normal distribution with a mean of 0 and a variance of 1 (a standard normal distribution). You could write:\n\n$$EPS(1) \\sim \\mathcal{N}(0,1)$$\n\nTo me it helps to quickly visualize the distribution by sampling from a standard normal distribution (mean = 0, variance = 1):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# sample from standard normal distribution\nx <- rnorm(100000, mean = 0, sd = 1)\nstd_norm <- tibble(x = x, source = \"unscaled\")\n\n# plot\nstd_norm |> \n  ggplot(aes(x = x, fill = source)) +\n  geom_density(alpha=0.2)+\n  labs(title = \"Standard normal distribution\", x = \"EPS(1)\", y = \"Density\")+\n  scale_fill_manual(\n    \"Source\",\n    values = c(\n      \"unscaled\" = \"#003049\"\n    )\n  ) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\nBut what happens now if we multiply this random variable with some scaling parameter W? Let's find out:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# multiply with W\nW <- 0.23\nx_scaled <- x * W\nstd_norm_scaled <- tibble(x = x_scaled, source = \"scaled\")\n\n# combine both\nstd_norm_combined <- bind_rows(std_norm, std_norm_scaled)\n\n# plot\nstd_norm_combined |> \n  ggplot(aes(x = x, fill = source)) +\n  geom_density(alpha = 0.2)+\n  labs(title = \"Normal distributions\", x = \"W * EPS(1)\", y = \"Density\")+\n  scale_fill_manual(\n    \"Source\",\n    values = c(\n      \"unscaled\" = \"#003049\",  # Blue color for original\n      \"scaled\" = \"#c1121f\"     # Orange color for scaled\n    )\n  ) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\nLet's compare the standard deviation and variance of both distributions:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# summarize data and calculate sd and variance\nstd_norm_combined |> \n  group_by(source) |>   \n  summarize(\n    sd = sd(x),\n    var = var(x)\n  ) |> \n  kbl() |> kable_styling()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> source </th>\n   <th style=\"text-align:right;\"> sd </th>\n   <th style=\"text-align:right;\"> var </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> scaled </td>\n   <td style=\"text-align:right;\"> 0.2297744 </td>\n   <td style=\"text-align:right;\"> 0.0527963 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> unscaled </td>\n   <td style=\"text-align:right;\"> 0.9990192 </td>\n   <td style=\"text-align:right;\"> 0.9980394 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\nFor the unscaled distribution we do not have any surprises: $\\sigma^2 \\approx 1$ and since $1^2 = 1$ we also say $\\sigma^2 \\approx \\sigma$. For the scaled distribution we can see that the resulting standard distribution $\\sigma$ is approximately equal to our scaling factor ```W_ADD```. This means that in the model code\n\n``` r\nW_ADD * EPS(1)\n```\n\nthe ```W_ADD``` parameter (specified via \\$THETA) was representing a standard deviation. \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}