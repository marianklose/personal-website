{
  "hash": "86b4e75a4ed14f86ed15672c1c8c2c96",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Coding RUV via THETA in NONMEM\"\ndescription: \"Have you ever been confused why some people use the $THETA block to code their RUV in NONMEM? You are not alone!\"\nauthor:\n  - name: Marian Klose\n    url: https://github.com/marianklose\n    orcid: 0009-0005-1706-6289\ndate: 10-09-2024\ncategories: [RUV, Error, NONMEM] \nimage: preview.jpg\nbibliography: C:/Users/mklose/Desktop/bibliography.bib\ndraft: false \necho: true\nexecute:\n  echo: true\n  message: false\n  warning: false\nformat:\n  html:\n    number-sections: false\n    toc: true\n    code-fold: true\n    code-tools: true\n---\n\n::: {.cell}\n\n```{.r .cell-code}\n# load packages\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(kableExtra)\n```\n:::\n\n\n\n::: {.callout-tip collapse=\"true\" appearance=\"minimal\"}\n## Summary\n- **Classical RUV Specification**: The classical way of specifying RUV in NONMEM involves directly specifying a variance in the `$SIGMA` block.\n- **Alternative RUV Specification**: Coding residual unexplained variability (RUV) via the `$THETA` block in NONMEM allows direct estimation of standard deviation or coefficient of variation, rather than variance.\n- **Fixed SIGMA Block**: In the \"alternative\" RUV coding method, `$SIGMA` is fixed to 1, making `EPS()` terms standard normal, which are then scaled by `$THETA` parameters representing standard deviations or coefficients of variation.\n- **Additive vs. Proportional Models**: In additive error models, `$THETA` represents a standard deviation. In proportional error models, `$THETA` represents a coefficient of variation (CV) relative to the predicted value (`IPRED`).\n- **Combining Error Components**: For combined proportional and additive error models, the joint standard deviation (`SD`) is computed as the square root of the sum of squared standard deviations, due to variance additivity rules.\n:::\n\n\nWhen I started my PhD in pharmacometrics, I wanted to try something fancy[^1]: specifying a combined proportional and additive error model in NONMEM for one of my projects. A colleague kindly sent me a reference model, and to my confusion, the code included a novel way (at least to me) of defining residual unexplained variability (RUV):\n\n\n\n::: {.cell filename='alternative way (combined)'}\n\n```{.r .cell-code  code-fold=\"false\"}\n$THETA\n0.15        ; RUV_PROP\n0.23        ; RUV_ADD\n\n$ERROR\nIPRED = F\nSD_PROP = THETA(1)*IPRED\nSD_ADD = THETA(2)\nSD = SQRT(SD_PROP**2 + SD_ADD**2)\nY = IPRED + SD * EPS(1)\n\n$SIGMA\n1 FIX\n```\n:::\n\n\n\nIt wasn’t immediately clear why it was set up this way, and I was left with some questions:\n\n- Why is the RUV in the `$SIGMA` block fixed to 1? And why only one entry, given we have both proportional and additive components?\n- What's with the scaling parameter `SD`?\n- Why are we squaring the proportional and additive error components and then taking the square root?\n- How do we interpret the units for `THETA(1)` and `THETA(2)`?\n\nIt seemed a bit odd to me. I was more familiar with defining RUV directly in the `$SIGMA` block, something like:\n\n\n\n::: {.cell filename='classical way v1'}\n\n```{.r .cell-code  code-fold=\"false\"}\n$ERROR\nIPRED = F\nY = IPRED + IPRED * EPS(1) + EPS(2)\n\n$SIGMA\n0.0225\n0.0529\n```\n:::\n\n\n\nor maybe in a slightly more elegant form:\n\n\n\n::: {.cell filename='classical way v2'}\n\n```{.r .cell-code  code-fold=\"false\"}\nY = IPRED * (1 + EPS(1)) + EPS(2)\n```\n:::\n\n\n\nSo, why use this “alternative”[^2] way of defining the error? Before we try to explain this way of writing a combined error model to ourselves, let's break down the additive and proportional error model separately to understand what's going on. Please note: most of this content can also be found elsewhere [@proostCombinedProportionalAdditive2017].\n\n## Additive error models\n\nThe \"classical\" way (if I can call it that) of specifying an additive error model in NONMEM is as follows:\n\n\n\n::: {.cell filename='classical way (additive)'}\n\n```{.r .cell-code  code-fold=\"false\"}\n$ERROR\nIPRED = F\nY = IPRED + EPS(1)\n\n$SIGMA\n0.0529\n```\n:::\n\n\n\nIn this approach, RUV is defined directly in the `$SIGMA` block, where `EPS(1)` is assumed to be normally distributed with a mean of 0 and variance of 0.0529:\n\n$$EPS(1) \\sim \\mathcal{N}(0,0.0529)$$\n\nIt is quite important to note that we are specifying variances in `$SIGMA`. Now the *alternative* way (my colleague called it the *Uppsala way*[^3]) of coding the additive error model looks like this:\n\n\n\n::: {.cell filename='alternative way (additive)'}\n\n```{.r .cell-code  code-fold=\"false\"}\n$THETA\n0.23        ; RUV_ADD\n\n$ERROR\nIPRED = F\nSD_ADD = THETA(1)\nY = IPRED + SD_ADD * EPS(1)\n\n$SIGMA\n1 FIX\n```\n:::\n\n\n\n\nHere, `$SIGMA` is fixed so `EPS(1)` has a variance of 1, effectively making it a standard normal distribution:\n\n$$EPS(1) \\sim \\mathcal{N}(0,1)$$\n\nBut we then multiply this random variable `EPS(1)`  by a scaling factor `SD_ADD` (which is being estimated as a `THETA` parameter) before the product is being added to the individual predicted `IPRED` value: \n\n\n\n::: {.cell filename='alternative way (additive)'}\n\n```{.r .cell-code  code-fold=\"false\"}\nY = IPRED + SD_ADD * EPS(1)\n```\n:::\n\n\n\nI am not super familiar what happens if we multiply a random variable with a scaling factor. So maybe it is a good idea to visualize what happens when we fix `$SIGMA` to 1 and multiply it by `SD = 0.23`. Let's start with plotting a standard normal distribution (`$SIGMA 1 FIX`):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# sample from standard normal distribution\nx <- rnorm(100000, mean = 0, sd = 1)\nstd_norm <- tibble(x = x, source = \"unscaled\")\n\n# plot\nstd_norm |> \n  ggplot(aes(x = x, fill = source)) +\n  geom_density(alpha=0.2)+\n  labs(title = \"Standard normal distribution\", x = \"\", y = \"Density\")+\n  scale_fill_manual(\n    \"Source\",\n    values = c(\n      \"unscaled\" = \"#003049\"\n    )\n  ) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\nThe resulting standard deviation should be 1, and since $1^2 = 1$, the resulting variance should also be 1. Let's be sure and check our empirical estimates (it is a simulation, after all) to confirm this:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# summarize data and calculate sd and variance\nstd_norm |> \n  group_by(source) |>   \n  summarize(\n    sd = sd(x) |> signif(digits = 3),\n    var = var(x) |> signif(digits = 3)\n  ) |> \n  rename(\n    \"Source\" = source,\n    \"Standard Deviation\" = sd,\n    \"Variance\" = var\n  ) |>\n  kbl() |> kable_styling()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Source </th>\n   <th style=\"text-align:right;\"> Standard Deviation </th>\n   <th style=\"text-align:right;\"> Variance </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> unscaled </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\nGood. But what happens now to this standard normal distribution if we multiply the random variable with some scaling parameter $SD = 0.23$? Let's find out:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set a seed\nset.seed(123)\n\n# multiply with W\nSD <- 0.23\nx_scaled <- x * SD\nstd_norm_scaled <- tibble(x = x_scaled, source = \"scaled\")\n\n# combine both\nstd_norm_combined <- bind_rows(std_norm, std_norm_scaled)\n\n# plot\nstd_norm_combined |> \n  ggplot(aes(x = x, fill = source)) +\n  geom_density(alpha = 0.2)+\n  labs(title = \"Normal distributions: Impact of scaling factor SD\", x = \"\", y = \"Density\")+\n  scale_fill_manual(\n    \"Source\",\n    values = c(\n      \"unscaled\" = \"#003049\",  # Blue color for original\n      \"scaled\" = \"#c1121f\"     # Orange color for scaled\n    )\n  ) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\nLet's compare the standard deviation and variance of both distributions:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# summarize data and calculate sd and variance\nstd_norm_combined |> \n  group_by(source) |>   \n  summarize(\n    sd = sd(x) |> signif(digits = 2),\n    var = var(x) |> signif(digits = 2)\n  ) |> \n  rename(\n    \"Source\" = source,\n    \"Standard Deviation\" = sd,\n    \"Variance\" = var\n  ) |> \n  kbl() |> \n  kable_styling()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Source </th>\n   <th style=\"text-align:right;\"> Standard Deviation </th>\n   <th style=\"text-align:right;\"> Variance </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> scaled </td>\n   <td style=\"text-align:right;\"> 0.23 </td>\n   <td style=\"text-align:right;\"> 0.053 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> unscaled </td>\n   <td style=\"text-align:right;\"> 1.00 </td>\n   <td style=\"text-align:right;\"> 1.000 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\nFor the scaled distribution, we can see that the resulting standard deviation $\\sigma$ is approximately equal to our scaling factor `SD_ADD` (which is 0.23) and the variance is $0.23^2 \\approx 0.053$. This means that in our model code\n\n\n\n::: {.cell filename='alternative way (additive)'}\n\n```{.r .cell-code  code-fold=\"false\"}\nSD_ADD * EPS(1)\n```\n:::\n\n\n\nthe `SD_ADD` parameter (specified via `$THETA`) is representing a standard deviation. Cool thing! Probably it's not too surprising given my naming scheme, but anyways.[^4] Overall, both of these models should be equivalent:\n\n\n\n::: {.cell filename='classical way (additive)'}\n\n```{.r .cell-code  code-fold=\"false\"}\n$SIGMA\n0.0529   ; variance\n```\n:::\n\n\n\nand\n\n\n\n::: {.cell filename='alternative way (additive)'}\n\n```{.r .cell-code  code-fold=\"false\"}\n$THETA\n0.23   ; standard deviation\n\n$SIGMA\n1 FIX\n```\n:::\n\n\n\nTo sum it up: We need to be careful with the units. If we use the *classical* way, we are estimating a variance via `$SIGMA`, but if we use the *alternative* way, we are estimating a standard deviation via `$THETA` and fix the `$SIGMA` to a standard normal. Typically, we would report the standard deviation (rather than the variance) if we use an additive model, and I think one of the advantages of the *alternative* way is that we directly read out the standard deviation from the parameter estimates (without the need to transform anything). Some also say that the estimation becomes more stable if we model the stochastic parts via `$THETA`, but I cannot judge if this is true or not.\n\n::: callout-tip\n## Specifying additive RUV via \\$THETA gives us a standard deviation\n\nWhenever we have an additive error model and we specify the RUV in the `$THETA` block (the *alternative* way), the resulting estimate is a standard deviation.\n:::\n\n\n## Proportional error models\n\nNow, let’s look at proportional error models. The *classical* way of specifying the proportional error model looks like this:\n\n\n\n::: {.cell filename='classical way (proportional)'}\n\n```{.r .cell-code  code-fold=\"false\"}\n$ERROR\nIPRED = F\nY = IPRED + IPRED * EPS(1)\n\n$SIGMA\n0.0225\n```\n:::\n\n\n\nAnd the *alternative* way is:\n\n\n\n::: {.cell filename='alternative way (proportional)'}\n\n```{.r .cell-code  code-fold=\"false\"}\n$THETA\n0.15        ; RUV_PROP\n\n$ERROR\nIPRED = F\nSD_PROP = IPRED * THETA(1)\nY = IPRED + SD_PROP * EPS(1)\n\n$SIGMA\n1 FIX\n```\n:::\n\n\n\nThe structure is similar to the additive model we discussed earlier, except that the standard deviation of the random noise around our prediction depends on the prediction itself. This is why we first calculate the standard deviation `SD_PROP` at the given prediction as:\n\n\n\n::: {.cell filename='alternative way (proportional)'}\n\n```{.r .cell-code  code-fold=\"false\"}\nSD_PROP = IPRED * THETA(1)\n```\n:::\n\n\n\nThis already gives us an understanding of the units of `THETA(1)`:  it represents the coefficient of variation (CV) of the prediction `IPRED`. Why? A coefficient of variation represents the ratio of the standard deviation to the mean. This is why we end up with a standard deviation (`SD_PROP`) if we multiply the prediction (`IPRED`) with the CV (`THETA(1)`). So we always have a fraction of the prediction representing our standard deviation at that point.\n\n### An example\n\nSuppose we have a prediction (`IPRED`) of 10 mg/L and we want to show the resulting distribution. For the *classical* approach, we would specify a variance (`EPS(1)`) of 0.0225, and for the *alternative* way, we would specify a CV (`THETA(1)`) of 0.15. What do you think? Will this be equivalent or not? Let’s find out!\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set seed for reproducibility\nset.seed(123)\n\n# Parameters\nIPRED <- 10         \nCV_percent <- 0.15    \nSD_prop <- CV_percent * IPRED  \nsd_classical <- IPRED * sqrt(0.0225)  \n\n# Number of samples\nn <- 100000\n\n# Classical way: Specify variance directly\neps_classical <- rnorm(n, mean = 10, sd = sd_classical)  \n\n# Alternative way: Specify CV%\neps_alternative <- rnorm(n, mean = 10, sd = 1 * SD_prop) \n\n# Create a tibble combining both distributions\nprop_models <- tibble(\n  value = c(eps_classical, eps_alternative),\n  source = rep(c(\"Classical (Variance = 0.0225)\", \"Alternative (CV = 0.15)\"), each = n)\n)\n\n# Plot the density of both distributions\nprop_models |> \n  ggplot(aes(x = value, fill = source)) +\n  geom_density(alpha = 0.4) +\n  labs(\n    title = \"Classical vs. alternative specification\",\n    x = \"Concentration [mg/L]\",\n    y = \"Density\"\n  ) +\n  scale_fill_manual(\n    \"Model Specification\",\n    values = c(\n      \"Classical (Variance = 0.0225)\" = \"#003049\",  # Blue\n      \"Alternative (CV = 0.15)\" = \"#c1121f\"      # Red\n    )\n  ) +\n  scale_x_continuous(breaks=c(4,6,8,10,12,14,16))+\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\nBoth models end up with the same distribution. In the *classical* way, we are given a variance of 0.0225. To get the standard deviation, we take the square root of the variance:\n\n$$\n\\sigma_{EPS} = \\sqrt{0.0225} = 0.15\n$$\nThis means, that our random variable `EPS(1)` has a standard deviation of 0.15 mg/L in our *classical* model:\n\n\n\n::: {.cell filename='classical way (proportional)'}\n\n```{.r .cell-code  code-fold=\"false\"}\nY = IPRED + IPRED * EPS(1)\n```\n:::\n\n\n\n\nBy multiplying this `EPS(1)` by the prediction (`IPRED`) of 10 mg/L, we are scaling this random variable to have the (desired) standard deviation of the prediction distribution (`PRED`):\n\n$$\n\\sigma_{Y} = 0.15 \\times 10 = 1.5 \\, \\text{mg/L}\n$$\n\nIn the *alternative* way, we are directly estimating the coefficient of variation (CV) as 0.15. \n\n\n\n::: {.cell filename='alternative way (proportional)'}\n\n```{.r .cell-code  code-fold=\"false\"}\nSD_PROP = IPRED * THETA(1)\nY = IPRED + SD_PROP * EPS(1)\n```\n:::\n\n\n\nWe are first calculating the respective standard deviation (`SD_PROP`) by multiplying `CV` with `IPRED`. We then turn this standard deviation into a random variable with this standard deviation by multiplying it with a random variable from a standard normal (`EPS(1)`). Also here, the respective standard deviation of the prediction distribution (`PRED`) is 1.5 mg/L:\n\n$$\n\\sigma_{Y} = 0.15 \\times 10 = 1.5 \\, \\text{mg/L}\n$$\n\nIn both cases, the resulting variability is the same, meaning both approaches lead to the same standard deviation of 1.5 mg/L. Again, it is a bit more convenient to specify the CV directly, as it is more intuitive and easier to interpret. And if the stability argument is true (see above), we would also make our estimation more robust this way.\n\n\n::: callout-tip\n## Specifying proportional RUV in \\$THETA gives us a coefficient of variation\n\nWhenever we have a proportional error model and we specify the RUV in the `$THETA` block, the resulting estimate is a coefficient of variation.\n:::\n\n\n\n\n## Combined proportional and additive error models\n\nFinally, let's combine our knowledge to understand the *alternative* way of specifying a combined proportional and additive error model:\n\n\n\n::: {.cell filename='alternative way (combined)'}\n\n```{.r .cell-code  code-fold=\"false\"}\n$THETA\n0.15        ; RUV_PROP\n0.23        ; RUV_ADD\n\n$ERROR\nIPRED = F\nSD_PROP = THETA(1)*IPRED\nSD_ADD = THETA(2)\nSD = SQRT(SD_PROP**2 + SD_ADD**2)\nY = IPRED + SD * EPS(1)\n\n$SIGMA\n1 FIX\n```\n:::\n\n\n\nTwo parts should already be familiar:\n\n\n\n::: {.cell filename='alternative way (combined)'}\n\n```{.r .cell-code  code-fold=\"false\"}\nSD_PROP = THETA(1)*IPRED\nSD_ADD = THETA(2)\n```\n:::\n\n\n\nIn the first part, we calculate `SD_PROP`, representing the resulting standard deviation of the proportional part (as `THETA(1)` is a CV). The second part, `SD_ADD`, gives us the standard deviation of the additive part. Now we want to find the joint standard deviation `SD` at the given concentration. But how do we combine these components?\n\n\n\n::: {.cell filename='alternative way (combined)'}\n\n```{.r .cell-code  code-fold=\"false\"}\nSD = SQRT(SD_PROP**2 + SD_ADD**2)\n```\n:::\n\n\n\nWe can see that we first square both terms, then add them together, then take the square root. Sounds complicated - why not just add them directly together? This is because variances are additive when combining independent random variables, while standard deviations are not [@sochVarianceSumTwo2020]. Written a bit more formally for two independent random variables (we typically assume the covariance to be 0 when modelling RUV):\n\n\n$$\\mathrm{Var}(X + Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y)$$\nIn our case, `SD_PROP` and `SD_ADD` are standard deviations, so we must first square them to get the variances and then add them. However, we want to go back to a standard deviation before we multiply `SD` with `EPS(1)` (being fixed to 1). Therefore, we take the square root in the end.\n\nThis operation has always confused me a bit, but once I understood that I can sum up variances, but not standard deviations [^5] it made more sense to me.\n\n::: callout-tip\n## Combined error models\n\nWhen specifying a combined error model, the estimates in the $THETA block represent a standard deviation for the additive part and a coefficient of variation for the proportional part.\n:::\n\n\n\n## Conclusion\n\nThis is a somewhat lengthy explanation of why and how we code the *alternative* approach in NONMEM. Personally, I wasn't very familiar with how distributions behave when its random variable is being multiplying by a factor, and I didn’t realize that while variances are additive when combining two random processes, standard deviations are not. If you have a stronger background in statistics, this might have been obvious, but I hope this explanation was still helpful for some others.\n\n[^1]: Yeah, I know, not really fancy. But that's how it feels when you touch a combined error model for the first time. \n[^2]: For many of you, this is likely quite standard. The naming reflects my perspective. \n[^3]: I'm not sure if this was initially introduced by one of the Uppsala groups or if this is just some hearsay.\n[^4]: Some people also code it with `W` instead of `SD` but it's always a good idea to find descriptive variable names.\n[^5]: Probably something you would tackle in the first semester of your statistics studies. But not if you study pharmacy ;)\n\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}