{
  "hash": "b2e607cdcdabebc904273ed2edd0bb20",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Reproducing NONMEM's MAP estimation in R\"\ndescription: \"How to perform Bayesian MAP estimation yourself using R and why you should care.\"\nauthor:\n  - name: Marian Klose\n    url: https://github.com/marianklose\n    orcid: 0009-0005-1706-6289\ndate: 03-31-2025  # MM-DD-YYYY\ncategories: [NONMEM, Bayes, MAP, EBE, Estimation] \nbibliography: C:/Users/mklose/Desktop/GitHub/personal-website/literature/bibliography.bib\nimage: preview.gif\ndraft: false \necho: true\nexecute:\n  freeze: true # never re-render during project render (dependencies might change over time)\n  echo: true\n  message: false\n  warning: false\ncitation: \n  url: https://marian-klose.com/posts/bayes_map_estimation_in_r/index.html\nformat:\n  html:\n    number-sections: true\n    toc: true\n    code-fold: true\n    code-tools: true\n---\n\n::: {.cell}\n\n```{.r .cell-code}\n# load packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr) \nlibrary(kableExtra)\nlibrary(xpose4)\nlibrary(tidyr)\n```\n:::\n\n\n\n\n\n\n# Motivation {#sec-motivation}\n\nBayesian estimation is a widely-used method in pharmacometrics. Many pharmacometricians utilize individual predictions (`IPRED`) in their diagnostics, which represent simulations based on the *maximum a-posteriori* (MAP) estimates [^1]. These estimates essentially represent the mode of the so-called *posterior* distribution, which combines prior knowledge about the population and new evidence about the particular individual at hand (more to that later). `MAP` estimates also play a crucial role in *Model-Informed Precision Dosing* (MIPD), where one aims to predict the concentration-time profile of an individual, given a pharmacokinetic NLME model and a set of drug concentrations measured through a *Therapeutic Drug Monitoring* (TDM) program. During model building, the underlying individualized `ETAs` are often used for covariate screening, provided that shrinkage permits it. As you can see, Bayesian estimation is quite prominent in pharmacometric workflows. Thomas Bayes would be proud of us!\n\n![Portrait of Thomas Bayes.](media/Thomas_Bayes.gif){width=40% #fig-thomas-bayes}  \n\nIn many scenarios, these Bayesian calculations happen automatically *under the hood* when fitting pharmacometric models (e.g., in NONMEM), making it quite easy to use Bayesian estimates without exploring the underlying concepts and calculations. I myself did this for quite some time. Reproducing  calculations through equations is often the best way to grasp the underlying principles. Therefore, I have decided to write this blogpost, in which I want to explain the Bayesian idea and reproduce a simple Bayesian MAP estimation in R. I will mainly focus on the point estimates of the posterior distribution (`MAP` or `EBE`). Addressing the full *posterior* distribution is also important and might be covered in a future post. There are other nice blogposts which cover full Bayesian approaches in more detail, such as @navarro2023.\n\nPlease note: These are just my personal reflections and interpretations of Bayesian concepts. I cannot guarantee that my explanations, equations, or terminology are flawless. If you notice any inaccuracies, please let me know so I can correct them! \n\n\n# Structure {#sec-structure}\n\nLet me briefly outline how I structured this blogpost. The overall goal is to better understand the Bayesian concept behind MAP estimation and to derive an individualized clearance estimate for a virtual patient using R, step by step.\n\nI start with my own little introduction of the Bayesian idea (@sec-bayesian-idea) using a real-world example and define the key components—prior, likelihood, and posterior. \n\nThis is then followed by a pharmacometric example using a simulated dataset (from my previous blogpost) and a simple one-compartment i.v. bolus PK model as reference (@sec-pharmex). This section also includes our NONMEM reference solution which is later used to validate our calculations in R.\n\nNext, we try to explore the Bayesian theory a bit more, dealing with the mathematical formulation of MAP estimation and explaining how the prior and likelihood combine to form the posterior (@sec-bayesth).\n\nAfterwards, we move on to the R-based MAP reproduction (@sec-rrepro). We implement the prior and likelihood functions, combine them into an optimization routine, and estimate the individual parameter from scratch. Finally, we are able to compare the R-based result to the NONMEM output and visualize how prior, likelihood, and posterior interact.\n\nWith that being said, let's get started!\n\n# The Bayesian Idea: Updating Beliefs {#sec-bayesian-idea}\n\n## Starting with an example {#sec-bayesian-idea-example}\n\nI would say that many of us naturally think Bayesian, even if we are not aware of it or if we don't label it that way. At its core, Bayesian statistics is about updating your beliefs once new data or information becomes available. Let’s consider an example from the real-world to illustrate this idea.\n\nImagine you drive to work every day. If someone would ask you \"How long will it take you to get to work tomorrow?\", you would probably have a good idea, simply based on your past experiences. 30 minutes might be your best guess, but would you bet money that it will be exactly 30 minutes? Probably not, as you will always have some uncertainty associated with it. Perhaps something between 20 and 40 minutes would be reasonable based on your previous trips. Now, let's assume you get into your car the next morning and after 25 minutes, you have made it halfway.\n\n![A traffic jam on your way to work. Credits: Aayush Srivastava, Pexels.](media/pexels-aayushsri-1445653.jpg){width=80% #fig-traffic-jam}\n\nWould you stick to your initial belief that it will take you 30 minutes? Probably not, you would likely revise it (or try to break all speed limits). But would you simply double the 25 minutes to predict a 50-minute commute? This is also unlikely, given your experience suggesting that 40 minutes is usually the upper limit. So, maybe you would adjust your estimate to 38 minutes, assuming that the traffic will get better soon. This is Bayesian thinking in action: Updating your beliefs with new data. But so far without any equation and rather based on gut-feeling.\n\n\n\n## Terminology {#sec-bayesian-idea-terminology}\n\nNow, let’s map this real-world scenario to some Bayesian terminology. There are three key components in every Bayesian analysis:\n\n- **Prior:** Your initial belief - 30 minutes, plus some uncertainty.\n- **Likelihood:** The new data - taking 25 min to cover half the distance and suggesting 50 min for the full trip.\n- **Posterior:** Your updated estimate - now around 38 minutes, combining prior and likelihood.\n\nBayesian analysis starts with a prior, which you revise using new data (likelihood) to obtain the posterior. In pharmacometrics, we often encounter two levels of complexity when it comes to Bayes:\n\n- **MAP / EBE:** Simply focuses on the mode of the posterior distribution and ignores its uncertainty.\n- **Full Bayesian:** Considers the entire posterior distribution, not just the mode or a point estimate.\n\nMany pharmacometricians use MAP estimates due to their computational simplicity and their direct integration into standard NLME routines, while full Bayesian methods offer a more comprehensive view by integrating uncertainty even after new data became available. Full Bayesian methods are more and more used in the field, also thanks to some nice tutorials by @margossianFlexibleEfficientBayesian2022 for Stan/Torsten and by @johnstonBayesianEstimationNONMEM2024 for NONMEM itself, both from the Metrum Research Group.\n\n\n## Our goal in pharmacometrics {#sec-bayesian-idea-goal}\n\nSo, what role does Bayesian methodology play in pharmacometrics? Obviously it is not about optimizing your commute.  Instead, we typically use Bayesian estimation to individualize the model parameters for a given individual $i$, which has a set of unique observations $Y_{i}$. This concept was first introduced to the pharmacometric community by @sheiner1972, if I am not mistaken. On the one hand, `MAP` estimation happens during model building, at each iteration step [^2] and then after the actual estimation has finished, at the `POSTHOC` step. On the other hand, it is also used in MIPD settings as described above. The approach remains the same, and we will later have a look at a simple pharmacokinetic example to illustrate this.\n\nSo far we used a relatable example rather focusing on the intuition behind Bayesian thinking. Now we want to dive a bit into the mathematics behind it to be able to formally reproduce a simple Bayesian MAP estimation in R.\n\n\n\n# Pharmacometric example and reference solution {#sec-pharmex}\n\n## Data and visualization {#sec-pharmex-data}\n\nWithout data, no Bayesian parameter individualization. For our MAP estimation, we are using the same simulated data set I have defined in the previous blogpost about NLME modelling. See @klose2025 for more details about this data set. Let's start by loading the simulated concentration-time data and showing the head of the data set:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# read in simulated dataset from previous blogpost\nsim_data <- read_csv(\"./data/sim_data.csv\")\n\n# show data \nsim_data |>  \n  head() |> \n  kable() |> \n  kable_styling()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> ID </th>\n   <th style=\"text-align:right;\"> TIME </th>\n   <th style=\"text-align:right;\"> EVID </th>\n   <th style=\"text-align:right;\"> AMT </th>\n   <th style=\"text-align:right;\"> RATE </th>\n   <th style=\"text-align:right;\"> DV </th>\n   <th style=\"text-align:right;\"> MDV </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 100 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.0000 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.01 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 30.8160 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 3.00 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 24.5520 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 6.00 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 17.9250 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 12.00 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 10.1100 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 24.00 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 3.5975 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n\nWe will focus on ID 5, which has lower simulated concentrations than the average individual, making it easier to observe differences between typical and individualized profiles. Here's a plot to visualize these concentration-time profiles of ID 5 and the other IDs of the virtual population:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# show individual profiles\nsim_data |> \n  filter(EVID == 0) |> \n  mutate(flag = if_else(ID == 5, \"Reference\", \"Others\")) |>\n  ggplot(aes(x=TIME, y=DV, group=ID, color=as.factor(flag))) +\n  geom_point()+\n  geom_line()+\n  theme_bw()+\n  scale_y_continuous(limits=c(0,NA))+\n  labs(x=\"Time after last dose [h]\", y=\"Concentration [mg/L]\")+\n  scale_color_manual(\"Individual\", values=c(\"grey\", \"darkblue\"))+\n  ggtitle(\"Simulated concentration time data after i.v. bolus injection\")\n```\n\n::: {.cell-output-display}\n![Simulated concentration-time profiles for 10 individuals after i.v. bolus injection. ID 5 (blue) represents our exemplary ID.](index_files/figure-html/fig-sim-vis-1.png){#fig-sim-vis width=672}\n:::\n:::\n\n\n\n\n\nWe will later need this data set when running the NONMEM-based MAP estimation, so we have to save it to file. Prior to that, we will filter the data set to only include the data for ID 5:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define sim_data with ID == 5\nsim_data_id5 <- sim_data |> \n  filter(ID == 5)\n\n# save data for NONMEM\nsim_data_id5 |> \n  write_csv(\"./data/sim_data_ID5.csv\") \n```\n:::\n\n\n\n\nWith that being done, we are all set to define the NLME model in NONMEM in a next step!\n\n\n## NLME model structure {#sec-pharmex-nlme}\n\nFor our example and reference, we'll use a simple one-compartment IV model with first-order kinetics, previously fitted to similar data (@klose2025). Here's a sketch of the simple model structure:\n\n![Model structure of our simple 1 cmt i.v. bolus model with first order kinetics.](media/model_structure.png){width=100% #fig-mod-struct}\n\nAssuming our model is already fitted, our goal is to individualize parameters for ID 5. The model parameter estimates are based on the fitted model from the previous blogpost and are as follows:\n\n- $CL$ = 0.247 L/h\n- $V$ = 3.15 L\n- $\\omega^2_{CL}$ = 0.11\n- $\\sigma^2_{RUV\\_ADD}$ = 2.00\n- $\\sigma^2_{RUV\\_PROP}$ = 0.50\n\nIn a next step, we will translate this into a NONMEM model. Note that we added a proportional error term to the previously used model to better illustrate the differences among prior, likelihood, and posterior distributions [^3]. For the same reason, we increased the additive error to 2 mg/L. Let's store this information, along with the 100 mg intravenous bolus dose administered to the individual, in a list object for easy retrieval during subsequent calculations:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# store model parameters in list\nmod_par <- list(\n  tvcl = 0.247,\n  tvvd = 3.15,\n  omega2_CL = 0.11,\n  sigma2_add = 2,\n  sigma2_prop = 0.50, \n  dose = 100 \n)\n \n# show \nmod_par \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$tvcl\n[1] 0.247\n\n$tvvd\n[1] 3.15\n\n$omega2_CL\n[1] 0.11\n\n$sigma2_add\n[1] 2\n\n$sigma2_prop\n[1] 0.5\n\n$dose\n[1] 100\n```\n\n\n:::\n:::\n\n\n\n\nGreat! Now we can go ahead and define the NONMEM model.\n\n\n## NONMEM model {#sec-pharmex-model}\n\nWe have updated our NONMEM model slightly from the last blogpost:\n\n```{.r filename=\"1cmt_iv_map_est.mod\"}\n$PROBLEM 1cmt_iv_map_est\n\n$INPUT ID TIME EVID AMT RATE DV MDV\n\n$DATA C:\\Users\\mklose\\Desktop\\GitHub\\personal-website\\posts\\bayes_map_estimation_r\\data\\sim_data_ID5.csv IGNORE=@\n\n$SUBROUTINES ADVAN1 TRANS2\n\n$PK\n; define fixed effects parameters\nCL = THETA(1) * EXP(ETA(1))\nV = THETA(2)\n\n; scaling\nS1=V\n\n$THETA\n0.247 FIX               ; 1 TVCL\n3.15 FIX                ; 2 TVV\n\n$OMEGA \n0.11 FIX                ; 1 OM_CL\n\n$SIGMA\n0.50 FIX                ; 1 SIG_PROP\n2.00 FIX                ; 2 SIG_ADD\n\n$ERROR \n; add residual unexplained variability\nIPRED = F\nY = IPRED + IPRED * EPS(1) + EPS(2)\n\n$ESTIMATION METHOD=1 INTERACTION MAXEVAL=0 SIGDIG=3 PRINT=1 NOABORT POSTHOC\n\n$TABLE ID TIME EVID AMT RATE DV PRED IPRED MDV ETA1 CL NOAPPEND ONEHEADER NOPRINT FILE=map_estim_out\n\n```\n\n\nWe have fixed the initial parameter estimates to the final estimates from before and incorporated a combined error model (as described above). By using `MAXEVAL=0`, we avoid any population parameter estimation and the model is simply being evaluated. The `POSTHOC` option computes individual MAP estimates as this is the core of our task. We need to make sure that we include the `INTERACTION` option in the `$ESTIMATION` block, as this tells NONMEM to include the effect of `ETA` on the residual error when performing MAP estimation [^4]. This is important as our error model now depends on `IPRED`, which in turn depends on `ETA.` \n\n\nAfter running the model, we'll read the output through `map_estim_out`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load simulated data\nnm_out <- read_nm_table(\"./models/map_estim_out\")\n\n# show simulated data\nnm_out |> \n  head() |> \n  kable() |> \n  kable_styling()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> ID </th>\n   <th style=\"text-align:right;\"> TIME </th>\n   <th style=\"text-align:right;\"> EVID </th>\n   <th style=\"text-align:right;\"> AMT </th>\n   <th style=\"text-align:right;\"> RATE </th>\n   <th style=\"text-align:right;\"> DV </th>\n   <th style=\"text-align:right;\"> PRED </th>\n   <th style=\"text-align:right;\"> IPRED </th>\n   <th style=\"text-align:right;\"> MDV </th>\n   <th style=\"text-align:right;\"> ETA1 </th>\n   <th style=\"text-align:right;\"> CL </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 100 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.00000 </td>\n   <td style=\"text-align:right;\"> 31.7460 </td>\n   <td style=\"text-align:right;\"> 31.7460 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.55194 </td>\n   <td style=\"text-align:right;\"> 0.42894 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 0.01 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 31.61900 </td>\n   <td style=\"text-align:right;\"> 31.7210 </td>\n   <td style=\"text-align:right;\"> 31.7030 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.55194 </td>\n   <td style=\"text-align:right;\"> 0.42894 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 3.00 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 19.66000 </td>\n   <td style=\"text-align:right;\"> 25.0920 </td>\n   <td style=\"text-align:right;\"> 21.1000 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.55194 </td>\n   <td style=\"text-align:right;\"> 0.42894 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 6.00 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 12.00700 </td>\n   <td style=\"text-align:right;\"> 19.8320 </td>\n   <td style=\"text-align:right;\"> 14.0230 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.55194 </td>\n   <td style=\"text-align:right;\"> 0.42894 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 12.00 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 4.33360 </td>\n   <td style=\"text-align:right;\"> 12.3890 </td>\n   <td style=\"text-align:right;\"> 6.1947 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.55194 </td>\n   <td style=\"text-align:right;\"> 0.42894 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 24.00 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.78148 </td>\n   <td style=\"text-align:right;\"> 4.8349 </td>\n   <td style=\"text-align:right;\"> 1.2088 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.55194 </td>\n   <td style=\"text-align:right;\"> 0.42894 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n\nThe `ETA1` (= $\\eta_i$) value represents the individual random effect, while `CL` (= $CL_i$) is the individual clearance estimate:\n\n$$CL_i = \\theta_{TVCL} \\cdot \\exp(\\eta_i)$$  {#eq-cl-individual}\n\nCalculating this in R yields:\n\n$$CL_i = 0.247 \\cdot \\exp(0.55194) = 0.42894$$ {#eq-cl-individual-num}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# calculate individual MAP estimate\nmod_par$tvcl*exp(nm_out$ETA1 |> unique())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4289448\n```\n\n\n:::\n:::\n\n\n\n\nGreat! We obtained our reference solution for the MAP parameter individualization. We can now visually compare `DV`, `PRED`, and `IPRED`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot DV, PRED, IPRED\nnm_out |> \n  filter(TIME > 0) |> \n  pivot_longer(cols=c(PRED, IPRED, DV), names_to=\"variable\", values_to=\"value\") |>\n  ggplot(aes(x=TIME, y=value, group=variable, color=variable))+\n  geom_point()+\n  geom_line()+\n  labs(\n    y = \"Concentration [mg/L]\",\n    x = \"Time after last dose [h]\",\n    title = \"Comparison of DV, PRED and IPRED for ID 5\",\n    color = \"Source\"\n  ) +\n  theme_bw() \n```\n\n::: {.cell-output-display}\n![Comparison of observations (DV), population predictions (PRED), and individualized predictions based on the MAP estimate for CL (IPRED).](index_files/figure-html/fig-comp-dv-pred-ipred-1.png){#fig-comp-dv-pred-ipred width=672}\n:::\n:::\n\n\n\n\nNotably, our reference individual's data (`DV`) diverges from typical predictions (`PRED`), while individualized predictions (`IPRED`) fall in between. Let's dive deeper into the process to understand these differences.\n\n\n# Bayesian Theory {#sec-bayesth}\n\n## General form of Bayes' theorem {#sec-bayesth-general}\n\nAs described earlier, the primary goal of a Bayesian approach is to obtain the posterior distribution, either as the mode or as the complete distribution. Bayes' theorem is commonly presented as follows:\n\n$$P(A|B) = \\frac{P(A) \\cdot P(B|A)}{P(B)}$$ {#eq-bayes-general}\n\nHere, conditional probabilities are utilized, and $P(A|B)$ represents the probability of event $A$ given event $B$ has occurred. However, this remains quite theoretical. Let's directly translate it into the context of a pharmacokinetic NLME model.\n\n\n## Pharmacometric context {#sec-bayesth-pharmetrx}\n\nIn pharmacometrics, we aim to determine the most likely individual random effect $\\eta_i$ (which translates to the individual parameter $CL_i$), given the observed concentration data $Y_{i}$ for the individual. Therefore, we are mostly interested to find the posterior distribution of the individual random effect $\\eta_i$ given the data $Y_{i}$:\n\n$$p(\\eta_i|Y_{i}) = \\frac{p(\\eta_i) \\cdot p(Y_{i}|\\eta_i)}{p(Y_i)}$$  {#eq-bayes-specific-norm}\n\nwith \n\n- $p(\\eta_i|Y_{i})$: posterior distribution of parameter $\\eta_i$ given the individual data $Y_{i}$\n- $p(\\eta_i)$: prior distribution of parameter $\\eta_i$\n- $p(Y_{i}|\\eta_i)$: likelihood of observing data $Y_{i}$ given parameter $\\eta_i$\n- $p(Y_{i})$: marginal likelihood of data $Y_{i}$\n\nThe marginal likelihood $p(Y_{i})$ is typically neglected because it just acts as a scaling factor, does not depend on $\\eta_i$, and is computationally difficult due to a high-dimensional integral. Thus, the formula is often simplified to:\n\n$$p(\\eta_i|Y_{i}) \\propto p(\\eta_i) \\cdot p(Y_{i}|\\eta_i)$$ {#eq-bayes-specific-unnorm}\n\nRemoving the marginal likelihood gives an unnormalized posterior distribution, indicated by the proportional sign. Working with an unnormalized posterior is acceptable when our when we solely care about finding the mode of the posterior distribution (MAP estimate), as normalization is unnecessary in this scenario. But how exactly do we calculate the MAP estimate?\n\n\n## MAP estimation {#sec-bayesth-map}\n\nTo find the most likely parameter for an individual, we must identify the maximum of the posterior distribution (MAP estimate). Mathematically, this involves finding the parameter $\\eta_i^*$ that maximizes the posterior:\n\n$$\\eta_i^* = \\underset{\\eta_i}{\\mathrm{argmax}}~ p(\\eta_i|Y_{i}) = \\underset{\\eta_i}{\\mathrm{argmax}}~ p(\\eta_i) \\cdot \\prod_{j=1}^{m} p(Y_{ij}|\\eta_i)$$ {#eq-map-norm}\n\nDealing with products of small probability densities can be cumbersome (and mathematically instable), so taking the logarithm simplifies the calculation:\n\n$$\\eta_i^* = \\underset{\\eta_i}{\\mathrm{argmax}}~ \\log(p(\\eta_i|Y_{i})) = \\underset{\\eta_i}{\\mathrm{argmax}}~ \\log(p(\\eta_i)) + \\sum_{j=1}^{m} \\log(p(Y_{ij}|\\eta_i))$$ {#eq-map-log}\n\nIn the end, we would have to use a numerical optimizer function to explore the parameter space of $\\eta_i$ and locate the maximum of the posterior distribution ($\\eta_i^*$). To fully express the equation, we must define both the log prior $\\log(p(\\eta_i))$ and log likelihood $\\log(p(Y_{ij}|\\eta_i))$ terms. Let's explore these in detail.\n\n### Prior term {#sec-bayesth-map-prior}\n\nThe prior distribution $p(\\eta_i)$ captures our inital beliefs and uncertainties regarding $\\eta_i$ before observing the data. Bayesian methods allow to  incorporate prior knowledge through various distributions and parameters (which is why Bayesians are sometimes being criticized of being too subjective). In pharmacometrics, a common approach is using a prior based on a previously estimated model, typically a normal distribution with mean 0 and variance $\\omega^2_{CL}$ estimated from the NLME model. In contrast to other disciplines, the choice of our priors is generally less controversial, although there has been recent discussion regarding whether this one-approach-fits-all is always appropriate or if priors should be adjusted (e.g., flattened compared to the NLME estimate) in certain situations. See @hughes2021 for more information. The general \"role\" of the prior term is that strong deviations from the \"typical\" individual should only be considered when it allows us do substantially better explain the observed data. I like to see it as some kind of penalty function which prevents us from doing a potentially flawed curve fitting exercise. The probability density function (PDF) of a normal distribution can be used to calculate its contribution:\n \n\n$$p(\\eta_i) = \\frac{1}{\\sqrt{2\\pi\\omega^2}} \\cdot \\exp\\left(-\\frac{(\\eta_i-\\mu)^2}{2\\omega^2}\\right)$$  {#eq-prior-1}\n\nSince $\\mu$ is typically 0, this simplifies to:\n\n$$p(\\eta_i) = \\frac{1}{\\sqrt{2\\pi\\omega^2}} \\cdot \\exp\\left(-\\frac{\\eta_i^2}{2\\omega^2}\\right)$$ {#eq-prior-2}\n\nTaking the log yields:\n\n$$\\log(p(\\eta_i)) = -0.5 \\log(2\\pi\\omega^2) - \\frac{\\eta_i^2}{2\\omega^2}$$ {#eq-prior-3}\n\nNow, we can compute the prior term for any given individual $\\eta_i$, using variance $\\omega^2_{CL}$ (e.g., 0.11) from our NLME model. If we have a good reason to believe that our prior should be different (e.g., we apply our model to another population), we could adjust the variance accordingly (as described in @hughes2021).\n\n### Likelihood term {#sec-bayesth-map-likelihood}\n\nThe likelihood term $p(Y_{ij}|\\eta_i)$ is the probability of observing the data point $Y_{ij}$ given the parameter $\\eta_i$. As we usually assume the residuals of our model predictions to be normally distributed, we again use the normal distribution PDF:\n\n$$p(Y_{ij}|\\eta_i) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\cdot \\exp\\left(-\\frac{(Y_{ij}-f(x_{ij}; \\eta_i))^2}{2\\sigma^2}\\right)$$ {#eq-likelihood-1}\n\nwhere\n\n- $Y_{ij}$: observed data point for individual $i$ at time $j$\n- $f(x_{ij}; \\eta_i)$: model prediction for individual $i$ at time $j$, given parameter $\\eta_i$\n- $\\sigma^2$: variance representing residual unexplained variability (RUV)\n\nTaking the log yields:\n\n$$\\log(p(Y_{ij}|\\eta_i)) = -0.5 \\log(2\\pi\\sigma^2) - \\frac{(Y_{ij}-f(x_{ij}; \\eta_i))^2}{2\\sigma^2}$$ {#eq-likelihood-2}\n\nPlease note that in our case (a combined additive and proportional RUV model), $\\sigma^2$ represents the combined variances at time $t_{ij}$ and prediction $f(x_{ij})$. As the structural solution of our model, $f(x_{ij})$, is a constant for a given $t_{ij}$, we can compute the combined variance as \n\n$$\\sigma^2 = \\sigma^2_{prop} \\cdot f(x_{ij})^2 + \\sigma^2_{add}$$ {#eq-sigma2}\n\nTherefore, we later have to calculate the $\\sigma^2$ for each data point $Y_{ij}$ individually.\n\n## Combining both terms {#sec-bayesth-map-final}\n\nWriting out the log prior (@eq-prior-3) and log likelihood (@eq-likelihood-2) terms in our MAP estimation objective function (@eq-map-log), we obtain:\n\n$$\\eta_i^* = \\underset{\\eta_i}{\\mathrm{argmax}}~\\left(\\left[-0.5 \\log(2\\pi\\omega^2) - \\frac{\\eta_i^2}{2\\omega^2}\\right] ~ + \\sum_{j=1}^m \\left[-0.5 \\log(2\\pi\\sigma^2) - \\frac{(Y_{ij}-f(x_{ij}; \\eta_i))^2}{2\\sigma^2}\\right]\\right)$$ {#eq-map-final}\n\nThis final equation will guide our numerical optimizer (`optim` function in R) to determine the maximum, providing us with the individual MAP estimate. Let's implement this method in R and reproduce our NONMEM reference solution!  \n\n\n# R-based MAP reproduction {#sec-rrepro}\n\n## Prior term {#sec-rrepro-prior}\n\nWe start by defining a function to calculate the prior term given by @eq-prior-3. Here, we simply pass the individual $\\eta_i$ and the model parameters (including $\\omega^2_{CL}$) as input arguments, and the function returns the log probability of the given $\\eta_i$ under the prior distribution:\n\n\n\n\n::: {.cell filename='function: prior_fun()'}\n\n```{.r .cell-code  code-fold=\"false\"}\n# define prior term function\nprior_fun <- function(eta_i, mod_par){\n  \n  # retrieve omega2 from model parameters\n  omega2 <- mod_par$omega2_CL\n  \n  # calculate probability\n  log_prob <- - 0.5 * log(2*pi*omega2) - eta_i^2/(2*omega2)\n  \n  # return log probability\n  return(log_prob)\n}\n```\n:::\n\n\n\n\nWe can test this function by illustrating the prior term across a range of $\\eta_i$ values:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define range\neta_i <- seq(-2, 2, 0.01)\n\n# calculate prior term\nprior <- prior_fun(eta_i = eta_i, mod_par = mod_par)\n\n# create tibble\nprior_tibble <- tibble(\n  eta_i = eta_i,\n  p = prior,\n  source = \"prior\"\n)\n\n# plot prior term\nprior_tibble |> \n  ggplot(aes(x=eta_i, y=exp(p)))+\n  geom_line(color = \"darkblue\", linewidth = 0.8)+\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"darkblue\", linewidth = 0.6)+\n  labs(\n    x = expression(eta[i]),\n    y = \"Probability density\",\n    title = \"Prior term (random effect)\"\n  )+\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![Probability density of different random effects given a normal distribution centered around 0 with a variance of 0.11.](index_files/figure-html/fig-prior-term-1.png){#fig-prior-term width=672}\n:::\n:::\n\n\n\n\n\nThe following plot shows how these $\\eta_i$ values map onto the clearance domain:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot prior term\nprior_tibble |> \n  ggplot(aes(x=mod_par$tvcl*exp(eta_i), y=exp(p)))+\n  geom_line(color = \"darkblue\", linewidth = 0.8)+\n  geom_vline(xintercept = mod_par$tvcl, linetype = \"dashed\", color = \"darkblue\", linewidth = 0.6)+\n  labs(\n    x = expression(CL[i]),\n    y = \"Probability density\",\n    title = \"Prior term (clearance)\"\n  )+\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![Probability density of different clearance values.](index_files/figure-html/fig-prior-term-cl-1.png){#fig-prior-term-cl width=672}\n:::\n:::\n\n\n\n\n \nFrom the plots, we see that certain values of $\\eta_i$ and $CL_i$ are more likely than others, based on our prior beliefs derived from the NLME model estimates, before observing any individual data. Let's continue with the likelihood term:\n\n\n## Likelihood term {#sec-rrepro-likelihood}\n\nTo calculate the log likelihood term described in @eq-likelihood-2, we first have to define a model prediction function. Given our simple one-compartment model, we use a closed-form expression, though the principles apply equally to ODE-based models. More details on the model prediction function can be found in the previous blogpost, see @klose2025.\n\n\n\n\n\n::: {.cell filename='function: model_fun()'}\n\n```{.r .cell-code  code-fold=\"false\"}\n# define model function\nmodel_fun <- function(eta_i, dose, vd, theta_tvcl, t) {\n  exp_eta_i <- exp(eta_i)\n  exponent <- -1 * (theta_tvcl * exp_eta_i / vd) * t\n  result <- (dose / vd) * exp(exponent)\n  return(result)\n}\n```\n:::\n\n\n\n\nNext, we construct the likelihood function for individual observations. Please note that we have to calculate the $\\sigma^2$ for each data point $Y_{ij}$ individually, as the variance contains an element proportional to the model prediction $f_i$ (see @eq-likelihood-2). The following equation calculates the log likelihood for a single observation:\n\n\n\n\n::: {.cell filename='function: log_likelihood_fun_single()'}\n\n```{.r .cell-code  code-fold=\"false\"}\n# define likelihood function for a single observation\nlog_likelihood_fun_single <- function(eta_i, dose, vd, theta_tvcl, t_ij, Y_ij, sigma2_add, sigma2_prop){\n  \n  # get model predictions\n  f_ij <- model_fun(eta_i, dose, vd, theta_tvcl, t_ij)\n  \n  # calculate resulting sigma2 for each timepoint (prop variance is scaled based on f_i^2)\n  sigma2 <- sigma2_prop * f_ij^2 + sigma2_add\n  \n  # calculate probability\n  log_lik <- - 0.5 * log(2*pi*sigma2) - (Y_ij - f_ij)^2/(2*sigma2)\n  \n  # return log likelihood\n  return(log_lik)\n}\n```\n:::\n\n\n\n\nTo illustrate this, we calculate the likelihood for each observation, then sum these to obtain the total likelihood for a given $\\eta_i$. We demonstrate this using an example with $\\eta_i = 0$:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# calculate lok lik per row\nsim_data_id5 <- sim_data_id5 |> \n  rowwise() |> \n  mutate(\n    log_lik = case_when(\n      EVID == 0 ~ log_likelihood_fun_single(\n        eta_i = 0, \n        dose = mod_par$dose, \n        vd = mod_par$tvvd, \n        theta_tvcl = mod_par$tvcl, \n        t_ij = TIME, \n        Y_ij = DV, \n        sigma2_add = mod_par$sigma2_add, \n        sigma2_prop = mod_par$sigma2_prop\n      ),\n      EVID == 1 ~ NA_real_\n    )\n  ) |> \n  ungroup() |> \n  mutate(\n    sum_log_lik = sum(log_lik, na.rm = TRUE)\n  )\n\n# show sim_data_id5\nsim_data_id5 |>\n  kable() |> \n  kable_styling()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> ID </th>\n   <th style=\"text-align:right;\"> TIME </th>\n   <th style=\"text-align:right;\"> EVID </th>\n   <th style=\"text-align:right;\"> AMT </th>\n   <th style=\"text-align:right;\"> RATE </th>\n   <th style=\"text-align:right;\"> DV </th>\n   <th style=\"text-align:right;\"> MDV </th>\n   <th style=\"text-align:right;\"> log_lik </th>\n   <th style=\"text-align:right;\"> sum_log_lik </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 100 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.00000 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> -17.93624 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 0.01 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 31.61900 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> -4.031343 </td>\n   <td style=\"text-align:right;\"> -17.93624 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 3.00 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 19.66000 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> -3.844624 </td>\n   <td style=\"text-align:right;\"> -17.93624 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 6.00 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 12.00700 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> -3.718827 </td>\n   <td style=\"text-align:right;\"> -17.93624 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 12.00 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 4.33360 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> -3.514076 </td>\n   <td style=\"text-align:right;\"> -17.93624 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 24.00 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.78148 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> -2.827369 </td>\n   <td style=\"text-align:right;\"> -17.93624 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\nWe observe individual likelihood contributions and their sum. To streamline this process, we define a function that computes the summed likelihood across multiple observations:\n\n\n\n\n::: {.cell filename='function: log_likelihood_fun_multiple()'}\n\n```{.r .cell-code  code-fold=\"false\"}\n# define likelihood function for a set of observations\nlog_likelihood_fun_multiple <- function(df, eta_i, mod_par){\n  \n  # retrieve information from mod_par\n  dose <- mod_par$dose\n  vd <- mod_par$tvvd\n  theta_tvcl <- mod_par$tvcl\n  sigma2_add <- mod_par$sigma2_add\n  sigma2_prop <- mod_par$sigma2_prop\n  \n  # calculate log lik per row\n  log_lik_sum <- df |>\n    rowwise() |> \n    mutate(\n      log_lik = case_when(\n        EVID == 0 ~ log_likelihood_fun_single(\n          eta_i = eta_i, \n          dose = dose, \n          vd = vd, \n          theta_tvcl = theta_tvcl, \n          t_ij = TIME, \n          Y_ij = DV, \n          sigma2_add = sigma2_add, \n          sigma2_prop = sigma2_prop\n        ),\n        EVID == 1 ~ NA_real_\n      )\n    ) |> \n    ungroup() |> \n    pull(log_lik) |>\n    sum(na.rm = TRUE)\n  \n  # return log likelihood sum\n  return(log_lik_sum)\n}\n```\n:::\n\n\n\n\nTesting this function quickly confirms consistency of the obtained summed up likelihood with the previous result from the table (see above):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# calculate likelihood\nlog_likelihood_fun_multiple(\n  df = sim_data_id5,\n  eta_i = 0,\n  mod_par = mod_par\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -17.93624\n```\n\n\n:::\n:::\n\n\n\n\nNow, we can visualize the likelihood term across a range of $\\eta_i$ values, similar to what we have did for the prior term:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define range\neta_i <- seq(-2, 2, 0.01)\n\n# empty list\nll_list <- list()\n\n# loop over each element of eta_i\nfor(cur_eta_i in eta_i){\n  # calculate likelihood term\n  ll_list[[as.character(cur_eta_i)]] <- log_likelihood_fun_multiple(\n    df = sim_data_id5,\n    eta_i = cur_eta_i,\n    mod_par = mod_par\n  )\n}\n\n# convert to vector\nll_vector <- ll_list |> unlist() |> unname()\n\n# create tibble\nlikelihood_tibble <- tibble(\n  eta_i = eta_i,\n  p = ll_vector,\n  source = \"likelihood\"\n)\n\n# plot likelihood term\nlikelihood_tibble |> \n  ggplot(aes(x=eta_i, y=exp(p)))+\n  geom_line(color = \"darkred\", linewidth = 0.8)+\n  geom_vline(xintercept = eta_i[which.max(ll_vector)], linetype = \"dashed\", color = \"darkred\", linewidth = 0.8)+\n  labs(\n    x = expression(eta[i]),\n    y = \"Likelihood\",\n    title = \"Likelihood term\"\n  )+\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![Likelihood of different random effects given the observed datapoints of our exemplary individual.](index_files/figure-html/fig-likelihood-term-1.png){#fig-likelihood-term width=672}\n:::\n:::\n\n\n\n\nThe likelihood plot clearly indicates which $\\eta_i$ values best describe the observed data. Identifying the maximum likelihood $\\eta_i$ is straightforward:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get eta_i with highest likelihood\neta_i_max_likelihood <- eta_i[which.max(ll_vector)]\n\n# show\neta_i_max_likelihood\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.94\n```\n\n\n:::\n:::\n\n\n\n\n\nThe optimal $\\eta_i$ value according to the likelihood term (and ignoring any prior information) is 0.94. Please note: As the likelihood is the product of many probabilities, each between 0 and 1, we typically deal with extremely small numerical values. Therefore, it is common practice to use logarithmic transformations for numerical stability. Next, we combine the prior and likelihood terms to perform MAP estimation.\n\n## MAP estimation (posterior) {#map-rrepro-mapestim}\n\nWe now have to define an objective function for numerical optimization, combining prior and likelihood terms to estimate the posterior (as shown in @eq-map-final).\n\n\n\n\n\n::: {.cell filename='function: map_obj_fun()'}\n\n```{.r .cell-code  code-fold=\"false\"}\n# define MAP estimation objective function\nmap_obj_fun <- function(eta_i, df, mod_par){\n  \n  # calculate log prior term\n  log_prior <- prior_fun(\n    eta_i = eta_i, \n    mod_par = mod_par\n  )\n  \n  # calculate log likelihood term\n  log_likelihood <- log_likelihood_fun_multiple(\n    df = df,\n    eta_i = eta_i,\n    mod_par = mod_par\n  )\n  \n  # combine both\n  log_posterior <- log_prior + log_likelihood\n  \n  # return negative log posterior\n  return(-log_posterior)\n}\n```\n:::\n\n\n\n\nPlease note that we are returning the negative log posterior as it is easier to minimize a function than to maximize it. Using R's `optim` function, we estimate the MAP numerically. Here is the output form the `optim` call:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# run optimization\nmap_est <- optim(\n  par = 0, \n  fn = map_obj_fun, \n  df = sim_data_id5,\n  mod_par = mod_par,\n  method = \"BFGS\"\n)\n\n# show estimation results\nmap_est \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$par\n[1] 0.5519271\n\n$value\n[1] 16.08691\n\n$counts\nfunction gradient \n      13        7 \n\n$convergence\n[1] 0\n\n$message\nNULL\n```\n\n\n:::\n:::\n\n\n\n\nThe resulting MAP estimate for $\\eta_i$ is 0.5519271, closely matching the reference solution (0.55194) with an acceptable difference (-0.0000128887). Similarly to the other terms, we can visualize the posterior term across a range of $\\eta_i$ values:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define range\neta_i <- seq(-2, 2, 0.01)\n\n# empty list\npost_list <- list()\n\n# loop over each element of eta_i\nfor(cur_eta_i in eta_i){\n  # calculate likelihood term\n  post_list[[as.character(cur_eta_i)]] <- map_obj_fun(\n    eta_i = cur_eta_i,\n    df = sim_data_id5,\n    mod_par = mod_par\n  )\n}\n\n# convert to vector\npost_vector <- post_list |> unlist() |> unname()\n\n# create tibble\nposterior_tibble <- tibble(\n  eta_i = eta_i,\n  p = -post_vector,\n  source = \"posterior\"\n)\n\n# plot likelihood term\nposterior_tibble |> \n  ggplot(aes(x=eta_i, y=exp(p)))+\n  geom_line(color = \"darkgreen\", linewidth = 0.8)+\n  geom_vline(xintercept = map_est$par, linetype = \"dashed\", color = \"darkgreen\", linewidth = 0.8)+\n  labs(\n    x = expression(eta[i]),\n    y = \"Unnormalized posterior density\",\n    title = \"Posterior distribution\"\n  )+\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![The unnormalized posterior density.](index_files/figure-html/fig-posterior-term-1.png){#fig-posterior-term width=672}\n:::\n:::\n\n\n\n\n## Comparison of prior, likelihood, and posterior {#sec-map-rrepro-comp} \n\nFinally, we compare the MAP estimate (mode of the posterior) with prior and likelihood terms visually:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# combine likelihood and prior\ncomp_tibble <- rbind(\n  prior_tibble,\n  likelihood_tibble,\n  posterior_tibble\n) |> \n  mutate(source = factor(source, levels = c(\"prior\", \"posterior\", \"likelihood\")))\n\n# define linewidth\nlinewidth <- 0.8\n\n# plot both distributions with different colors and fill\ncomp_tibble |>  \n  ggplot(aes(x=eta_i, y=exp(p), fill = source, color = source))+\n  scale_x_continuous(breaks = c(-2,-1,0, signif(map_est$par,2), signif(eta_i_max_likelihood,2), 2))+\n  geom_line(linewidth = linewidth)+\n  geom_vline(\n    data = data.frame(source = factor(\"prior\", levels = c(\"prior\", \"posterior\", \"likelihood\"))),\n    aes(xintercept = 0),\n    color = \"darkblue\", linetype = \"dashed\", linewidth = linewidth\n  ) +\n  geom_vline(\n    data = data.frame(source = factor(\"posterior\", levels = c(\"prior\", \"posterior\", \"likelihood\"))),\n    aes(xintercept = map_est$par),\n    color = \"darkgreen\", linetype = \"dashed\", linewidth = linewidth\n  ) +\n  geom_vline(\n    data = data.frame(source = factor(\"likelihood\", levels = c(\"prior\", \"posterior\", \"likelihood\"))),\n    aes(xintercept = eta_i_max_likelihood),\n    color = \"darkred\", linetype = \"dashed\", linewidth = linewidth\n  ) +\n  scale_color_manual(values = c(prior = \"darkblue\", \n                                posterior = \"darkgreen\", \n                                likelihood = \"darkred\")) +\n  facet_wrap(~source, scales = \"free\", ncol = 1)+\n  labs(\n    x = expression(eta[i]),\n    y = \"(Unnormalized) probability density\",\n    title = \"Comparison of prior, likelihood, and posterior\",\n    color = \"Source\"\n  ) +\n  theme_bw() +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![Comparison of prior, likelihood, and posterior.](index_files/figure-html/fig-comparison-prior-ll-post-term-1.png){#fig-comparison-prior-ll-post-term width=672}\n:::\n:::\n\n\n\n\n\nThis clearly demonstrates how the posterior is influenced by both the prior and the likelihood, with the MAP estimate (mode of the posterior, green dashed line) reflecting a balance between these two sources of information.\n\n\n# Conclusion {#map-rrepro-conclusion}\n\nThis simple example illustrates how to estimate the individual parameter $\\eta_i$ (which translates to an individual clearance $CL_i$) for a single subject using a Bayesian approach. We have seen that the posterior distribution, often summarized by its mode (the MAP or EBE estimate), represents a compromise between the prior and the likelihood. This powerful technique enables the continuous integration of prior knowledge with new data and is very useful for many pharmacometric workflows. With that, I conclude this blog post and hope you enjoyed it. If you have any questions or feedback, please feel free to leave a comment.\n\n\n[^1]: *MAP* estimates are also known as *Empirical Bayes Estimates* (EBE).\n[^2]: Depending on the estimation algorithm.\n[^3]: The dense sampling in combination with a simple additive error model for RUV led to a scenario where the posterior was nearly equal to the likelihood. Therefore, I have increased the additive error and introduced a proportional term as well. This places the posterior in the middle and I can recycle the old dataset I have simulated before. \n[^4]: Actually, this took me quite some time to figure out. I was using the model from the previous blogpost, which only had an additive error and was missing the `INTERACTION` option (which doesn't matter if you are just using a constant additive error). With the newly introduced proportional error, I then had a mismatch between my calculations and the reference and I couldn't figure out why. The missing `INTERACTION` was the solution, as with the introduction of the proportional error the RUV does not remain constant (which NONMEM is assuming when `INTERACTION` is missing). Lesson learned!\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}