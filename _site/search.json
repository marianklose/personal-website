[
  {
    "objectID": "publications/publications.html",
    "href": "publications/publications.html",
    "title": "Publications",
    "section": "",
    "text": "Publication\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbstract/Poster"
  },
  {
    "objectID": "publications/publications.html#section",
    "href": "publications/publications.html#section",
    "title": "Publications",
    "section": "2024",
    "text": "2024\n\n\n\n\n\n\nA model-informed clinical trial simulation tool with a graphical user interface for Duchenne muscular dystrophy\n\n\n\n\n\nJournal: CPT: Pharmacometrics & Systems Pharmacology\nPublication Date: 2024-10-03\nDOI: 10.1002/psp4.13246\nAuthors: Kim J, Morales JF, Kang S, Klose M, Willcocks RJ, Daniels MJ, Belfiore-Oshan R, Walter GA, Rooney WD, Vandenborne K, Kim S.\n\n\n\n\n\n\n\n\n\nType 1 diabetes prevention clinical trial simulator: Case reports of model-informed drug development tool\n\n\n\n\n\nJournal: CPT: Pharmacometrics & Systems Pharmacology\nPublication Date: 2024-07-03\nDOI: 10.1002/psp4.13193\nAuthors: Morales JF, Klose M, Hoffert Y, Podichetty JT, Burton J, Schmidt S, Romero K, O’Doherty I, Martin F, Campbell-Thompson M, Haller MJ, Atkinson MA, Kim S.\n\n\n\n\n\n\n\n\n\nPopulation pharmacokinetics of TLD-1, a novel liposomal doxorubicin, in a phase I trial\n\n\n\n\n\nJournal: Cancer Chemother Pharmacol\nPublication Date: 2024-09\nDOI: 10.1007/s00280-024-04679-z\nAuthors: Mc Laughlin AM, Hess D, Michelet R, Colombo I, Haefliger S, Bastian S, Rabaglio M, Schwitter M, Fischer S, Eckhardt K, Hayoz S, Kopp C, Klose M, Sessa C, Stathis A, Halbherr S, Huisinga W, Joerger M, Kloft C.\n\n\n\n\n\n\n\n\n\nMachine learning-driven flattening of model priors: A comparative simulation study across multiple compounds\n\n\n\n\n\nEvent: PAGE (2024) Publication Date: 2024-06-26\nLink: https://www.page-meeting.org/default.asp?abstract=10858\nAuthors: Klose M, Thoma F, Kovar L, Huisinga W, Michelet R, Kloft C.\n\n\n\n\n\n\n\n\n\nTLD-1, a novel liposomal doxorubicin, in patients with advanced solid tumors: Dose escalation and expansion part of a multicenter open-label phase I trial (SAKK 65/16)\n\n\n\n\n\nJournal: European Journal of Cancer\nPublication Date: 2024-04\nDOI: 10.1016/j.ejca.2024.113588\nAuthors: Colombo I, Koster KL, Holer L, Haefliger S, Rabaglio M, Bastian S, Schwitter M, Eckhardt K, Hayoz S, Mc Laughlin AM, Kloft C, Klose M, Halbherr S, Baumgartner C, Sessa C, Stathis A, Hess D, Joerger M.\n\n\n\n\n\n\n\n\n\nExploring the impact of CYP2D6 and UGT2B7 gene-drug interactions, and CYP-mediated DDI on oxycodone and oxymorphone pharmacokinetics using physiologically-based pharmacokinetic modeling and simulation\n\n\n\n\n\nJournal: Eur J Pharm Sci\nPublication Date: 2024-03-01\nDOI: 10.1016/j.ejps.2023.106689\nAuthors: Klose M, Cristofoletti R, Silva CM, Mangal N, Turgeon J, Michaud V, Lesko LJ, Schmidt S."
  },
  {
    "objectID": "publications/publications.html#section-1",
    "href": "publications/publications.html#section-1",
    "title": "Publications",
    "section": "2022",
    "text": "2022\n\n\n\n\n\n\nThe Impact of UGT2B7 and CYP2D6 Gene-Drug- and CYP-mediated Drug-Drug-Interactions on Oxycodone and Oxymorphone Pharmacokinetics using PBPK Modelling.\n\n\n\n\n\nEvent: ACoP13 (2022)\nPublication Date: 2022\nLink: www.go-acop.org/?abstract=413\nAuthors: Klose M, Schmidt S, Cristofoletti R."
  },
  {
    "objectID": "publications/publications.html#section-2",
    "href": "publications/publications.html#section-2",
    "title": "Publications",
    "section": "2021",
    "text": "2021\n\n\n\n\n\n\nEvaluation of the MeroRisk Calculator, A User-Friendly Tool to Predict the Risk of Meropenem Target Non-Attainment in Critically Ill Patients\n\n\n\n\n\nJournal: Antibiotics (Basel)\nPublication Date: 2021-04-20\nDOI: 10.3390/antibiotics10040468\nAuthors: Liebchen U, Klose M, Paal M, Vogeser M, Zoller M, Schroeder I, Schmitt L, Huisinga W, Michelet R, Zander J, Scharf C, Weinelt FA, Kloft C.\n\n\n\n\n\n\n\n\n\nUsing microdose-based activity measurement to individualise dosing of cytochrome P450 metabolised drugs: a case study with yohimbine and tamoxifen.\n\n\n\n\n\nEvent: PAGE (2021) Publication Date: 2021\nLink: https://www.page-meeting.org/default.asp?abstract=9807\nAuthors: Michelet R, Weinelt FA, Klose M, Mc Laughlin AM, Kluwe F, Montefusco-Pereira C, Van Dyk M, Vay M, Huisinga W, Kloft C, Mikus G."
  },
  {
    "objectID": "posts/2024-10-09/index.html",
    "href": "posts/2024-10-09/index.html",
    "title": "Coding RUV via THETA in NONMEM",
    "section": "",
    "text": "Code\n# load packages\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(kableExtra)\nWhen I started my PhD in pharmacometrics, I wanted to try something fancy1: specifying a combined proportional and additive error model in NONMEM for one of my projects. A colleague kindly sent me a reference model, and to my confusion, the code included a novel way (at least to me) of defining residual unexplained variability (RUV):\nalternative way (combined)\n\n$THETA\n0.15        ; RUV_PROP\n0.23        ; RUV_ADD\n\n$ERROR\nIPRED = F\nSD_PROP = THETA(1)*IPRED\nSD_ADD = THETA(2)\nSD = SQRT(SD_PROP**2 + SD_ADD**2)\nY = IPRED + SD * EPS(1)\n\n$SIGMA\n1 FIX\nIt wasn’t immediately clear why it was set up this way, and I was left with some questions:\nIt seemed a bit odd to me. I was more familiar with defining RUV directly in the $SIGMA block, something like:\nclassical way v1\n\n$ERROR\nIPRED = F\nY = IPRED + IPRED * EPS(1) + EPS(2)\n\n$SIGMA\n0.0225\n0.0529\nor maybe in a slightly more elegant form:\nclassical way v2\n\nY = IPRED * (1 + EPS(1)) + EPS(2)\nSo, why use this “alternative”2 way of defining the error? Before we try to explain this way of writing a combined error model to ourselves, let’s break down the additive and proportional error model separately to understand what’s going on. Please note: most of this content can also be found elsewhere (Proost 2017)."
  },
  {
    "objectID": "posts/2024-10-09/index.html#additive-error-models",
    "href": "posts/2024-10-09/index.html#additive-error-models",
    "title": "Coding RUV via THETA in NONMEM",
    "section": "Additive error models",
    "text": "Additive error models\nThe “classical” way (if I can call it that) of specifying an additive error model in NONMEM is as follows:\n\n\n\nclassical way (additive)\n\n$ERROR\nIPRED = F\nY = IPRED + EPS(1)\n\n$SIGMA\n0.0529\n\n\nIn this approach, RUV is defined directly in the $SIGMA block, where EPS(1) is assumed to be normally distributed with a mean of 0 and variance of 0.0529:\n\\[EPS(1) \\sim \\mathcal{N}(0,0.0529)\\]\nIt is quite important to note that we are specifying variances in $SIGMA. Now the alternative way (my colleague called it the Uppsala way3) of coding the additive error model looks like this:\n\n\n\nalternative way (additive)\n\n$THETA\n0.23        ; RUV_ADD\n\n$ERROR\nIPRED = F\nSD_ADD = THETA(1)\nY = IPRED + SD_ADD * EPS(1)\n\n$SIGMA\n1 FIX\n\n\nHere, $SIGMA is fixed so EPS(1) has a variance of 1, effectively making it a standard normal distribution:\n\\[EPS(1) \\sim \\mathcal{N}(0,1)\\]\nBut we then multiply this random variable EPS(1) by a scaling factor SD_ADD (which is being estimated as a THETA parameter) before the product is being added to the individual predicted IPRED value:\n\n\n\nalternative way (additive)\n\nY = IPRED + SD_ADD * EPS(1)\n\n\nI am not super familiar what happens if we multiply a random variable with a scaling factor. So maybe it is a good idea to visualize what happens when we fix $SIGMA to 1 and multiply it by SD = 0.23. Let’s start with plotting a standard normal distribution ($SIGMA 1 FIX):\n\n\nCode\n# sample from standard normal distribution\nx &lt;- rnorm(100000, mean = 0, sd = 1)\nstd_norm &lt;- tibble(x = x, source = \"unscaled\")\n\n# plot\nstd_norm |&gt; \n  ggplot(aes(x = x, fill = source)) +\n  geom_density(alpha=0.2)+\n  labs(title = \"Standard normal distribution\", x = \"\", y = \"Density\")+\n  scale_fill_manual(\n    \"Source\",\n    values = c(\n      \"unscaled\" = \"#003049\"\n    )\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThe resulting standard deviation should be 1, and since \\(1^2 = 1\\), the resulting variance should also be 1. Let’s be sure and check our empirical estimates (it is a simulation, after all) to confirm this:\n\n\nCode\n# summarize data and calculate sd and variance\nstd_norm |&gt; \n  group_by(source) |&gt;   \n  summarize(\n    sd = sd(x) |&gt; signif(digits = 3),\n    var = var(x) |&gt; signif(digits = 3)\n  ) |&gt; \n  rename(\n    \"Source\" = source,\n    \"Standard Deviation\" = sd,\n    \"Variance\" = var\n  ) |&gt;\n  kbl() |&gt; kable_styling()\n\n\n\n\n\nSource\nStandard Deviation\nVariance\n\n\n\n\nunscaled\n1\n1\n\n\n\n\n\n\n\nGood. But what happens now to this standard normal distribution if we multiply the random variable with some scaling parameter \\(SD = 0.23\\)? Let’s find out:\n\n\nCode\n# set a seed\nset.seed(123)\n\n# multiply with W\nSD &lt;- 0.23\nx_scaled &lt;- x * SD\nstd_norm_scaled &lt;- tibble(x = x_scaled, source = \"scaled\")\n\n# combine both\nstd_norm_combined &lt;- bind_rows(std_norm, std_norm_scaled)\n\n# plot\nstd_norm_combined |&gt; \n  ggplot(aes(x = x, fill = source)) +\n  geom_density(alpha = 0.2)+\n  labs(title = \"Normal distributions: Impact of scaling factor SD\", x = \"\", y = \"Density\")+\n  scale_fill_manual(\n    \"Source\",\n    values = c(\n      \"unscaled\" = \"#003049\",  # Blue color for original\n      \"scaled\" = \"#c1121f\"     # Orange color for scaled\n    )\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nLet’s compare the standard deviation and variance of both distributions:\n\n\nCode\n# summarize data and calculate sd and variance\nstd_norm_combined |&gt; \n  group_by(source) |&gt;   \n  summarize(\n    sd = sd(x) |&gt; signif(digits = 2),\n    var = var(x) |&gt; signif(digits = 2)\n  ) |&gt; \n  rename(\n    \"Source\" = source,\n    \"Standard Deviation\" = sd,\n    \"Variance\" = var\n  ) |&gt; \n  kbl() |&gt; \n  kable_styling()\n\n\n\n\n\nSource\nStandard Deviation\nVariance\n\n\n\n\nscaled\n0.23\n0.053\n\n\nunscaled\n1.00\n1.000\n\n\n\n\n\n\n\nFor the scaled distribution, we can see that the resulting standard deviation \\(\\sigma\\) is approximately equal to our scaling factor SD_ADD (which is 0.23) and the variance is \\(0.23^2 \\approx 0.053\\). This means that in our model code\n\n\n\nalternative way (additive)\n\nSD_ADD * EPS(1)\n\n\nthe SD_ADD parameter (specified via $THETA) is representing a standard deviation. Cool thing! Probably it’s not too surprising given my naming scheme, but anyways.4 Overall, both of these models should be equivalent:\n\n\n\nclassical way (additive)\n\n$SIGMA\n0.0529   ; variance\n\n\nand\n\n\n\nalternative way (additive)\n\n$THETA\n0.23   ; standard deviation\n\n$SIGMA\n1 FIX\n\n\nTo sum it up: We need to be careful with the units. If we use the classical way, we are estimating a variance via $SIGMA, but if we use the alternative way, we are estimating a standard deviation via $THETA and fix the $SIGMA to a standard normal. Typically, we would report the standard deviation (rather than the variance) if we use an additive model, and I think one of the advantages of the alternative way is that we directly read out the standard deviation from the parameter estimates (without the need to transform anything). Some also say that the estimation becomes more stable if we model the stochastic parts via $THETA, but I cannot judge if this is true or not.\n\n\n\n\n\n\nSpecifying additive RUV via $THETA gives us a standard deviation\n\n\n\nWhenever we have an additive error model and we specify the RUV in the $THETA block (the alternative way), the resulting estimate is a standard deviation."
  },
  {
    "objectID": "posts/2024-10-09/index.html#proportional-error-models",
    "href": "posts/2024-10-09/index.html#proportional-error-models",
    "title": "Coding RUV via THETA in NONMEM",
    "section": "Proportional error models",
    "text": "Proportional error models\nNow, let’s look at proportional error models. The classical way of specifying the proportional error model looks like this:\n\n\n\nclassical way (proportional)\n\n$ERROR\nIPRED = F\nY = IPRED + IPRED * EPS(1)\n\n$SIGMA\n0.0225\n\n\nAnd the alternative way is:\n\n\n\nalternative way (proportional)\n\n$THETA\n0.15        ; RUV_PROP\n\n$ERROR\nIPRED = F\nSD_PROP = IPRED * THETA(1)\nY = IPRED + SD_PROP * EPS(1)\n\n$SIGMA\n1 FIX\n\n\nThe structure is similar to the additive model we discussed earlier, except that the standard deviation of the random noise around our prediction depends on the prediction itself. This is why we first calculate the standard deviation SD_PROP at the given prediction as:\n\n\n\nalternative way (proportional)\n\nSD_PROP = IPRED * THETA(1)\n\n\nThis already gives us an understanding of the units of THETA(1): it represents the coefficient of variation (CV) of the prediction IPRED. Why? A coefficient of variation represents the ratio of the standard deviation to the mean. This is why we end up with a standard deviation (SD_PROP) if we multiply the prediction (IPRED) with the CV (THETA(1)). So we always have a fraction of the prediction representing our standard deviation at that point.\n\nAn example\nSuppose we have a prediction (IPRED) of 10 mg/L and we want to show the resulting distribution. For the classical approach, we would specify a variance (EPS(1)) of 0.0225, and for the alternative way, we would specify a CV (THETA(1)) of 0.15. What do you think? Will this be equivalent or not? Let’s find out!\n\n\nCode\n# Set seed for reproducibility\nset.seed(123)\n\n# Parameters\nIPRED &lt;- 10         \nCV_percent &lt;- 0.15    \nSD_prop &lt;- CV_percent * IPRED  \nsd_classical &lt;- IPRED * sqrt(0.0225)  \n\n# Number of samples\nn &lt;- 100000\n\n# Classical way: Specify variance directly\neps_classical &lt;- rnorm(n, mean = 10, sd = sd_classical)  \n\n# Alternative way: Specify CV%\neps_alternative &lt;- rnorm(n, mean = 10, sd = 1 * SD_prop) \n\n# Create a tibble combining both distributions\nprop_models &lt;- tibble(\n  value = c(eps_classical, eps_alternative),\n  source = rep(c(\"Classical (Variance = 0.0225)\", \"Alternative (CV = 0.15)\"), each = n)\n)\n\n# Plot the density of both distributions\nprop_models |&gt; \n  ggplot(aes(x = value, fill = source)) +\n  geom_density(alpha = 0.4) +\n  labs(\n    title = \"Classical vs. alternative specification\",\n    x = \"Concentration [mg/L]\",\n    y = \"Density\"\n  ) +\n  scale_fill_manual(\n    \"Model Specification\",\n    values = c(\n      \"Classical (Variance = 0.0225)\" = \"#003049\",  # Blue\n      \"Alternative (CV = 0.15)\" = \"#c1121f\"      # Red\n    )\n  ) +\n  scale_x_continuous(breaks=c(4,6,8,10,12,14,16))+\n  theme_bw()\n\n\n\n\n\n\n\n\n\nBoth models end up with the same distribution. In the classical way, we are given a variance of 0.0225. To get the standard deviation, we take the square root of the variance:\n\\[\n\\sigma_{EPS} = \\sqrt{0.0225} = 0.15\n\\] This means, that our random variable EPS(1) has a standard deviation of 0.15 mg/L in our classical model:\n\n\n\nclassical way (proportional)\n\nY = IPRED + IPRED * EPS(1)\n\n\nBy multiplying this EPS(1) by the prediction (IPRED) of 10 mg/L, we are scaling this random variable to have the (desired) standard deviation of the prediction distribution (PRED):\n\\[\n\\sigma_{Y} = 0.15 \\times 10 = 1.5 \\, \\text{mg/L}\n\\]\nIn the alternative way, we are directly estimating the coefficient of variation (CV) as 0.15.\n\n\n\nalternative way (proportional)\n\nSD_PROP = IPRED * THETA(1)\nY = IPRED + SD_PROP * EPS(1)\n\n\nWe are first calculating the respective standard deviation (SD_PROP) by multiplying CV with IPRED. We then turn this standard deviation into a random variable with this standard deviation by multiplying it with a random variable from a standard normal (EPS(1)). Also here, the respective standard deviation of the prediction distribution (PRED) is 1.5 mg/L:\n\\[\n\\sigma_{Y} = 0.15 \\times 10 = 1.5 \\, \\text{mg/L}\n\\]\nIn both cases, the resulting variability is the same, meaning both approaches lead to the same standard deviation of 1.5 mg/L. Again, it is a bit more convenient to specify the CV directly, as it is more intuitive and easier to interpret. And if the stability argument is true (see above), we would also make our estimation more robust this way.\n\n\n\n\n\n\nSpecifying proportional RUV in $THETA gives us a coefficient of variation\n\n\n\nWhenever we have a proportional error model and we specify the RUV in the $THETA block, the resulting estimate is a coefficient of variation."
  },
  {
    "objectID": "posts/2024-10-09/index.html#combined-proportional-and-additive-error-models",
    "href": "posts/2024-10-09/index.html#combined-proportional-and-additive-error-models",
    "title": "Coding RUV via THETA in NONMEM",
    "section": "Combined proportional and additive error models",
    "text": "Combined proportional and additive error models\nFinally, let’s combine our knowledge to understand the alternative way of specifying a combined proportional and additive error model:\n\n\n\nalternative way (combined)\n\n$THETA\n0.15        ; RUV_PROP\n0.23        ; RUV_ADD\n\n$ERROR\nIPRED = F\nSD_PROP = THETA(1)*IPRED\nSD_ADD = THETA(2)\nSD = SQRT(SD_PROP**2 + SD_ADD**2)\nY = IPRED + SD * EPS(1)\n\n$SIGMA\n1 FIX\n\n\nTwo parts should already be familiar:\n\n\n\nalternative way (combined)\n\nSD_PROP = THETA(1)*IPRED\nSD_ADD = THETA(2)\n\n\nIn the first part, we calculate SD_PROP, representing the resulting standard deviation of the proportional part (as THETA(1) is a CV). The second part, SD_ADD, gives us the standard deviation of the additive part. Now we want to find the joint standard deviation SD at the given concentration. But how do we combine these components?\n\n\n\nalternative way (combined)\n\nSD = SQRT(SD_PROP**2 + SD_ADD**2)\n\n\nWe can see that we first square both terms, then add them together, then take the square root. Sounds complicated - why not just add them directly together? This is because variances are additive when combining independent random variables, while standard deviations are not (Soch 2020). Written a bit more formally for two independent random variables (we typically assume the covariance to be 0 when modelling RUV):\n\\[\\mathrm{Var}(X + Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y)\\] In our case, SD_PROP and SD_ADD are standard deviations, so we must first square them to get the variances and then add them. However, we want to go back to a standard deviation before we multiply SD with EPS(1) (being fixed to 1). Therefore, we take the square root in the end.\nThis operation has always confused me a bit, but once I understood that I can sum up variances, but not standard deviations 5 it made more sense to me.\n\n\n\n\n\n\nCombined error models\n\n\n\nWhen specifying a combined error model, the estimates in the $THETA block represent a standard deviation for the additive part and a coefficient of variation for the proportional part."
  },
  {
    "objectID": "posts/2024-10-09/index.html#conclusion",
    "href": "posts/2024-10-09/index.html#conclusion",
    "title": "Coding RUV via THETA in NONMEM",
    "section": "Conclusion",
    "text": "Conclusion\nThis is a somewhat lengthy explanation of why and how we code the alternative approach in NONMEM. Personally, I wasn’t very familiar with how distributions behave when its random variable is being multiplying by a factor, and I didn’t realize that while variances are additive when combining two random processes, standard deviations are not. If you have a stronger background in statistics, this might have been obvious, but I hope this explanation was still helpful for some others."
  },
  {
    "objectID": "posts/2024-10-09/index.html#footnotes",
    "href": "posts/2024-10-09/index.html#footnotes",
    "title": "Coding RUV via THETA in NONMEM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYeah, I know, not really fancy. But that’s how it feels when you touch a combined error model for the first time.↩︎\nFor many of you, this is likely quite standard. The naming reflects my perspective.↩︎\nI’m not sure if this was initially introduced by one of the Uppsala groups or if this is just some hearsay.↩︎\nSome people also code it with W instead of SD but it’s always a good idea to find descriptive variable names.↩︎\nProbably something you would tackle in the first semester of your statistics studies. But not if you study pharmacy ;)↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Marian Klose",
    "section": "",
    "text": "Hi there! I’m Marian Klose, currently a PhD student at Freie Universitaet Berlin, and part of the PharMetrX graduate research training program. I am directly supervised by Prof. Charlotte Kloft from Freie Universitaet Berlin and co-supervised by Prof. Wilhelm Huisinga from University of Potsdam.\nHow did I end up here? After my A-levels I did a work-and-travel year and then started studying chemistry and biochemisty. I realized that I wanted to work in a field that was a bit more applied, so I switched to pharmacy. There I got into pharmacometrics pretty early on and loved the mix of computer science, pharmacy, and math. After my studies, I headed to Florida for six months to work at the University of Florida’s Center for Pharmacometrics and Systems Pharmacology under supervison of Prof. Stephan Schmidt and Prof. Rodrigo Cristofoletti, and also did a three-month internship at Boehringer Ingelheim to see if my interest would remain steady. And I was still fascinated by pharmacometrics, so I decided to start my PhD and applied to the PharMetrX program — and here I am today!\nIn the field of pharmacometrics, not everything is perfectly documented. Thankfully, there are many helpful blogs and articles available that break down complex topics into simpler, more accessible concepts. Whenever I come across a concept I don’t fully understand (which happens quite often), I try to explain it to myself using a Quarto or RMarkdown document. Since I’m anyways creating these documents for myself, I thought I might as well share them as little blog posts. My goal really isn’t to produce perfectly polished content, but rather to document my own learning process and potentially help others who might be facing the same challenges."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Marian Klose",
    "section": "",
    "text": "Hi there! I’m Marian Klose, currently a PhD student at Freie Universitaet Berlin, and part of the PharMetrX graduate research training program. I am directly supervised by Prof. Charlotte Kloft from Freie Universitaet Berlin and co-supervised by Prof. Wilhelm Huisinga from University of Potsdam.\nHow did I end up here? After my A-levels I did a work-and-travel year and then started studying chemistry and biochemisty. I realized that I wanted to work in a field that was a bit more applied, so I switched to pharmacy. There I got into pharmacometrics pretty early on and loved the mix of computer science, pharmacy, and math. After my studies, I headed to Florida for six months to work at the University of Florida’s Center for Pharmacometrics and Systems Pharmacology under supervison of Prof. Stephan Schmidt and Prof. Rodrigo Cristofoletti, and also did a three-month internship at Boehringer Ingelheim to see if my interest would remain steady. And I was still fascinated by pharmacometrics, so I decided to start my PhD and applied to the PharMetrX program — and here I am today!\nIn the field of pharmacometrics, not everything is perfectly documented. Thankfully, there are many helpful blogs and articles available that break down complex topics into simpler, more accessible concepts. Whenever I come across a concept I don’t fully understand (which happens quite often), I try to explain it to myself using a Quarto or RMarkdown document. Since I’m anyways creating these documents for myself, I thought I might as well share them as little blog posts. My goal really isn’t to produce perfectly polished content, but rather to document my own learning process and potentially help others who might be facing the same challenges."
  },
  {
    "objectID": "posts/2024-10-05/index.html",
    "href": "posts/2024-10-05/index.html",
    "title": "About this blog",
    "section": "",
    "text": "As someone involved in pharmacometrics, I’ve often found that many concepts are either poorly documented or explained in such technical language that they become hard to grasp (at least for me). This can be quite frustrating when you’re trying to learn or understand something new for which you don’t have a suitable mathematical and statistical background.\nThere are many nice blogs and articles in the field of pharmacometrics, such as\n\nDanielle Navarro\nPMX Solutions\nTingjie Guo’s NMHelp\nmrgsolve\n\nFor instance, Danielle Navarro’s post on population pharmacokinetic models in stan was incredibly helpful in my understanding of implementing pop-PK models in Stan. She just explained the concepts in a very easy-to-understand way and it is at the same time quite entertaining to read. Please check out her blog, if you haven’t already!\nI often explain new concepts to myself using Quarto or RMarkdown documents and I thought I might as well share them as little blog posts. While I don’t expect to match the quality of the blogs I’ve mentioned, I still hope that some posts can be helpful to others who are tackling similar challenges."
  },
  {
    "objectID": "posts/2024-10-05/index.html#motivation",
    "href": "posts/2024-10-05/index.html#motivation",
    "title": "About this blog",
    "section": "",
    "text": "As someone involved in pharmacometrics, I’ve often found that many concepts are either poorly documented or explained in such technical language that they become hard to grasp (at least for me). This can be quite frustrating when you’re trying to learn or understand something new for which you don’t have a suitable mathematical and statistical background.\nThere are many nice blogs and articles in the field of pharmacometrics, such as\n\nDanielle Navarro\nPMX Solutions\nTingjie Guo’s NMHelp\nmrgsolve\n\nFor instance, Danielle Navarro’s post on population pharmacokinetic models in stan was incredibly helpful in my understanding of implementing pop-PK models in Stan. She just explained the concepts in a very easy-to-understand way and it is at the same time quite entertaining to read. Please check out her blog, if you haven’t already!\nI often explain new concepts to myself using Quarto or RMarkdown documents and I thought I might as well share them as little blog posts. While I don’t expect to match the quality of the blogs I’ve mentioned, I still hope that some posts can be helpful to others who are tackling similar challenges."
  },
  {
    "objectID": "posts/2024-10-05/index.html#disclaimers",
    "href": "posts/2024-10-05/index.html#disclaimers",
    "title": "About this blog",
    "section": "Disclaimers",
    "text": "Disclaimers\nBefore diving in, a few things to note:\n\nI’m not an expert in everything I write about; I’m learning as I go and simply documenting my process. Expect some mistakes along the way!\nEnglish is not my first language, so I rely on tools like ChatGPT and DeepL to help with writing. I also use tools like ChatGPT to generate ideas or to find good ways to visualize concepts.\nI do my best to give credit where it’s due and cite sources when appropriate. Much of what I share builds on others’ work, and I’m not trying to reinvent the wheel."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Coding RUV via THETA in NONMEM\n\n\n\nRUV\n\n\nError\n\n\nNONMEM\n\n\n\nHave you ever been confused why some people use the $THETA block to code their RUV in NONMEM? You are not alone!\n\n\n\nMarian Klose\n\n\nOct 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbout this blog\n\n\n\nMotivation\n\n\nDisclaimer\n\n\n\nSome motivations and disclaimers\n\n\n\nMarian Klose\n\n\nOct 5, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  }
]