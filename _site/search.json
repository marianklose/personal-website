[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Bayesian MAP estimation in R\n\n\n\nNONMEM\n\n\nBayes\n\n\nMAP\n\n\n\nUnderstanding the Bayesian idea and reproducing a MAP estimation in R\n\n\n\nMarian Klose\n\n\nMar 29, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy attempt to understand the NLME estimation algorithm behind NONMEM\n\n\n\nEstimation\n\n\nNONMEM\n\n\nLaplacian\n\n\nR\n\n\n\nAn R-based reproduction using straight-line equations.\n\n\n\nMarian Klose\n\n\nJan 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExpressing RUV as $THETA in NONMEM\n\n\n\nRUV\n\n\nError\n\n\nNONMEM\n\n\n\nHave you ever been confused why some people use the $THETA block to code their RUV in NONMEM? You are not alone!\n\n\n\nMarian Klose\n\n\nDec 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbout this blog\n\n\n\nMotivation\n\n\nDisclaimer\n\n\n\nSome motivations and disclaimers\n\n\n\nMarian Klose\n\n\nDec 5, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/expressing_ruv_as_theta/index.html",
    "href": "posts/expressing_ruv_as_theta/index.html",
    "title": "Expressing RUV as $THETA in NONMEM",
    "section": "",
    "text": "Code\n# load packages\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(kableExtra)\nWhen I started my PhD in pharmacometrics, I wanted to try something fancy1: specifying a combined proportional and additive error model in NONMEM for one of my projects. A colleague kindly sent me a reference model, and to my confusion, the code included a novel way (at least to me) of defining residual unexplained variability (RUV):\nalternative way (combined)\n\n$THETA\n0.15        ; RUV_PROP\n0.23        ; RUV_ADD\n\n$ERROR\nIPRED = F\nSD_PROP = THETA(1)*IPRED\nSD_ADD = THETA(2)\nSD = SQRT(SD_PROP**2 + SD_ADD**2)\nY = IPRED + SD * EPS(1)\n\n$SIGMA\n1 FIX\nIt wasn’t immediately clear why it was set up this way, and I was left with some questions:\nIt seemed a bit odd to me. I was more familiar with defining RUV directly in the $SIGMA block, something like:\nclassical way v1\n\n$ERROR\nIPRED = F\nY = IPRED + IPRED * EPS(1) + EPS(2)\n\n$SIGMA\n0.0225\n0.0529\nor maybe in a slightly more elegant form:\nclassical way v2\n\nY = IPRED * (1 + EPS(1)) + EPS(2)\nSo, why use this “alternative”2 way of defining the error? Before we try to explain this way of writing a combined error model to ourselves, let’s break down the additive and proportional error model separately to understand what’s going on. Please note: most of this content can also be found elsewhere (Proost 2017)."
  },
  {
    "objectID": "posts/expressing_ruv_as_theta/index.html#additive-error-models",
    "href": "posts/expressing_ruv_as_theta/index.html#additive-error-models",
    "title": "Expressing RUV as $THETA in NONMEM",
    "section": "1 Additive error models",
    "text": "1 Additive error models\nThe “classical” way (if I can call it that) of specifying an additive error model in NONMEM is as follows:\n\n\n\nclassical way (additive)\n\n$ERROR\nIPRED = F\nY = IPRED + EPS(1)\n\n$SIGMA\n0.0529\n\n\nIn this approach, RUV is defined directly in the $SIGMA block, where EPS(1) is assumed to be normally distributed with a mean of 0 and variance of 0.0529:\n\\[EPS(1) \\sim \\mathcal{N}(0,0.0529)\\]\nIt is quite important to note that we are specifying variances in $SIGMA. Now the alternative way (my colleague called it the Uppsala way3) of coding the additive error model looks like this:\n\n\n\nalternative way (additive)\n\n$THETA\n0.23        ; RUV_ADD\n\n$ERROR\nIPRED = F\nSD_ADD = THETA(1)\nY = IPRED + SD_ADD * EPS(1)\n\n$SIGMA\n1 FIX\n\n\nHere, $SIGMA is fixed so EPS(1) has a variance of 1, effectively making it a standard normal distribution:\n\\[EPS(1) \\sim \\mathcal{N}(0,1)\\]\nBut we then multiply this random variable EPS(1) by a scaling factor SD_ADD (which is being estimated as a THETA parameter) before the product is being added to the individual predicted IPRED value:\n\n\n\nalternative way (additive)\n\nY = IPRED + SD_ADD * EPS(1)\n\n\nI am not super familiar what happens if we multiply a random variable with a scaling factor. So maybe it is a good idea to visualize what happens when we fix $SIGMA to 1 and multiply it by SD = 0.23. Let’s start with plotting a standard normal distribution ($SIGMA 1 FIX):\n\n\nCode\n# sample from standard normal distribution\nx &lt;- rnorm(100000, mean = 0, sd = 1)\nstd_norm &lt;- tibble(x = x, source = \"unscaled\")\n\n# plot\nstd_norm |&gt; \n  ggplot(aes(x = x, fill = source)) +\n  geom_density(alpha=0.2)+\n  labs(title = \"Standard normal distribution\", x = \"\", y = \"Density\")+\n  scale_fill_manual(\n    \"Source\",\n    values = c(\n      \"unscaled\" = \"#003049\"\n    )\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThe resulting standard deviation should be 1, and since \\(1^2 = 1\\), the resulting variance should also be 1. Let’s be sure and check our empirical estimates (it is a simulation, after all) to confirm this:\n\n\nCode\n# summarize data and calculate sd and variance\nstd_norm |&gt; \n  group_by(source) |&gt;   \n  summarize(\n    sd = sd(x) |&gt; signif(digits = 3),\n    var = var(x) |&gt; signif(digits = 3)\n  ) |&gt; \n  rename(\n    \"Source\" = source,\n    \"Standard Deviation\" = sd,\n    \"Variance\" = var\n  ) |&gt;\n  kbl() |&gt; kable_styling()\n\n\n\n\n\nSource\nStandard Deviation\nVariance\n\n\n\n\nunscaled\n0.998\n0.997\n\n\n\n\n\n\n\nGood. But what happens now to this standard normal distribution if we multiply the random variable with some scaling parameter \\(SD = 0.23\\)? Let’s find out:\n\n\nCode\n# set a seed\nset.seed(123)\n\n# multiply with W\nSD &lt;- 0.23\nx_scaled &lt;- x * SD\nstd_norm_scaled &lt;- tibble(x = x_scaled, source = \"scaled\")\n\n# combine both\nstd_norm_combined &lt;- bind_rows(std_norm, std_norm_scaled)\n\n# plot\nstd_norm_combined |&gt; \n  ggplot(aes(x = x, fill = source)) +\n  geom_density(alpha = 0.2)+\n  labs(title = \"Normal distributions: Impact of scaling factor SD\", x = \"\", y = \"Density\")+\n  scale_fill_manual(\n    \"Source\",\n    values = c(\n      \"unscaled\" = \"#003049\",  # Blue color for original\n      \"scaled\" = \"#c1121f\"     # Orange color for scaled\n    )\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nLet’s compare the standard deviation and variance of both distributions:\n\n\nCode\n# summarize data and calculate sd and variance\nstd_norm_combined |&gt; \n  group_by(source) |&gt;   \n  summarize(\n    sd = sd(x) |&gt; signif(digits = 2),\n    var = var(x) |&gt; signif(digits = 2)\n  ) |&gt; \n  rename(\n    \"Source\" = source,\n    \"Standard Deviation\" = sd,\n    \"Variance\" = var\n  ) |&gt; \n  kbl() |&gt; \n  kable_styling()\n\n\n\n\n\nSource\nStandard Deviation\nVariance\n\n\n\n\nscaled\n0.23\n0.053\n\n\nunscaled\n1.00\n1.000\n\n\n\n\n\n\n\nFor the scaled distribution, we can see that the resulting standard deviation \\(\\sigma\\) is approximately equal to our scaling factor SD_ADD (which is 0.23) and the variance is \\(0.23^2 \\approx 0.053\\). This means that in our model code\n\n\n\nalternative way (additive)\n\nSD_ADD * EPS(1)\n\n\nthe SD_ADD parameter (specified via $THETA) is representing a standard deviation. Cool thing! Probably it’s not too surprising given my naming scheme, but anyways.4 Overall, both of these models should be equivalent:\n\n\n\nclassical way (additive)\n\n$SIGMA\n0.0529   ; variance\n\n\nand\n\n\n\nalternative way (additive)\n\n$THETA\n0.23   ; standard deviation\n\n$SIGMA\n1 FIX\n\n\nTo sum it up: We need to be careful with the units. If we use the classical way, we are estimating a variance via $SIGMA, but if we use the alternative way, we are estimating a standard deviation via $THETA and fix the $SIGMA to a standard normal. Typically, we would report the standard deviation (rather than the variance) if we use an additive model, and I think one of the advantages of the alternative way is that we directly read out the standard deviation from the parameter estimates (without the need to transform anything). Some also say that the estimation becomes more stable if we model the stochastic parts via $THETA, but I cannot judge if this is true or not.\n\n\n\n\n\n\nSpecifying additive RUV via $THETA gives us a standard deviation\n\n\n\nWhenever we have an additive error model and we specify the RUV in the $THETA block (the alternative way), the resulting estimate is a standard deviation."
  },
  {
    "objectID": "posts/expressing_ruv_as_theta/index.html#proportional-error-models",
    "href": "posts/expressing_ruv_as_theta/index.html#proportional-error-models",
    "title": "Expressing RUV as $THETA in NONMEM",
    "section": "2 Proportional error models",
    "text": "2 Proportional error models\nNow, let’s look at proportional error models. The classical way of specifying the proportional error model looks like this:\n\n\n\nclassical way (proportional)\n\n$ERROR\nIPRED = F\nY = IPRED + IPRED * EPS(1)\n\n$SIGMA\n0.0225\n\n\nAnd the alternative way is:\n\n\n\nalternative way (proportional)\n\n$THETA\n0.15        ; RUV_PROP\n\n$ERROR\nIPRED = F\nSD_PROP = IPRED * THETA(1)\nY = IPRED + SD_PROP * EPS(1)\n\n$SIGMA\n1 FIX\n\n\nThe structure is similar to the additive model we discussed earlier, except that the standard deviation of the random noise around our prediction depends on the prediction itself. This is why we first calculate the standard deviation SD_PROP at the given prediction as:\n\n\n\nalternative way (proportional)\n\nSD_PROP = IPRED * THETA(1)\n\n\nThis already gives us an understanding of the units of THETA(1): it represents the coefficient of variation (CV) of the prediction IPRED. Why? A coefficient of variation represents the ratio of the standard deviation to the mean. This is why we end up with a standard deviation (SD_PROP) if we multiply the prediction (IPRED) with the CV (THETA(1)). So we always have a fraction of the prediction representing our standard deviation at that point.\n\n2.1 An example\nSuppose we have a prediction (IPRED) of 10 mg/L and we want to show the resulting distribution. For the classical approach, we would specify a variance (EPS(1)) of 0.0225, and for the alternative way, we would specify a CV (THETA(1)) of 0.15. What do you think? Will this be equivalent or not? Let’s find out!\n\n\nCode\n# Set seed for reproducibility\nset.seed(123)\n\n# Parameters\nIPRED &lt;- 10         \nCV_percent &lt;- 0.15    \nSD_prop &lt;- CV_percent * IPRED  \nsd_classical &lt;- IPRED * sqrt(0.0225)  \n\n# Number of samples\nn &lt;- 100000\n\n# Classical way: Specify variance directly\neps_classical &lt;- rnorm(n, mean = 10, sd = sd_classical)  \n\n# Alternative way: Specify CV%\neps_alternative &lt;- rnorm(n, mean = 10, sd = 1 * SD_prop) \n\n# Create a tibble combining both distributions\nprop_models &lt;- tibble(\n  value = c(eps_classical, eps_alternative),\n  source = rep(c(\"Classical (Variance = 0.0225)\", \"Alternative (CV = 0.15)\"), each = n)\n)\n\n# Plot the density of both distributions\nprop_models |&gt; \n  ggplot(aes(x = value, fill = source)) +\n  geom_density(alpha = 0.4) +\n  labs(\n    title = \"Classical vs. alternative specification\",\n    x = \"Concentration [mg/L]\",\n    y = \"Density\"\n  ) +\n  scale_fill_manual(\n    \"Model Specification\",\n    values = c(\n      \"Classical (Variance = 0.0225)\" = \"#003049\",  # Blue\n      \"Alternative (CV = 0.15)\" = \"#c1121f\"      # Red\n    )\n  ) +\n  scale_x_continuous(breaks=c(4,6,8,10,12,14,16))+\n  theme_bw()\n\n\n\n\n\n\n\n\n\nBoth models end up with the same distribution. In the classical way, we are given a variance of 0.0225. To get the standard deviation, we take the square root of the variance:\n\\[\n\\sigma_{EPS} = \\sqrt{0.0225} = 0.15\n\\] This means, that our random variable EPS(1) has a standard deviation of 0.15 mg/L in our classical model:\n\n\n\nclassical way (proportional)\n\nY = IPRED + IPRED * EPS(1)\n\n\nBy multiplying this EPS(1) by the prediction (IPRED) of 10 mg/L, we are scaling this random variable to have the (desired) standard deviation of the prediction distribution (PRED):\n\\[\n\\sigma_{Y} = 0.15 \\times 10 = 1.5 \\, \\text{mg/L}\n\\]\nIn the alternative way, we are directly estimating the coefficient of variation (CV) as 0.15.\n\n\n\nalternative way (proportional)\n\nSD_PROP = IPRED * THETA(1)\nY = IPRED + SD_PROP * EPS(1)\n\n\nWe are first calculating the respective standard deviation (SD_PROP) by multiplying CV with IPRED. We then turn this standard deviation into a random variable with this standard deviation by multiplying it with a random variable from a standard normal (EPS(1)). Also here, the respective standard deviation of the prediction distribution (PRED) is 1.5 mg/L:\n\\[\n\\sigma_{Y} = 0.15 \\times 10 = 1.5 \\, \\text{mg/L}\n\\]\nIn both cases, the resulting variability is the same, meaning both approaches lead to the same standard deviation of 1.5 mg/L. Again, it is a bit more convenient to specify the CV directly, as it is more intuitive and easier to interpret. And if the stability argument is true (see above), we would also make our estimation more robust this way.\n\n\n\n\n\n\nSpecifying proportional RUV in $THETA gives us a coefficient of variation\n\n\n\nWhenever we have a proportional error model and we specify the RUV in the $THETA block, the resulting estimate is a coefficient of variation."
  },
  {
    "objectID": "posts/expressing_ruv_as_theta/index.html#combined-proportional-and-additive-error-models",
    "href": "posts/expressing_ruv_as_theta/index.html#combined-proportional-and-additive-error-models",
    "title": "Expressing RUV as $THETA in NONMEM",
    "section": "3 Combined proportional and additive error models",
    "text": "3 Combined proportional and additive error models\nFinally, let’s combine our knowledge to understand the alternative way of specifying a combined proportional and additive error model:\n\n\n\nalternative way (combined)\n\n$THETA\n0.15        ; RUV_PROP\n0.23        ; RUV_ADD\n\n$ERROR\nIPRED = F\nSD_PROP = THETA(1)*IPRED\nSD_ADD = THETA(2)\nSD = SQRT(SD_PROP**2 + SD_ADD**2)\nY = IPRED + SD * EPS(1)\n\n$SIGMA\n1 FIX\n\n\nTwo parts should already be familiar:\n\n\n\nalternative way (combined)\n\nSD_PROP = THETA(1)*IPRED\nSD_ADD = THETA(2)\n\n\nIn the first part, we calculate SD_PROP, representing the resulting standard deviation of the proportional part (as THETA(1) is a CV). The second part, SD_ADD, gives us the standard deviation of the additive part. Now we want to find the joint standard deviation SD at the given concentration. But how do we combine these components?\n\n\n\nalternative way (combined)\n\nSD = SQRT(SD_PROP**2 + SD_ADD**2)\n\n\nWe can see that we first square both terms, then add them together, then take the square root. Sounds complicated - why not just add them directly together? This is because variances are additive when combining independent random variables, while standard deviations are not (Soch 2020). Written a bit more formally for two independent random variables (we typically assume the covariance to be 0 when modelling RUV):\n\\[\\mathrm{Var}(X + Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y)\\] In our case, SD_PROP and SD_ADD are standard deviations, so we must first square them to get the variances and then add them. However, we want to go back to a standard deviation before we multiply SD with EPS(1) (being fixed to 1). Therefore, we take the square root in the end.\nThis operation has always confused me a bit, but once I understood that I can sum up variances, but not standard deviations 5 it made more sense to me.\n\n\n\n\n\n\nCombined error models\n\n\n\nWhen specifying a combined error model, the estimates in the $THETA block represent a standard deviation for the additive part and a coefficient of variation for the proportional part."
  },
  {
    "objectID": "posts/expressing_ruv_as_theta/index.html#conclusion",
    "href": "posts/expressing_ruv_as_theta/index.html#conclusion",
    "title": "Expressing RUV as $THETA in NONMEM",
    "section": "4 Conclusion",
    "text": "4 Conclusion\nThis is a somewhat lengthy explanation of why and how we code the alternative approach in NONMEM. Personally, I wasn’t very familiar with how distributions behave when its random variable is being multiplying by a factor, and I didn’t realize that while variances are additive when combining two random processes, standard deviations are not. If you have a stronger background in statistics, this might have been obvious, but I hope this explanation was still helpful for some others."
  },
  {
    "objectID": "posts/expressing_ruv_as_theta/index.html#footnotes",
    "href": "posts/expressing_ruv_as_theta/index.html#footnotes",
    "title": "Expressing RUV as $THETA in NONMEM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYeah, I know, not really fancy. But that’s how it feels when you touch a combined error model for the first time.↩︎\nFor many of you, this is likely quite standard. The naming reflects my perspective.↩︎\nI’m not sure if this was initially introduced by one of the Uppsala groups or if this is just some hearsay.↩︎\nSome people also code it with W instead of SD but it’s always a good idea to find descriptive variable names.↩︎\nProbably something you would tackle in the first semester of your statistics studies. But not if you study pharmacy ;)↩︎"
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html",
    "href": "posts/understanding_nlme_estimation/index.html",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "",
    "text": "Code\n# get rid of workspace\nrm(list = ls())\n\n# load packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(kableExtra)\nlibrary(xpose4)\nlibrary(tidyr)\nlibrary(plotly)\n\n# define base path\nbase_path &lt;- paste0(here::here(), \"/posts/understanding_nlme_estimation/\")\n\n# define function for html table output\nmytbl &lt;- function(tbl){\n  tbl |&gt; \n    kable() |&gt; \n    kable_styling()\n}"
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-prol-mot",
    "href": "posts/understanding_nlme_estimation/index.html#sec-prol-mot",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "1.1 Motivation",
    "text": "1.1 Motivation\nIn this (somewhat lengthy) document I want to share my attempt at understanding and reproducing NONMEM’s objective function in R. Of course you can use NONMEM effectively without knowing the exact calculations behind the objective function and I did so myself for quite a while. But I believe that it’s helpful to have some understanding of what’s happening under the hood, even if it’s just to some extent. Calculating the objective function manually and understanding the math behind the estimation has always been on my bucket list, but I never really knew where to start. After getting some very helpful input during the PharMetrX (2024) A5 module, I felt ready to give it at least a try. So, here we are!"
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-prol-struc",
    "href": "posts/understanding_nlme_estimation/index.html#sec-prol-struc",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "1.2 Structure",
    "text": "1.2 Structure\nLet me briefly outline how I structured this document. The main goal is to manually calculate the objective function in R using straight line equations. Furthermore, I would like to visualize its 3D surface to see the path the estimation algorithm is taking. I also want to reproduce two key steps associated with the estimation: The COVARIANCE step to assess the parameter precision and the POSTHOC step to get individual parameter estimates. As we try to work through these calculations, I also aim to explore and explain to myself some of the theory and intuition behind concepts and calculations along the way.\nTo achieve this, we will first define a simple one-compartment pharmacokinetic model with intravenous bolus administration and first-order elimination (Section 2). Afterwards we will use this model to simulate some virtual concentration-time data (Section 3), which we will then fit using the Laplacian estimation (Section 4) to obtain a reference solution. I chose the Laplacian algorithm because it makes fewer assumptions and simplifications compared to FOCE or FO. It should be easier to go from Laplacian to FOCE-I than vice versa.\nThen, in the biggest part of this document, we will try to construct the equations needed to calculate the objective function value for a given set of parameters and understand why we are taking each step (Section 5). After this is done, we will implement the functions in R (Section 6), reproduce each iteration of the NONMEM run and compare the results to the reference solution.\nFinally, the reward for all the hard work: We will visualize the objective function surface in a 3D plot (Section 7). This should give us a better understanding of the search algorithm and the behavior of the objective function in dependence of the parameter estimates.\nAfter that, we will attempt a complete estimation directly in R using the optim() function (Section 8) instead of only reproducing the objective function based on the iterations NONMEM took. We will compare our R-based parameter estimates against those obtained by NONMEM and discuss any differences.\nFollowing this, we will mimick the COVARIANCE step by retrieving and inverting the Hessian matrix (Section 9) obtained during the R-based optimization to assess parameter precision (relative standard errors). In a last step, we will then calculate the individual ETAs by reproducing the POSTHOC step (Section 10) and comparing the results against NONMEM’s outputs."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-prol-acknow",
    "href": "posts/understanding_nlme_estimation/index.html#sec-prol-acknow",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "1.3 Acknowledgments",
    "text": "1.3 Acknowledgments\nA special thanks to Dr. Niklas Hartung for his input during the PharMetrX (2024) A5 module and his assistance while writing this blog post, Prof. Wilhelm Huisinga for his contributions during the module, and Dr. Christin Nyhoegen for the helpful discussions."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-prol-discl",
    "href": "posts/understanding_nlme_estimation/index.html#sec-prol-discl",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "1.4 Disclaimer",
    "text": "1.4 Disclaimer\nBefore we get started, I want to note a few disclaimers to ward off any imposter syndrome that might kick in. I’m just a PhD student, not formally trained in mathematics or statistics, and I’m learning as I go. If you’re expecting an entirely error-free derivation with coherent statistical notation, this is not be the best resource. You better go with Wang (2007) in that case. My focus here is more about the intuition and maybe about developing a general understanding of the underlying processes.\nMuch of the content in this document is based on the work of others, and there’s not a lot of original thought here. I’ve relied heavily on several key publications (e.g., Wang (2007)), gained a lot of intuition from the PharMetrX (2024) A5 module, had important input from colleagues (see above) and used tools like Wolfram/Mathematica for some calculations and ChatGPT for parts of the writing. With that being said, let’s start!"
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-moddef-code",
    "href": "posts/understanding_nlme_estimation/index.html#sec-moddef-code",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "2.1 Model code",
    "text": "2.1 Model code\nThe NONMEM model code for our simple one-compartment PK model is shown below. Please note that we are using the ADVAN1 routine, which relies on the analytical expression for a one-compartment model, rather than an ODE solver.\n\n\n1cmt_iv_sim.mod\n\n# read_file(paste0(base_path, \"/models/simulation/1cmt_iv_sim.mod\"))\n\n$PROBLEM 1cmt_iv_sim\n\n$INPUT ID TIME EVID AMT RATE DV MDV\n\n$DATA C:\\Users\\maria\\Desktop\\G\\Mitarbeiter\\Klose\\Miscellaneous\\NLME_reproduction_R\\data\\input_for_sim\\input_data.csv IGNORE=@\n\n$SUBROUTINES ADVAN1 TRANS2\n\n$PK\n; define fixed effects parameters\nCL = THETA(1) * EXP(ETA(1))\nV = THETA(2)\n\n; scaling\nS1=V\n\n$THETA\n(0, 0.2, 1)             ; 1 TVCL\n3.15 FIX                  ; 2 TVV\n\n$OMEGA \n0.2                     ; 1 OM_CL\n\n$SIGMA\n0.1 FIX                 ; 1 SIG_ADD\n\n$ERROR \n; add additive error\nY = F + EPS(1)\n\n; store error for table output\nERR1 = EPS(1)\n\n; $ESTIMATION METHOD=COND LAPLACIAN MAXEVAL=9999 SIGDIG=3 PRINT=1 NOABORT POSTHOC\n$SIMULATION (12345678) ONLYSIM\n\n$TABLE ID TIME EVID AMT RATE DV MDV ETA1 ERR1 NOAPPEND ONEHEADER NOPRINT FILE=sim_out\n\nDepending on the task (simulation vs. estimation), we’ll adjust the model code slightly. For simulation, we use $SIMULATION (12345678) ONLYSIM and for estimation, we are going to use $ESTIMATION METHOD=COND LAPLACIAN MAXEVAL=9999 SIGDIG=3 PRINT=1 NOABORT POSTHOC. Additionally, the $DATA block will vary depending on the task. For simulation, we will just pass a dosing and sampling scheme. For estimation, we’ll then use the simulated data from the first step and use the simulated concentration values for parameter estimation."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-sim-input",
    "href": "posts/understanding_nlme_estimation/index.html#sec-sim-input",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "3.1 Input dataset generation",
    "text": "3.1 Input dataset generation\nTo simulate data, we consider a scenario with 10 individuals, each having five observations at different time points within 24 hours. The selected time points are 0.01, 3, 6, 12, and 24 hours. To my understanding, it is crucial to have at least two observations per individual to reliably estimate inter-individual variability, as having only one observation per individual would make it impossible to distinguish between inter-individual variability and residual variability.\nThe dataset includes dosing records (EVID = 1) and observation records (EVID = 0). For now the dependent variable (DV) will be flagged to -99 since we will simulate these values in the following steps. Here is how our input dataset looks like:\n\n\nCode\n# generate NONMEM input dataset for n individuals with 3 observations at different timepoints\nn_ind &lt;- 10\nn_obs &lt;- 5\ntimepoints &lt;- c(0.01, 3, 6, 12, 24)\n\n# observation events (EVID == 0)\nobs_input &lt;- tibble(\n  ID = rep(1:n_ind, each = n_obs),\n  TIME = rep(timepoints, n_ind),\n  EVID = 0,\n  AMT = 0,\n  RATE = 0,\n  DV = -99, # DV is what we want to simulate\n  MDV = 0\n)\n\n# dosing events (EVID == 1)\ndosing_input &lt;- tibble(\n  ID = rep(1:n_ind, each = 1),\n  TIME = 0,\n  EVID = 1,\n  AMT = 100,\n  RATE = 0,\n  DV = 0,\n  MDV = 1\n)\n\n# bind together and sort by ID and TIME\ninput_data &lt;- bind_rows(obs_input, dosing_input) |&gt; \n  arrange(ID, TIME)\n\n# show input data\ninput_data |&gt; \n  head(n=10) |&gt; \n  mytbl()\n\n\n\n\n\nID\nTIME\nEVID\nAMT\nRATE\nDV\nMDV\n\n\n\n\n1\n0.00\n1\n100\n0\n0\n1\n\n\n1\n0.01\n0\n0\n0\n-99\n0\n\n\n1\n3.00\n0\n0\n0\n-99\n0\n\n\n1\n6.00\n0\n0\n0\n-99\n0\n\n\n1\n12.00\n0\n0\n0\n-99\n0\n\n\n1\n24.00\n0\n0\n0\n-99\n0\n\n\n2\n0.00\n1\n100\n0\n0\n1\n\n\n2\n0.01\n0\n0\n0\n-99\n0\n\n\n2\n3.00\n0\n0\n0\n-99\n0\n\n\n2\n6.00\n0\n0\n0\n-99\n0\n\n\n\n\n\n\n\nCode\n# save data to file\nwrite_csv(input_data, paste0(base_path, \"data/input_for_sim/input_data.csv\"))\n# write_csv(input_data, \"C:\\\\Users\\\\maria\\\\Desktop\\\\G\\\\Mitarbeiter\\\\Klose\\\\Miscellaneous\\\\NLME_reproduction_R\\\\data\\\\input_for_sim\\\\input_data.csv\")\n\n\nWe arbitrarily decided that each individual receives the same dose of 100 mg at time 0. Same dose fits all, right? We can see that at 0.01, 3, 6, 12, and 24 hours after dosing we encoded sampling events (EVID = 0). We can now plug this dataset into NONMEM and simulate these concentrations!"
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-sim-sim",
    "href": "posts/understanding_nlme_estimation/index.html#sec-sim-sim",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "3.2 Simulation",
    "text": "3.2 Simulation\nWith the input dataset generated, the next step is to use it to simulate virtual concentration-time data. This simulation is performed using NONMEM, using the dosing and sampling dataset defined above. This step happens in NONMEM itself and will be be executed separately from this R session.\n\n3.2.1 Read in simulated data\nAfter running the simulation in NONMEM, the generated output (in our case a file called sim_out) contains the simulated concentration values within the DV column. The simulated dataset also includes additional columns such as ETA1, which represents the realization of inter-individual variability, and ERR1, which represents the realization of residual unexplained variability (RUV). So for each individual we have drawn one ETA1 and for each observation we have one ERR1.\n\n\nCode\n# load simulated data\nsim_data &lt;- read_nm_table(paste0(base_path, \"models/simulation/sim_out\"))\n\n# show simulated data\nsim_data |&gt; \n  head(n=10) |&gt; \n  mytbl()\n\n\n\n\n\nID\nTIME\nEVID\nAMT\nRATE\nDV\nMDV\nETA1\nERR1\n\n\n\n\n1\n0.00\n1\n100\n0\n0.0000\n1\n0.39117\n0.136050\n\n\n1\n0.01\n0\n0\n0\n30.8160\n0\n0.39117\n-0.900050\n\n\n1\n3.00\n0\n0\n0\n24.5520\n0\n0.39117\n0.598410\n\n\n1\n6.00\n0\n0\n0\n17.9250\n0\n0.39117\n-0.148090\n\n\n1\n12.00\n0\n0\n0\n10.1100\n0\n0.39117\n-0.180050\n\n\n1\n24.00\n0\n0\n0\n3.5975\n0\n0.39117\n0.262420\n\n\n2\n0.00\n1\n100\n0\n0.0000\n1\n0.12054\n-0.037607\n\n\n2\n0.01\n0\n0\n0\n31.4550\n0\n0.12054\n-0.268190\n\n\n2\n3.00\n0\n0\n0\n26.2030\n0\n0.12054\n0.595370\n\n\n2\n6.00\n0\n0\n0\n20.5520\n0\n0.12054\n-0.103830\n\n\n\n\n\n\n\nWe can see that we obtained a dataset with the simulated concentration values in the DV column. The simulated data now needs to be saved as .csv in order to use it in the subsequent steps within NONMEM.\n\n\nCode\n# subset dataframe\nsim_data_reduced &lt;- sim_data |&gt; \n  select(ID, TIME, EVID, AMT, RATE, DV, MDV)\n\n# save dataframe\nwrite_csv(sim_data_reduced, paste0(base_path, \"data/output_from_sim/sim_data.csv\"))\n\n# filter to have EVID == 0 only\nsim_data_reduced &lt;- sim_data_reduced |&gt; \n  filter(EVID == 0) \n\n\nGreat! The sim_data.csv dataframe was successfully saved and we can later use it for the generation of the reference solution in NONMEM."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-sim-vis",
    "href": "posts/understanding_nlme_estimation/index.html#sec-sim-vis",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "3.3 Concentration-time profiles",
    "text": "3.3 Concentration-time profiles\nNow we can visualize the simulated data stratified by individual to get a feeling for our virtual clinical data.\n\n\nCode\n# show individual profiles\nsim_data |&gt; \n  filter(EVID == 0) |&gt; \n  ggplot(aes(x=TIME, y=DV, group=ID, color=as.factor(ID))) +\n  geom_point()+\n  geom_line()+\n  theme_bw()+\n  scale_y_continuous(limits=c(0,NA))+\n  labs(x=\"Time after last dose [h]\", y=\"Concentration [mg/L]\")+\n  ggtitle(\"Simulated data\")+\n  scale_color_discrete(\"Individual\")\n\n\n\n\n\n\n\n\nFigure 2: Simulated concentration-time profiles for 10 individuals.\n\n\n\n\n\nAs each of the 10 individuals received the same dose, we can clearly see the impact of variability on clearance on concentration-time profiles. Additionally, the single data points are influenced by the residual unexplained variability, which adds noise to the readouts.\nWe are making progress! In a next step we now want to generate a reference solution for the estimation within NONMEM. By doing so we can (in the end) compare our own implementation of the objective function to the one NONMEM uses."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-nm-est-est",
    "href": "posts/understanding_nlme_estimation/index.html#sec-nm-est-est",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "4.1 Estimation",
    "text": "4.1 Estimation\nAs mentioned before, the actual estimation will again happen in NONMEM and therefore needs to be executed separately from this R session. In this step, we are just going to read in the NONMEM output files and visualize them appropriately. This is going to happen as a next step."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-nm-est-read",
    "href": "posts/understanding_nlme_estimation/index.html#sec-nm-est-read",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "4.2 NONMEM output files",
    "text": "4.2 NONMEM output files\n\n4.2.1 .lst file\nFirst of all, we can read in the .lst file. It contains many information about the parameter estimation process and is a nice way to get a quick overview of the model run.\n\n\n\n\n\n\n.lst file\n\n\n\n\n\n\n\nCode\nread_file(paste0(base_path, \"models/estimation/1cmt_iv_est.lst\"))\n\n\n\n\n1cmt_iv_sim.lst\n\nMon 12/30/2024 \n10:54 PM\n$PROBLEM    1cmt_iv_est\n$INPUT      ID TIME EVID AMT RATE DV MDV\n$DATA      sim_data.csv IGNORE=@\n$SUBROUTINE ADVAN1 TRANS2\n$PK\n; define fixed effects parameters\nCL = THETA(1) * EXP(ETA(1))\nV = THETA(2)\n\n; scaling\nS1=V\n\n$THETA  (0,0.1,1) ; 1 TVCL\n 3.15 FIX ; 2 TVV\n$OMEGA  0.15  ;    1 OM_CL\n$SIGMA  0.1  FIX  ;  1 SIG_ADD\n$ERROR \n; add additive error\nY = F + EPS(1)\n\n; store error for table output\nERR1 = EPS(1)\n\n$ESTIMATION METHOD=COND LAPLACIAN MAXEVAL=9999 SIGDIG=3 PRINT=1\n            NOABORT POSTHOC\n$COVARIANCE PRINT=E\n$TABLE      ID TIME EVID AMT RATE DV MDV ETA1 ERR1 CL NOAPPEND\n            ONEHEADER NOPRINT FILE=estim_out\n\n  \nNM-TRAN MESSAGES \n  \n WARNINGS AND ERRORS (IF ANY) FOR PROBLEM    1\n             \n (WARNING  2) NM-TRAN INFERS THAT THE DATA ARE POPULATION.\n  \nLicense Registered to: Freie Universitaet Berlin Department of Clinical Pharmacy Biochemistry\nExpiration Date:    14 JAN 2025\nCurrent Date:       30 DEC 2024\n  **** WARNING!!! Days until program expires :  19 ****\n  **** CONTACT idssoftware@iconplc.com FOR RENEWAL ****\n1NONLINEAR MIXED EFFECTS MODEL PROGRAM (NONMEM) VERSION 7.5.1\n ORIGINALLY DEVELOPED BY STUART BEAL, LEWIS SHEINER, AND ALISON BOECKMANN\n CURRENT DEVELOPERS ARE ROBERT BAUER, ICON DEVELOPMENT SOLUTIONS,\n AND ALISON BOECKMANN. IMPLEMENTATION, EFFICIENCY, AND STANDARDIZATION\n PERFORMED BY NOUS INFOSYSTEMS.\n\n PROBLEM NO.:         1\n 1cmt_iv_est\n0DATA CHECKOUT RUN:              NO\n DATA SET LOCATED ON UNIT NO.:    2\n THIS UNIT TO BE REWOUND:        NO\n NO. OF DATA RECS IN DATA SET:       60\n NO. OF DATA ITEMS IN DATA SET:   7\n ID DATA ITEM IS DATA ITEM NO.:   1\n DEP VARIABLE IS DATA ITEM NO.:   6\n MDV DATA ITEM IS DATA ITEM NO.:  7\n0INDICES PASSED TO SUBROUTINE PRED:\n   3   2   4   5   0   0   0   0   0   0   0\n0LABELS FOR DATA ITEMS:\n ID TIME EVID AMT RATE DV MDV\n0(NONBLANK) LABELS FOR PRED-DEFINED ITEMS:\n CL ERR1\n0FORMAT FOR DATA:\n (7E8.0)\n\n TOT. NO. OF OBS RECS:       50\n TOT. NO. OF INDIVIDUALS:       10\n0LENGTH OF THETA:   2\n0DEFAULT THETA BOUNDARY TEST OMITTED:    NO\n0OMEGA HAS SIMPLE DIAGONAL FORM WITH DIMENSION:   1\n0DEFAULT OMEGA BOUNDARY TEST OMITTED:    NO\n0SIGMA HAS SIMPLE DIAGONAL FORM WITH DIMENSION:   1\n0DEFAULT SIGMA BOUNDARY TEST OMITTED:    NO\n0INITIAL ESTIMATE OF THETA:\n LOWER BOUND    INITIAL EST    UPPER BOUND\n  0.0000E+00     0.1000E+00     0.1000E+01\n  0.3150E+01     0.3150E+01     0.3150E+01\n0INITIAL ESTIMATE OF OMEGA:\n 0.1500E+00\n0INITIAL ESTIMATE OF SIGMA:\n 0.1000E+00\n0SIGMA CONSTRAINED TO BE THIS INITIAL ESTIMATE\n0COVARIANCE STEP OMITTED:        NO\n EIGENVLS. PRINTED:             YES\n SPECIAL COMPUTATION:            NO\n COMPRESSED FORMAT:              NO\n GRADIENT METHOD USED:     NOSLOW\n SIGDIGITS ETAHAT (SIGLO):                  -1\n SIGDIGITS GRADIENTS (SIGL):                -1\n EXCLUDE COV FOR FOCE (NOFCOV):              NO\n Cholesky Transposition of R Matrix (CHOLROFF):0\n KNUTHSUMOFF:                                -1\n RESUME COV ANALYSIS (RESUME):               NO\n SIR SAMPLE SIZE (SIRSAMPLE):\n NON-LINEARLY TRANSFORM THETAS DURING COV (THBND): 1\n PRECONDTIONING CYCLES (PRECOND):        0\n PRECONDTIONING TYPES (PRECONDS):        TOS\n FORCED PRECONDTIONING CYCLES (PFCOND):0\n PRECONDTIONING TYPE (PRETYPE):        0\n FORCED POS. DEFINITE SETTING DURING PRECONDITIONING: (FPOSDEF):0\n SIMPLE POS. DEFINITE SETTING: (POSDEF):-1\n0TABLES STEP OMITTED:    NO\n NO. OF TABLES:           1\n SEED NUMBER (SEED):    11456\n NPDTYPE:    0\n INTERPTYPE:    0\n RANMETHOD:             3U\n MC SAMPLES (ESAMPLE):    300\n WRES SQUARE ROOT TYPE (WRESCHOL): EIGENVALUE\n0-- TABLE   1 --\n0RECORDS ONLY:    ALL\n04 COLUMNS APPENDED:    NO\n PRINTED:                NO\n HEADER:                YES\n FILE TO BE FORWARDED:   NO\n FORMAT:                S1PE11.4\n IDFORMAT:\n LFORMAT:\n RFORMAT:\n FIXED_EFFECT_ETAS:\n0USER-CHOSEN ITEMS:\n ID TIME EVID AMT RATE DV MDV ETA1 ERR1 CL\n1DOUBLE PRECISION PREDPP VERSION 7.5.1\n\n ONE COMPARTMENT MODEL (ADVAN1)\n0MAXIMUM NO. OF BASIC PK PARAMETERS:   2\n0BASIC PK PARAMETERS (AFTER TRANSLATION):\n   ELIMINATION RATE (K) IS BASIC PK PARAMETER NO.:  1\n\n TRANSLATOR WILL CONVERT PARAMETERS\n CLEARANCE (CL) AND VOLUME (V) TO K (TRANS2)\n0COMPARTMENT ATTRIBUTES\n COMPT. NO.   FUNCTION   INITIAL    ON/OFF      DOSE      DEFAULT    DEFAULT\n                         STATUS     ALLOWED    ALLOWED    FOR DOSE   FOR OBS.\n    1         CENTRAL      ON         NO         YES        YES        YES\n    2         OUTPUT       OFF        YES        NO         NO         NO\n1\n ADDITIONAL PK PARAMETERS - ASSIGNMENT OF ROWS IN GG\n COMPT. NO.                             INDICES\n              SCALE      BIOAVAIL.   ZERO-ORDER  ZERO-ORDER  ABSORB\n                         FRACTION    RATE        DURATION    LAG\n    1            3           *           *           *           *\n    2            *           -           -           -           -\n             - PARAMETER IS NOT ALLOWED FOR THIS MODEL\n             * PARAMETER IS NOT SUPPLIED BY PK SUBROUTINE;\n               WILL DEFAULT TO ONE IF APPLICABLE\n0DATA ITEM INDICES USED BY PRED ARE:\n   EVENT ID DATA ITEM IS DATA ITEM NO.:      3\n   TIME DATA ITEM IS DATA ITEM NO.:          2\n   DOSE AMOUNT DATA ITEM IS DATA ITEM NO.:   4\n   DOSE RATE DATA ITEM IS DATA ITEM NO.:     5\n\n0PK SUBROUTINE CALLED WITH EVERY EVENT RECORD.\n PK SUBROUTINE NOT CALLED AT NONEVENT (ADDITIONAL OR LAGGED) DOSE TIMES.\n0ERROR SUBROUTINE CALLED WITH EVERY EVENT RECORD.\n1\n\n\n #TBLN:      1\n #METH: Laplacian Conditional Estimation\n\n ESTIMATION STEP OMITTED:                 NO\n ANALYSIS TYPE:                           POPULATION\n NUMBER OF SADDLE POINT RESET ITERATIONS:      0\n GRADIENT METHOD USED:               NOSLOW\n CONDITIONAL ESTIMATES USED:              YES\n CENTERED ETA:                            NO\n EPS-ETA INTERACTION:                     NO\n LAPLACIAN OBJ. FUNC.:                    YES\n NUMERICAL 2ND DERIVATIVES:               NO\n NO. OF FUNCT. EVALS. ALLOWED:            9999\n NO. OF SIG. FIGURES REQUIRED:            3\n INTERMEDIATE PRINTOUT:                   YES\n ESTIMATE OUTPUT TO MSF:                  NO\n ABORT WITH PRED EXIT CODE 1:             NO\n IND. OBJ. FUNC. VALUES SORTED:           NO\n NUMERICAL DERIVATIVE\n       FILE REQUEST (NUMDER):               NONE\n MAP (ETAHAT) ESTIMATION METHOD (OPTMAP):   0\n ETA HESSIAN EVALUATION METHOD (ETADER):    0\n INITIAL ETA FOR MAP ESTIMATION (MCETA):    0\n SIGDIGITS FOR MAP ESTIMATION (SIGLO):      100\n GRADIENT SIGDIGITS OF\n       FIXED EFFECTS PARAMETERS (SIGL):     100\n NOPRIOR SETTING (NOPRIOR):                 0\n NOCOV SETTING (NOCOV):                     OFF\n DERCONT SETTING (DERCONT):                 OFF\n FINAL ETA RE-EVALUATION (FNLETA):          1\n EXCLUDE NON-INFLUENTIAL (NON-INFL.) ETAS\n       IN SHRINKAGE (ETASTYPE):             NO\n NON-INFL. ETA CORRECTION (NONINFETA):      0\n RAW OUTPUT FILE (FILE): psn.ext\n EXCLUDE TITLE (NOTITLE):                   NO\n EXCLUDE COLUMN LABELS (NOLABEL):           NO\n FORMAT FOR ADDITIONAL FILES (FORMAT):      S1PE12.5\n PARAMETER ORDER FOR OUTPUTS (ORDER):       TSOL\n KNUTHSUMOFF:                               0\n INCLUDE LNTWOPI:                           NO\n INCLUDE CONSTANT TERM TO PRIOR (PRIORC):   NO\n INCLUDE CONSTANT TERM TO OMEGA (ETA) (OLNTWOPI):NO\n ADDITIONAL CONVERGENCE TEST (CTYPE=4)?:    NO\n EM OR BAYESIAN METHOD USED:                 NONE\n\n\n THE FOLLOWING LABELS ARE EQUIVALENT\n PRED=NPRED\n RES=NRES\n WRES=NWRES\n IWRS=NIWRES\n IPRD=NIPRED\n IRS=NIRES\n\n MONITORING OF SEARCH:\n\n\n0ITERATION NO.:    0    OBJECTIVE VALUE:   41.8454368139310        NO. OF FUNC. EVALS.:   4\n CUMULATIVE NO. OF FUNC. EVALS.:        4\n NPARAMETR:  1.0000E-01  1.5000E-01\n PARAMETER:  1.0000E-01  1.0000E-01\n GRADIENT:  -1.0545E+02 -1.0294E+02\n\n0ITERATION NO.:    1    OBJECTIVE VALUE:  -2.74307593433339        NO. OF FUNC. EVALS.:   5\n CUMULATIVE NO. OF FUNC. EVALS.:        9\n NPARAMETR:  1.6837E-01  4.8397E-01\n PARAMETER:  7.0000E-01  6.8569E-01\n GRADIENT:   2.8868E+00  9.4212E+00\n\n0ITERATION NO.:    2    OBJECTIVE VALUE:  -3.06819849357734        NO. OF FUNC. EVALS.:   8\n CUMULATIVE NO. OF FUNC. EVALS.:       17\n NPARAMETR:  1.6369E-01  3.8814E-01\n PARAMETER:  6.6620E-01  5.7537E-01\n GRADIENT:  -2.2554E+00  5.6641E+00\n\n0ITERATION NO.:    3    OBJECTIVE VALUE:  -11.2971148236422        NO. OF FUNC. EVALS.:   5\n CUMULATIVE NO. OF FUNC. EVALS.:       22\n NPARAMETR:  2.1655E-01  1.4655E-01\n PARAMETER:  1.0114E+00  8.8367E-02\n GRADIENT:   6.6259E+00  2.6609E+00\n\n0ITERATION NO.:    4    OBJECTIVE VALUE:  -11.2971148236422        NO. OF FUNC. EVALS.:  10\n CUMULATIVE NO. OF FUNC. EVALS.:       32\n NPARAMETR:  2.1655E-01  1.4655E-01\n PARAMETER:  1.0114E+00  8.8367E-02\n GRADIENT:  -1.3899E+01  2.6592E+00\n\n0ITERATION NO.:    5    OBJECTIVE VALUE:  -12.4187275503686        NO. OF FUNC. EVALS.:   7\n CUMULATIVE NO. OF FUNC. EVALS.:       39\n NPARAMETR:  2.5349E-01  8.6243E-02\n PARAMETER:  1.2172E+00 -1.7673E-01\n GRADIENT:   4.7000E+00 -5.6454E+00\n\n0ITERATION NO.:    6    OBJECTIVE VALUE:  -12.7891181563899        NO. OF FUNC. EVALS.:   7\n CUMULATIVE NO. OF FUNC. EVALS.:       46\n NPARAMETR:  2.4329E-01  1.1715E-01\n PARAMETER:  1.1625E+00 -2.3573E-02\n GRADIENT:  -1.7854E+00  1.1790E+00\n\n0ITERATION NO.:    7    OBJECTIVE VALUE:  -12.8236151522707        NO. OF FUNC. EVALS.:   7\n CUMULATIVE NO. OF FUNC. EVALS.:       53\n NPARAMETR:  2.4614E-01  1.1120E-01\n PARAMETER:  1.1779E+00 -4.9675E-02\n GRADIENT:  -2.9776E-01  2.0940E-01\n\n0ITERATION NO.:    8    OBJECTIVE VALUE:  -12.8245888793110        NO. OF FUNC. EVALS.:   7\n CUMULATIVE NO. OF FUNC. EVALS.:       60\n NPARAMETR:  2.4672E-01  1.0995E-01\n PARAMETER:  1.1810E+00 -5.5291E-02\n GRADIENT:   1.8561E-02 -1.2024E-02\n\n0ITERATION NO.:    9    OBJECTIVE VALUE:  -12.8245933812655        NO. OF FUNC. EVALS.:   7\n CUMULATIVE NO. OF FUNC. EVALS.:       67\n NPARAMETR:  2.4668E-01  1.1002E-01\n PARAMETER:  1.1808E+00 -5.4985E-02\n GRADIENT:  -1.3169E-04  1.3509E-04\n\n0ITERATION NO.:   10    OBJECTIVE VALUE:  -12.8245933812655        NO. OF FUNC. EVALS.:   4\n CUMULATIVE NO. OF FUNC. EVALS.:       71\n NPARAMETR:  2.4668E-01  1.1002E-01\n PARAMETER:  1.1808E+00 -5.4985E-02\n GRADIENT:  -1.3169E-04  1.3509E-04\n\n #TERM:\n0MINIMIZATION SUCCESSFUL\n NO. OF FUNCTION EVALUATIONS USED:       71\n NO. OF SIG. DIGITS IN FINAL EST.:  4.5\n\n ETABAR IS THE ARITHMETIC MEAN OF THE ETA-ESTIMATES,\n AND THE P-VALUE IS GIVEN FOR THE NULL HYPOTHESIS THAT THE TRUE MEAN IS 0.\n\n ETABAR:        -1.8954E-05\n SE:             1.0473E-01\n N:                      10\n\n P VAL.:         9.9986E-01\n\n ETASHRINKSD(%)  1.5107E-01\n ETASHRINKVR(%)  3.0192E-01\n EBVSHRINKSD(%)  1.3728E-01\n EBVSHRINKVR(%)  2.7437E-01\n RELATIVEINF(%)  9.9726E+01\n EPSSHRINKSD(%)  1.8336E+01\n EPSSHRINKVR(%)  3.3311E+01\n\n  \n TOTAL DATA POINTS NORMALLY DISTRIBUTED (N):           50\n N*LOG(2PI) CONSTANT TO OBJECTIVE FUNCTION:    91.893853320467272     \n OBJECTIVE FUNCTION VALUE WITHOUT CONSTANT:   -12.824593381265490     \n OBJECTIVE FUNCTION VALUE WITH CONSTANT:       79.069259939201785     \n REPORTED OBJECTIVE FUNCTION DOES NOT CONTAIN CONSTANT\n  \n TOTAL EFFECTIVE ETAS (NIND*NETA):                            10\n  \n #TERE:\n Elapsed estimation  time in seconds:     0.02\n Elapsed covariance  time in seconds:     0.00\n Elapsed postprocess time in seconds:     0.00\n1\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n ************************************************************************************************************************\n ********************                                                                                ********************\n ********************                         LAPLACIAN CONDITIONAL ESTIMATION                       ********************\n #OBJT:**************                       MINIMUM VALUE OF OBJECTIVE FUNCTION                      ********************\n ********************                                                                                ********************\n ************************************************************************************************************************\n \n\n\n\n\n\n #OBJV:********************************************      -12.825       **************************************************\n1\n ************************************************************************************************************************\n ********************                                                                                ********************\n ********************                         LAPLACIAN CONDITIONAL ESTIMATION                       ********************\n ********************                             FINAL PARAMETER ESTIMATE                           ********************\n ********************                                                                                ********************\n ************************************************************************************************************************\n \n\n\n THETA - VECTOR OF FIXED EFFECTS PARAMETERS   *********\n\n\n         TH 1      TH 2     \n \n         2.47E-01  3.15E+00\n \n\n\n OMEGA - COV MATRIX FOR RANDOM EFFECTS - ETAS  ********\n\n\n         ETA1     \n \n ETA1\n+        1.10E-01\n \n\n\n SIGMA - COV MATRIX FOR RANDOM EFFECTS - EPSILONS  ****\n\n\n         EPS1     \n \n EPS1\n+        1.00E-01\n \n1\n\n\n OMEGA - CORR MATRIX FOR RANDOM EFFECTS - ETAS  *******\n\n\n         ETA1     \n \n ETA1\n+        3.32E-01\n \n\n\n SIGMA - CORR MATRIX FOR RANDOM EFFECTS - EPSILONS  ***\n\n\n         EPS1     \n \n EPS1\n+        3.16E-01\n \n1\n ************************************************************************************************************************\n ********************                                                                                ********************\n ********************                         LAPLACIAN CONDITIONAL ESTIMATION                       ********************\n ********************                            STANDARD ERROR OF ESTIMATE                          ********************\n ********************                                                                                ********************\n ************************************************************************************************************************\n \n\n\n THETA - VECTOR OF FIXED EFFECTS PARAMETERS   *********\n\n\n         TH 1      TH 2     \n \n         2.59E-02 .........\n \n\n\n OMEGA - COV MATRIX FOR RANDOM EFFECTS - ETAS  ********\n\n\n         ETA1     \n \n ETA1\n+        4.87E-02\n \n\n\n SIGMA - COV MATRIX FOR RANDOM EFFECTS - EPSILONS  ****\n\n\n         EPS1     \n \n EPS1\n+       .........\n \n1\n\n\n OMEGA - CORR MATRIX FOR RANDOM EFFECTS - ETAS  *******\n\n\n         ETA1     \n \n ETA1\n+        7.35E-02\n \n\n\n SIGMA - CORR MATRIX FOR RANDOM EFFECTS - EPSILONS  ***\n\n\n         EPS1     \n \n EPS1\n+       .........\n \n1\n ************************************************************************************************************************\n ********************                                                                                ********************\n ********************                         LAPLACIAN CONDITIONAL ESTIMATION                       ********************\n ********************                          COVARIANCE MATRIX OF ESTIMATE                         ********************\n ********************                                                                                ********************\n ************************************************************************************************************************\n \n\n            TH 1      TH 2      OM11      SG11  \n \n TH 1\n+        6.72E-04\n \n TH 2\n+       ......... .........\n \n OM11\n+        7.98E-04 .........  2.37E-03\n \n SG11\n+       ......... ......... ......... .........\n \n1\n ************************************************************************************************************************\n ********************                                                                                ********************\n ********************                         LAPLACIAN CONDITIONAL ESTIMATION                       ********************\n ********************                          CORRELATION MATRIX OF ESTIMATE                        ********************\n ********************                                                                                ********************\n ************************************************************************************************************************\n \n\n            TH 1      TH 2      OM11      SG11  \n \n TH 1\n+        2.59E-02\n \n TH 2\n+       ......... .........\n \n OM11\n+        6.32E-01 .........  4.87E-02\n \n SG11\n+       ......... ......... ......... .........\n \n1\n ************************************************************************************************************************\n ********************                                                                                ********************\n ********************                         LAPLACIAN CONDITIONAL ESTIMATION                       ********************\n ********************                      INVERSE COVARIANCE MATRIX OF ESTIMATE                     ********************\n ********************                                                                                ********************\n ************************************************************************************************************************\n \n\n            TH 1      TH 2      OM11      SG11  \n \n TH 1\n+        2.48E+03\n \n TH 2\n+       ......... .........\n \n OM11\n+       -8.32E+02 .........  7.01E+02\n \n SG11\n+       ......... ......... ......... .........\n \n1\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n ************************************************************************************************************************\n ********************                                                                                ********************\n ********************                         LAPLACIAN CONDITIONAL ESTIMATION                       ********************\n ********************                      EIGENVALUES OF COR MATRIX OF ESTIMATE                     ********************\n ********************                                                                                ********************\n ************************************************************************************************************************\n \n\n             1         2\n \n         3.68E-01  1.63E+00\n \n Elapsed finaloutput time in seconds:     0.01\n #CPUT: Total CPU Time in Seconds,        0.016\nStop Time: \nMon 12/30/2024 \n10:54 PM\n\n\n\n\nIt is still quite a long file and a lot of text, that’s why the output is collapsed.\n\n\n4.2.2 PSN sumo (run summary)\nAs a next step we can run the PsN sumo command to get a quick run summary. It tells us if the minimization was successful, if there has been any rounding errors, zero gradients, and so on.\n\n\n\n\n\n\nFigure 3: PsN sumo output summarizing the minimization process.\n\n\n\nIn this screenshot we can also already see our maximum likelihood estimates for THETA and OMEGA and their associated relative standard errors.\n\n\n4.2.3 Iteration information\nBut which steps did the NONMEM algorithm take to end up in the maximum likelihood estimate? We can read in the .ext file, which contains information about the iterations of the estimation process and also the objective function value at each iteration. Here is how it looks like:\n\n\nCode\n# Read the data, skipping the first line\next_file &lt;- read_table(paste0(base_path, \"models/estimation/1cmt_iv_est.ext\"), skip = 1)\n\n# keep only positive iterations\next_file &lt;- ext_file |&gt; \n  filter(ITERATION &gt;= 0)\n\n# rename columns\next_file &lt;- ext_file |&gt; \n  rename(\n    CL = \"THETA1\",\n    V = \"THETA2\",\n    RUV_VAR = \"SIGMA(1,1)\",\n    IIV_VAR = \"OMEGA(1,1)\"\n  )\n\n# Show the tibble\next_file |&gt; \n  head(n=10) |&gt; \n  mytbl()\n\n\n\n\n\nITERATION\nCL\nV\nRUV_VAR\nIIV_VAR\nOBJ\n\n\n\n\n0\n0.100000\n3.15\n0.1\n0.1500000\n41.845437\n\n\n1\n0.168370\n3.15\n0.1\n0.4839690\n-2.743076\n\n\n2\n0.163689\n3.15\n0.1\n0.3881430\n-3.068198\n\n\n3\n0.216552\n3.15\n0.1\n0.1465500\n-11.297115\n\n\n4\n0.216552\n3.15\n0.1\n0.1465500\n-11.297115\n\n\n5\n0.253493\n3.15\n0.1\n0.0862428\n-12.418728\n\n\n6\n0.243287\n3.15\n0.1\n0.1171540\n-12.789118\n\n\n7\n0.246139\n3.15\n0.1\n0.1111950\n-12.823615\n\n\n8\n0.246715\n3.15\n0.1\n0.1099530\n-12.824589\n\n\n9\n0.246682\n3.15\n0.1\n0.1100200\n-12.824593\n\n\n\n\n\n\n\nIn total, we have 10 entries/rows in the .ext file. The first row, iteration 0, contains the initial estimates (which we have provided in the model code) and its associated objective function value. Then we have iterations 1-8, which are intermediate steps. Finally, we are going to end up in iteration 9, at which point the convergence criterium was fulfilled and the estimation process has ended.\nThe columns carry the following information:\n\nITERATION = iteration number\nCL = Typical value of clearance in the population\nV = Volume of distribution (fixed)\nRUV_VAR = Variance of the residual unexplained variability (fixed)\nIIV_VAR = Variance of the inter-individual variability\nOBJ = Objective function value\n\nWe can use this output file to visualize the change in parameter values and objective function value over the iteration number, so we get a better understanding what is going on during the estimation steps:\n\n\nCode\n# Visualize\next_file |&gt; \n  pivot_longer(cols = c(CL, V, RUV_VAR, IIV_VAR, OBJ), names_to = \"parameter\", values_to = \"value\") |&gt;\n  ggplot(aes(x=ITERATION, y=value))+\n  geom_line()+\n  geom_point()+\n  facet_wrap(~parameter, scales=\"free\")+\n  theme_bw()+\n  labs(x=\"Iteration\", y=\"Estimation diagnostic\")+\n  ggtitle(\"Estimation diagnostics over iterations\")\n\n\n\n\n\n\n\n\nFigure 4: Iterative diagnostics of the NONMEM estimation process, showing the progression of clearance, inter-individual variability, and objective function values over 10 iterations. Fixed parameters (RUV_VAR, V) remain constant.\n\n\n\n\n\nIn this exercise, the reproduction of the objective function value for a given set of parameters will be the main goal and this .ext file provides us with the reference solution. I actually don’t want to go down the rabbit hole of trying to reproduce the math behind these optimization algorithms. However, in Section 8, we are going to at least use the optim function in R to partly reproduce the search algorithm.\nIn the next big chapter we will try to understand the theory behind calculating the log-likelihood needed for the objective function based on our simple example."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-theory-statmod",
    "href": "posts/understanding_nlme_estimation/index.html#sec-theory-statmod",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "5.1 Statistical model",
    "text": "5.1 Statistical model\nIn our little example we assume to have a (simple) hierarchical nonlinear mixed-effects (NLME) model, for which we want to conduct the parameter estimation. To my understanding the hierarchical structure is given by having variability defined on a population (=parameter) level and an individual (=observation) level, while the individual level depends on the parameter level. Let’s have a closer look to both of these levels.\n\n5.1.1 Population (parameter) level\nThe population level is represented by an inter-individual variability (IIV) term, which assumes a log-normal distribution around a typical parameter value. In this simplified example we only consider IIV on clearance and do not consider any other random effects. The population (or parameter) level can be defined as follows:\n\\[CL_i = \\theta_{TVCL} \\cdot e^{\\eta_{i}},~~~~~\\eta_{i} \\sim N(0, \\omega^2) \\tag{1}\\]\nHere, the individual clearance (\\(CL_i\\)) is modeled as a log-normally distributed parameter, where (\\(\\theta_{\\text{TVCL}}\\)) is the typical clearance value and \\(\\eta_{i}\\) is a random effect accounting for inter-individual variability (IIV). We assume that this random effect follows a normal distribution with mean zero and variance \\(\\omega^2\\). An example plot of such a population level is shown below:\n\n\nCode\n# Set seed for reproducibility\nset.seed(123)\n\n# Define population parameters\ntheta_TVCL &lt;- 10  # Typical clearance (L/h)\nomega_sq &lt;- 0.2   # Variance of IIV on clearance\nomega &lt;- sqrt(omega_sq)  # Standard deviation of IIV\nnum_individuals &lt;- 5000    # Number of individuals in the population\n\n# Simulate individual random effects\neta_i &lt;- rnorm(num_individuals, mean = 0, sd = omega)\n\n# Compute individual clearances\nCL_i &lt;- theta_TVCL * exp(eta_i)\n\n# Create a data frame for plotting\npopulation_data &lt;- data.frame(\n  Individual = 1:num_individuals,\n  eta_i = eta_i,\n  CL = CL_i\n)\n\n# Calculate density for annotation placement\ndensity_CL &lt;- density(CL_i)\nmax_density &lt;- max(density_CL$y)\n\n# Create the ggplot\np_pop &lt;- ggplot(population_data, aes(x = CL)) +\n  # Histogram of clearance values\n  geom_histogram(aes(y = ..density..), binwidth = 0.5, fill = \"lightgreen\", color = \"black\", alpha = 0.7) +\n  # Density curve\n  geom_density(color = \"darkgreen\", size = 1) +\n  # Vertical line for typical clearance\n  geom_vline(xintercept = theta_TVCL, color = \"blue\", linetype = \"dashed\", size = 1) +\n  # Labels and theme\n  labs(\n    title = \"Population level: Clearance\",\n    x = \"Clearance (L/h)\",\n    y = \"Density\"\n  ) +\n  theme_bw() +\n  # Annotation for Typical Clearance\n  annotate(\"text\", x = theta_TVCL + 5, y = max_density * 0.9,\n           label = expression(theta[TVCL]~\": Typical Clearance\"),\n           color = \"blue\", hjust = 0) +\n  geom_segment(aes(x = theta_TVCL + 5, y = max_density * 0.9,\n                   xend = theta_TVCL, yend = max_density * 0.8),\n               arrow = arrow(length = unit(0.2, \"cm\")), color = \"blue\") +\n  # Annotation for IIV\n  annotate(\"text\", x = theta_TVCL + 13, y = max_density * 0.6,\n           label = expression(IIV~\"(\"~omega^2~\")\"),\n           color = \"darkgreen\", hjust = 0.5) +\n  geom_segment(aes(x = theta_TVCL + 10, y = max_density * 0.6,\n                   xend = theta_TVCL+3, yend = max_density * 0.55),\n               arrow = arrow(length = unit(0.2, \"cm\")), color = \"darkgreen\")\n\n# Display the plot\nprint(p_pop)\n\n\n\n\n\n\n\n\nFigure 5: Distribution of clearance values at the population level, modeled as a log-normal distribution. The dashed blue line indicates the typical clearance value , while the green histogram and curve represent the density of clearance values in the population, accounting for inter-individual variability.\n\n\n\n\n\nThis plot represents the population level of our model, where the clearance values are sampled from a log-normal distribution around the typical clearance value. The dashed line represents the typical clearance value \\(\\theta_{TVCL}\\), and the green curve/bars represents the distribution of clearances in the population.\nThe random effect \\(\\eta_i\\) itself follows a normal distribution \\(N(0, \\omega^2)\\) and is visualized below:\n\n\nCode\n# plot histogram\npopulation_data |&gt; \n  ggplot(aes(x=eta_i)) +\n  geom_histogram(aes(y = ..density..), color=\"black\", fill=\"lightblue\", binwidth = 0.05, alpha=0.7) +\n  labs(x=\"ETA\", y = \"Density\", title = \"Population level: ETA\",)+\n  geom_vline(xintercept = 0, linetype = \"dashed\", linewidth = 1)+\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure 6: Distribution of ETA values at the population level, following a normal distribution centered around 0. The dashed line indicates 0.\n\n\n\n\n\n\n\n5.1.2 Individual (observation) level\nThe individual level on the other hand is defined by the observed and predicted concentrations for each subject. The predictions are based on the structural model and dependent on the individual parameters (which can be treated as a random variable, in our case CL). The individual level also incorporates residual unexplained variability (RUV), which distribution tells us how to define the likelihood function in the end. The individual level can be defined by:\n\\[Y_{ij} \\mid CL_i = f(x_{ij}; CL_i) + \\epsilon_{ij},~~~~~\\epsilon_{ij} \\sim N(0, \\sigma^2) \\tag{2}\\] where we can note that:\n\n\\(Y_{ij}\\) is the observed concentration for the \\(i^{th}\\) individual at the \\(j^{th}\\) time point, conditionally distributed given the individual’s clearance \\(CL_i\\).\n\\(f(x_{ij}; CL_i)\\) is the predicted concentration. It depends on \\(CL_i\\) (the individual clearance) and \\(x_{ij}\\) (all the information about covariates, dosing and sampling events for the \\(i^{th}\\) individual at the \\(j^{th}\\) time point).\n\\(\\epsilon_{ij}\\) is the realization of the residual unexplained variability for the \\(i^{th}\\) individual at the \\(j^{th}\\) time point. It typically follows a normal distribution \\(N(0, \\sigma^2)\\)\n\nIn our example we have two random variables, \\(Y_{ij}\\) and \\(CL_i\\), with parameters \\(\\beta := (\\theta_{TVCL}, \\omega^2, \\sigma^2)\\). In our example we just want to estimate the typical clearance \\(\\theta_{TVCL}\\) and the IIV on clearance \\(\\omega^2\\). The residual unexplained variability \\(\\sigma^2\\) is assumed to be known and fixed. The individual level with RUV is illustrated below:\n\n\nCode\n# Set seed for reproducibility\nset.seed(123)\n\n# Define model parameters\ntheta_TVCL &lt;- 10  # Typical clearance (L/h)\nomega &lt;- 0.447    # IIV on clearance (sqrt(omega^2) where omega^2 = 0.2)\nsigma &lt;- 0.5      # Residual unexplained variability (standard deviation)\nV &lt;- 20           # Volume of distribution (L)\nDose &lt;- 100       # Dose administered (mg)\n\n# Simulate data for one individual\nindividual_id &lt;- 1\neta_i &lt;- rnorm(1, mean = 0, sd = omega)  # Individual random effect\nCL_i &lt;- theta_TVCL * exp(eta_i)          # Individual clearance\n\n# Define time points\ntime &lt;- seq(0, 10, by = 1)  # From 0 to 10 hours\n\n# Compute predicted concentrations based on the 1 cmt model\nC_pred &lt;- (Dose / V) * exp(- (CL_i / V) * time)\n\n# Add residual unexplained variability\nepsilon_ij &lt;- rnorm(length(time), mean = 0, sd = sigma)\nC_obs &lt;- C_pred + epsilon_ij\n\n# Create a data frame\nind_lvl_data &lt;- data.frame(\n  Time = time,\n  Predicted = C_pred,\n  Observed = C_obs\n)\n\n# Compute the upper and lower bounds for the normal distribution around prediction\nind_lvl_data &lt;- ind_lvl_data |&gt;\n  mutate(\n    Upper = Predicted + sigma,\n    Lower = Predicted - sigma\n  )\n\n# Create the ggplot\np &lt;- ggplot(ind_lvl_data, aes(x = Time)) +\n  # Shaded area for normal distribution around prediction\n  geom_ribbon(aes(ymin = Lower, ymax = Upper), fill = \"lightblue\", alpha = 0.5) +\n  # Predicted concentration line\n  geom_line(aes(y = Predicted), color = \"black\", size = 1) +\n  # Observed data points\n  geom_point(aes(y = Observed), color = \"red\", size = 2) +\n  # Labels and theme\n  labs(\n    title = \"Individual level\",\n    x = \"Time (hours)\",\n    y = \"Concentration (mg/L)\"\n  ) +\n  theme_bw() +\n  # Adjusted annotation for f(x)\n  annotate(\"text\", x = 6, y = 1.8, label = \"f(x): Predicted Concentration\", color = \"black\", hjust = 0) +\n  geom_segment(aes(x = 7.2, y = 1.6, xend = 6, yend = 0.5),\n               arrow = arrow(length = unit(0.2, \"cm\")), color = \"black\") +\n  # Adjusted annotation for Y_ij\n  annotate(\"text\", x = 2, y = 4.15, label = \"Yij: Observed Concentration\", color = \"red\", hjust = 0) +\n  geom_segment(aes(x = 2, y = 4.1, xend = 1.1, yend = 4.15),\n               arrow = arrow(length = unit(0.2, \"cm\")), color = \"red\") +\n  # Adjusted annotation for Residual Variability\n  annotate(\"text\", x = 1.8, y = 0.6, label = \"Residual Unexplained Variability (σ)\", color = \"blue\", hjust = 0.5) +\n  geom_segment(aes(x = 2, y = 0.8, xend = 3, yend = 1),\n               arrow = arrow(length = unit(0.2, \"cm\")), color = \"blue\") +\n  # Add a legend manually\n  scale_fill_manual(\n    name = \"Components\",\n    values = c(\"lightblue\" = \"lightblue\"),\n    labels = c(\"±1σ around Prediction\")\n  ) +\n  guides(fill = guide_legend(override.aes = list(alpha = 0.5)))+\n  scale_x_continuous(breaks=seq(0,10,2))\n\n# Display the plot\nprint(p)\n\n\n\n\n\n\n\n\nFigure 7: Illustrative example of observed and predicted concentrations at the individual level, with residual unexplained variability shown as the shaded area around predictions.\n\n\n\n\n\nThis plot shows exemplary shows the predicted and observed concentrations as well as the residual unexplained variability around the prediction. It represents our individual or observation level of the model. This is a simple illustration; typically, datasets would not include negative concentrations and would be rather flagged as below the quantification limit."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-theory-mle",
    "href": "posts/understanding_nlme_estimation/index.html#sec-theory-mle",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "5.2 Maximum likelihood estimation",
    "text": "5.2 Maximum likelihood estimation\nIn our case, we have only two parameters to estimate: \\(\\theta_{TVCL}\\) and \\(\\omega^2\\). The overall goal is to infer the parameters of interest \\((\\hat{\\theta}_{TVCL}, \\hat{\\omega^2})\\) from our observed data \\(y_{1:n}\\). In this case, \\(y_{1}\\) would denote the vector of \\(m_i\\) observations for the first individual out of n total individuals. Ideally, we would like to infer the parameters by directly maximizing the complete data log-likelihood (\\(\\ln L\\)) function:\n\\[(\\hat{\\theta}_{TVCL}, \\hat{\\omega^2})_{ML} = \\underset{\\theta_{TVCL},~ \\omega^2}{\\mathrm{argmax}}~\\ln L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}, CL_{i:n}\\right) \\tag{3}\\]\nTo align more with the notation in Wang (2007), we can re-write the expression based on \\(\\eta_i\\) values instead of \\(CL_i\\). We can re-write:\n\\[(\\hat{\\theta}_{TVCL}, \\hat{\\omega^2})_{ML} = \\underset{\\theta_{TVCL},~ \\omega^2}{\\mathrm{argmax}}~\\ln L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}, \\eta_{i:m}\\right) \\tag{4}\\]\nPlease note that Equation 3 and Equation 4 represent the complete data log-likelihood, but more to that later in Section 5.2.2. The reason why we deal with log-likelihood is that it makes a lot of calculations a bit easier (e.g., products become sums). Furthermore, likelihood terms can become very small and this can lead to numerical difficulties. By taking the logarithm, we can avoid this issue.\nBefore we continue, let’s first remind ourselves what likelihood is all about and what is the difference compared to probability.\n\n5.2.1 Likelihood vs probability\nI personally see the difference between likelihood and probability as a matter of from which “direction” we are looking at the things. Probability is about looking forward (into the future): “If we have a fully specified model and set of parameters, what are the probabilities of certain future events happening?”. On the other hand, Likelihood is about looking backward: “Now that we have the data / made that particular observation, what is the most likely model or parameters that could have produced it?”\nLet’s take a simple example to visualize the likelihood. We assume that the height of a human being is normally distributed and we randomly picked a guy on the streets with a height of 180 cm. Which set of parameters is more likely to lead to this height measurement? A set of parameters that assumes a mean height of 170 cm and a standard deviation of 30 cm or a set of parameters that assumes a mean height of 190 cm and a standard deviation of 5 cm?\n\n\nCode\n# Define the parameter sets\nparams &lt;- data.frame(\n  parameter_set = c(\"Mean = 175 cm, SD = 30 cm\", \"Mean = 190 cm, SD = 5 cm\"),\n  mean = c(175, 190),\n  sd = c(30, 5)\n)\n\n# Define the range of heights for plotting\nx_values &lt;- seq(100, 250, by = 0.1)\n\n# Generate density data for each parameter set\ndensity_data &lt;- params |&gt;\n  rowwise() |&gt;\n  do(data.frame(\n    parameter_set = .$parameter_set,\n    x = x_values,\n    density = dnorm(x_values, mean = .$mean, sd = .$sd)\n  )) |&gt;\n  ungroup()\n\n# Calculate the density (likelihood) at 180 cm for each parameter set\nlikelihoods &lt;- params |&gt;\n  rowwise() |&gt;\n  mutate(density_at_180 = dnorm(180, mean = mean, sd = sd)) |&gt;\n  select(parameter_set, density_at_180)\n\n# Merge the likelihoods with the density data for plotting\ndensity_data &lt;- density_data |&gt;\n  left_join(likelihoods, by = \"parameter_set\")\n\n# Create the plot\nggplot(density_data, aes(x = x, y = density)) +\n  geom_line(color = \"darkblue\") +  # Plot the density curves\n  facet_wrap(~ parameter_set, ncol = 1) +  # Create separate panels\n  geom_vline(xintercept = 180, linetype = \"dashed\", color = \"red\") +  # Dashed line at 180 cm\n  geom_point(data = likelihoods, aes(x = 180, y = density_at_180), color = \"blue\", size = 2, pch = 8) +  # Point at 180 cm\n  geom_text(data = likelihoods,\n            aes(x = 180, y = density_at_180,\n                label = sprintf(\"Likelihood: %.5f\", density_at_180)),\n            hjust = 1.1, vjust = -0.5, color = \"blue\") +  # Likelihood label on the left\n  labs(title = \"Likelihood of Observing a Height of 180 cm\",\n       x = \"Height (cm)\",\n       y = \"Density\") +\n  theme_bw()  # Apply black-and-white theme\n\n\n\n\n\n\n\n\nFigure 8: Comparison of likelihoods for observing a height of 180 cm under two parameter sets: (1) mean = 175 cm, SD = 30 cm and (2) mean = 190 cm, SD = 5 cm.\n\n\n\n\n\nWe can see the parameters \\(\\mu = 175~cm\\) and \\(\\sigma = 30~cm\\) are more likely to have produced the observed height of 180 cm than the alternative set of parameters. The likelihood can be calculated with its respective probability density or probability mass function. More to that later. What becomes clear is that the concept of likelihood fundamentally requires observed data to be meaningful. And this fact leads to an issue when trying to calculate the joint likelihood for our NLME model.\n\n\n5.2.2 The problem with the joint likelihood\nWe are trying to estimate the joint log-likelihood \\(\\ln L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}, \\eta_{i:n}\\right)\\), which is the likelihood of the parameters \\(\\theta_{TVCL}\\) and \\(\\omega^2\\) given that we have observed \\(y_{1:n}\\) and \\(\\eta_{i:n}\\). Now we have a problem. While we directly observe \\(y_{1:n}\\), we do not observe \\(\\eta_i\\) (or its respective \\(CL_i\\)) values directly. In other words: You will never receive a dataset where you have an “observed” individual clearance or an “observed” individual random effect parameter. This is why \\(\\eta_i\\) can be called an unobserved latent variable. Previously we have defined the complete data log-likelihood in Equation 3 and Equation 4, which is the log-likelihood of both the observed data and the unobserved latent variables. But without observations for \\(\\eta_{i:n}\\) we cannot compute the complete data likelihood.\nNow what? Our approach would be to somehow get rid of the general dependence on \\(\\eta_i\\). This is where the so-called marginal likelihood comes into play, which does not longer depend on an \\(\\eta_i\\) observation. In our case the maximum likelihood estimates are based on the marginal likelihood:\n\\[(\\hat{\\theta}_{TVCL}, \\hat{\\omega^2})_{ML} = \\underset{\\theta_{TVCL}, \\omega^2}{\\mathrm{argmax}}~\\ln L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right) \\tag{5}\\]\nPlease note that we do not depend on an actual \\(\\eta_{i:n}\\) observation anymore, only on \\(y_{1:n}\\). To set up the actual equation let’s first rewrite the Likelihood as a probability:\n\\[L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right) = p(y_{1:n}; \\theta_{TVCL}, \\omega^2) \\tag{6}\\]\nAssuming that the observations \\(y_1, y_2, ..., y_n\\) are independent, the likelihood can be expressed as the product of the individual likelihoods:\n\\[L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right) = \\prod_{i=1}^n p(y_{i}; \\theta_{TVCL},~ \\omega^2) \\tag{7}\\]\nFor each individual, we have \\(m_i\\) observations and \\(y_{i}\\) represents a vector of these \\(m_i\\) observations. We can also write it more explicitly to avoid misunderstandings, again assuming independence across observations.\n\\[L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right) = \\prod_{i=1}^n \\prod_{j=1}^{m_i}  p(y_{ij}; \\theta_{TVCL},~ \\omega^2) \\tag{8}\\]\nTo align the likelihood expression with the structure of our model in Equation 1 and Equation 2, we need to reformulate Equation 8 to explicitly include the individual random effects \\(\\eta_i\\). This allows us to express the likelihood in terms of both the individual-level and population-level components, with which we can actually do calculations. In other words: Equation 8 does not yet reflect the specific hierarchical form of our model. By reformulating the likelihood (see next steps), we transform this generic form into a structure that directly incorporates our model’s individual and population components and allows us to use our previously specified model.\n\\[L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right) = \\prod_{i=1}^n \\int \\left( \\prod_{j=1}^{m_i} p(y_{ij}, \\eta_i; \\theta_{TVCL}, \\omega^2) \\right) \\cdot d\\eta_i \\tag{9}\\]\nIn this first step, we are integrating over all possible values of \\(\\eta_i\\) (since we can’t directly observe \\(\\eta_i\\)). We can now further split this marginal likelihood equation by using the chain rule of probability, which brings us closer to the population and individual level structure of our model and closer to a state where we can use the model:\n\\[L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right) = \\prod_{i=1}^n \\int \\left( \\prod_{j=1}^{m_i} p(y_{ij}| \\eta_i; \\theta_{TVCL}, \\omega^2)\\right) \\cdot p(\\eta_i | \\theta_{TVCL}, \\omega^2) \\cdot d\\eta_i \\tag{10}\\]\nAs \\(p(y_{ij}| \\eta_i; \\theta_{TVCL}, \\omega^2)\\) does not actually depend on \\(\\omega^2\\), and \\(p(\\eta_i | \\theta_{TVCL}, \\omega^2)\\) does not actually depend on \\(\\theta_{TVCL}\\), we can then simplify the equation to:\n\\[L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right) = \\prod_{i=1}^n  \\int \\left( \\prod_{j=1}^{m_i} p(y_{ij}| \\eta_i; \\theta_{TVCL})\\right) \\cdot p(\\eta_i | \\omega^2) \\cdot d\\eta_i \\tag{11}\\]\nThe integral now consists of two parts: The individual level term \\(p(y_{ij}| \\eta_i; \\theta_{TVCL})\\) and the population level term \\(p(\\eta_i | \\omega^2)\\). The intuition behind this can be seen as something like this: For a given \\(\\eta_i\\) within the integral, the population term \\(p(\\eta_i |\\omega^2)\\) tells us how likely it is to observe this particular \\(\\eta_i\\) value in the population. The individual term \\(p(y_{ij}| \\eta_i; \\theta_{TVCL})\\) on the other hand tells us how likely it is to observe the \\(j^{th}\\) observation of the \\(i^{th}\\) individual given that particular \\(\\eta_i\\) value. The Likelihood will be maximal when the product of both terms is maximal, so that it is very likely to have this particular \\(\\eta_i\\) value in the population and that it is very likely to observe the set of \\(y_{ij}\\) values given this \\(\\eta_i\\) value.\nSo is it all good now? Kind of. We got rid of the dependence on directly observing \\(\\eta_i\\) for the complete data log-likelihood and have instead a nice marginal likelihood equation. However, solving the marginal likelihood is much harder due to this complex integral. So the next important task is to find a way to deal with this integral. And if it is hard to calculate, why not just approximate it?"
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-theory-intapprox",
    "href": "posts/understanding_nlme_estimation/index.html#sec-theory-intapprox",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "5.3 Approximating the integral",
    "text": "5.3 Approximating the integral\n\n5.3.1 Laplacian approximation\nSo far we’ve understood that we cannot solve the integral in the marginal likelihood equation easily. I am not sure if it is impossible or if it is just very hard to do so. But either way, in reality we need to approximate it somehow. To simplify things a bit, we will focus on the individual Likelihood \\(L_i\\) for now (for one single individual i) to avoid writing out the product (for the population) every time:\n\\[L_i\\left(\\theta_{TVCL}, \\omega^2| y_{i}\\right) = \\int \\left( \\prod_{j=1}^{m_i} p(y_{ij}| \\eta_i; \\theta_{TVCL})\\right) \\cdot p(\\eta_i | \\omega^2) \\cdot d\\eta_i \\tag{12}\\]\nIn the end, we have to take the product of all individual Likelihoods to get the likelihood for the population. Now we are going to tackle the integral. Apparently, one way of approximating integrals is the Laplacian approximation. It is a method which simplifies difficult integrals by focusing on the most important part (contribution-wise) of the function being integrated, which is the point where the function reaches its peak (the maximum or mode of the function). The idea is to approximate the function around this maximum point by a so-called second-order Taylor expansion. This approximation assumes that the integrand behaves like a Gaussian (bell-shaped) function around its maximum. The Taylor expansion with different orders (n = 1/2/3/10) is visualized here (2024):\n\n\n\n\n\n\nFigure 9: Visualization of second-order Taylor expansions (n=1,2,3,10) around the logarithmic function ln(x).\n\n\n\nNow, there seems to exist an useful feature of integrals that they often become analytically solvable if they take a certain form. If we can express the integral in the following exponential form:\n\\[\\int{\\exp(f(x))~dx} \\tag{13}\\]\nwhere \\(f(x)\\) is a second-order polynomial of the form \\(f(x) = ax^2 + bx + c\\) with a negative quadratic coefficient (\\(a &lt; 0\\)), and \\(f(x)\\) is twice differentiable with a unique maximum, we can solve it analytically. Therefore, the next step would be to bring our integral into this form, so we can make use of this nice property to kick out the annoying integral. That would allow us to get rid of any integration and just directly deal with the analytical solution.\n\n\n5.3.2 Bringing the expression in the right form\nTo bring our Likelihood expression into the right form, we can use a \\(\\exp(\\ln())\\) operator. In total this won’t do anything, but it allows us to simplify our inner expression a little, as products turn into sums due to the logarithm. And (as mentioned above), we need the exponential form of Equation 13 to solve the integral analytically. Thus, we take Equation 12 and apply the \\(\\exp(\\ln())\\) operator, which leads to a re-expression of the individual Likelihood as:\n\\[L_i\\left(\\theta_{TVCL}, \\omega^2| y_{i}\\right) = \\int \\exp \\left(\\ln \\left(\\left( \\prod_{j=1}^{m_i} p(y_{ij}| \\eta_i; \\theta_{TVCL})\\right)\\cdot p(\\eta_i | \\omega^2)\\right)\\right) \\cdot d\\eta_i \\tag{14}\\]\nAfter applying the log to each element, we get a sum:\n\\[L_i\\left(\\theta_{TVCL}, \\omega^2| y_{i}\\right) = \\int \\exp \\left(\\ln \\left(\\left( \\prod_{j=1}^{m_i} p(y_{ij}| \\eta_i; \\theta_{TVCL})\\right)\\right) + \\ln \\left(p(\\eta_i | \\omega^2)\\right)\\right)\\cdot d\\eta_i \\tag{15}\\]\nAlso the product sum of the individual level term turns into a sum:\n\\[L_i\\left(\\theta_{TVCL}, \\omega^2| y_{i}\\right) = \\int \\exp \\left( \\left(\\sum_{j=1}^{m_i}  \\ln \\left(p(y_{ij}| \\eta_i; \\theta_{TVCL})\\right)\\right) + \\ln \\left(p(\\eta_i | \\omega^2)\\right)\\right) \\cdot d\\eta_i \\tag{16}\\]\nWe can now substitute the inner term with a function \\(g_i(\\eta_i)\\), which allows us to write:\n\\[L_i\\left(\\theta_{TVCL}, \\omega^2| y_{i}\\right) = \\int \\exp\\left(g_i(\\eta_i)\\right) \\cdot d\\eta_i \\tag{17}\\]\nwith\n\\[g_i(\\eta_i) = \\left(\\sum_{j=1}^{m_i}  \\ln \\left(p(y_{ij}| \\eta_i; \\theta_{TVCL})\\right)\\right) + \\ln \\left(p(\\eta_i | \\omega^2)\\right) \\tag{18}\\]\nWe have successfully brought our expression into a format which allows us to later get rid of the integral itself. Please note the similarity between Equation 13 and Equation 17. In a next step we want to approximate \\(g_i(\\eta_i)\\) as a Gaussian (second-order polynomial) function around its maximum point (mode) \\(\\eta_i^*\\) via a Taylor expansion.\n\n\n5.3.3 Taylor expansion\nOkay, let’s approximate \\(g_i(\\eta_i)\\) by a second order Taylor expansion of \\(g_i(\\eta_i)\\) at point \\(\\eta_i^*\\), as we cannot explicitly and directly calculate that integral1. A second-order Taylor expansion at point \\(\\eta_i^*\\) is given by the following expression:\n\\[g_i(\\eta_i) \\approx g_i(\\eta_i^*) + g_i'(\\eta_i^*) \\cdot (\\eta_i - \\eta_i^*) + \\frac{1}{2} g_i''(\\eta_i^*)(\\eta_i-\\eta_i^*)^2 \\tag{19}\\]\nDuring Laplacian estimation, we want to choose \\(\\eta_i^*\\) so that we are in the mode of \\(g_i\\). First of all, around this point we have the biggest contribution to the integral. Since a Taylor approximation is always most accurate at the point for which we are expanding it, it makes sense that this should be the point at which the integral is the most sensitive to. Second, the mode is the point where the first derivative (\\(g_i'(\\eta_i^*)\\)) is zero. Therefore, it allows us to drop the second term of the Taylor expansion:\n\\[g_i(\\eta_i) \\approx g_i(\\eta_i^*) + \\frac{1}{2} g_i''(\\eta_i^*)(\\eta_i-\\eta_i^*)^2 \\tag{20}\\]\ngiven that\n\\[\\eta_i^* = \\underset{\\eta_i}{\\mathrm{argmax}}\\left[g_i(\\eta_i)\\right] = \\underset{\\eta_i}{\\mathrm{argmax}}\\left[\\left(\\sum_{j=1}^{m_i}  \\ln \\left(p(y_{ij}| \\eta_i; \\theta_{TVCL})\\right)\\right) + \\ln \\left(p(\\eta_i | \\omega^2)\\right)\\right] \\tag{21}\\]\nGreat! Now we have approximated the complex expression \\(g_i(\\eta_i)\\) and can now try to get rid of the integral in a next step.\n\n\n5.3.4 Kicking out the integral\nOkay, so the first thing we do is to take Equation 17 and substitute \\(g_i(\\eta_i)\\) with the 2nd-order approximation we obtained via Equation 20. This gives us:\n\\[L_i\\left(\\theta_{TVCL}, \\omega^2| y_{i}\\right) = \\int \\exp\\left(g_i(\\eta_i^*) + \\frac{1}{2} g_i''(\\eta_i^*)(\\eta_i-\\eta_i^*)^2\\right) \\cdot d\\eta_i \\tag{22}\\]\nOur first goal is to isolate expressions which are dependent on \\(\\eta_i\\) and those which are not. This will later allow us to get rid of the integral. The term \\(g_i(\\eta_i^*)\\) is independent on \\(\\eta_i\\) (it is just being evaluated at the mode \\(\\eta_i^*\\)), while the term \\(\\frac{1}{2} g_i''(\\eta_i^*)(\\eta_i-\\eta_i^*)^2\\) is actually dependent on \\(\\eta_i\\). Therefore, we are now going to split the expression into two parts:\n\\[L_i\\left(\\theta_{TVCL}, \\omega^2| y_{i}\\right) = \\int \\exp\\left(g_i(\\eta_i^*)\\right) \\cdot \\exp\\left( \\frac{1}{2} g_i''(\\eta_i^*)(\\eta_i-\\eta_i^*)^2\\right) \\cdot d\\eta_i \\tag{23}\\]\nNow in order that our plan (kicking out the integral) works, we need to ensure that the exponent of the second term is negative. This is important, because we want to approximate the integral as a Gaussian function, which is only possible if this exponent is negative. Technically, we can be sure that this is the case, because we are expanding around the mode and the second derivative at this point is negative for our function. However, since I want to align more with the reference solution in Wang (2007), I re-write the expression in a way that makes this more explicit:\n\\[L_i\\left(\\theta_{TVCL}, \\omega^2| y_{i}\\right) = \\int \\exp\\left(g_i(\\eta_i^*)\\right) \\cdot \\exp\\left( -\\frac{1}{2} \\left| g_i''(\\eta_i^*)\\right| (\\eta_i-\\eta_i^*)^2\\right) \\cdot d\\eta_i \\tag{24}\\]\nWe just introduced a negative sign and took the absolute value of the second derivative. Now we can substitute \\(g_i(\\eta_i^*)\\) with the respective expression given in Equation 18 (and evaluated at the mode \\(\\eta_i^*\\)) and get:\n\\[\\begin{split} L_i\\left(\\theta_{TVCL}, \\omega^2| y_{i}\\right) = \\int \\exp\\left(\\left(\\sum_{j=1}^{m_i}  \\ln \\left(p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right)\\right) + \\ln \\left(p(\\eta_i^* | \\omega^2)\\right)\\right) \\\\\n\\cdot \\exp\\left( -\\frac{1}{2} \\left| g_i''(\\eta_i^*)\\right| (\\eta_i-\\eta_i^*)^2\\right) \\cdot d\\eta_i \\end{split} \\tag{25}\\]\nNow the term \\(\\exp\\left(\\left(\\sum_{j=1}^{m_i}  \\ln \\left(p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right)\\right) + \\ln \\left(p(\\eta_i^* | \\omega^2)\\right)\\right)\\) does not depend anymore on \\(\\eta_i\\) (over which we are integrating), since the expression is just evaluated at a given value of \\(\\eta_i^*\\). This means it is a constant and can be factored out of the integral:\n\\[ \\begin{split}  L_i\\left(\\theta_{TVCL}, \\omega^2| y_{i}\\right) = \\exp\\left(\\left(\\sum_{j=1}^{m_i}  \\ln \\left(p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right)\\right) + \\ln \\left(p(\\eta_i^* | \\omega^2)\\right)\\right) \\\\ \\cdot\\int  \\exp\\left( -\\frac{1}{2} \\left| g_i''(\\eta_i^*)\\right| (\\eta_i-\\eta_i^*)^2\\right) \\cdot d\\eta_i \\end{split} \\tag{26}\\]\nRemember the trick we have used to bring the integral in the right form? This operator \\(\\exp(\\ln())\\) is not needed for the term which has been factored out, so we can simplify it to \\(\\left(\\prod_{j=1}^{m_i}  p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right) \\cdot p(\\eta_i^* | \\omega^2)\\). Please note, that the summation becomes a product again as we transform to the normal domain:\n\\[L_i\\left(\\theta_{TVCL}, \\omega^2| y_{i}\\right) = \\left(\\prod_{j=1}^{m_i}  p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right) \\cdot p(\\eta_i^* | \\omega^2) \\cdot \\int \\exp\\left( -\\frac{1}{2} \\left| g_i''(\\eta_i^*)\\right| (\\eta_i-\\eta_i^*)^2\\right) \\cdot d\\eta_i \\tag{27}\\]\nNow the whole plan of those people, who came up with this derivation, works out: The remaining integral is the integral of a Gaussian function and can be solved analytically. We now shortly just focus on the integral part of Equation 27:\n\\[\\int \\exp\\left( -\\frac{1}{2} \\left| g_i''(\\eta_i^*)\\right| (\\eta_i-\\eta_i^*)^2\\right) \\cdot d\\eta_i \\tag{28}\\]\nIn general, a Gaussian integral has the following form (and analytical solution):\n\\[\\int \\exp\\left(-\\frac{1}{2} \\frac{x^2}{\\sigma^2} \\right) dx = \\sqrt{2\\pi\\sigma^2} \\tag{29}\\]\nTo see how our expression fits this form, we notice that\n\\[-\\frac{1}{2} \\left| g_i''(\\eta_i^*)\\right| (\\eta_i-\\eta_i^*)^2 \\tag{30}\\]\ncan be written as \\(-\\frac{1}{2} x^2\\) via the substitution\n\\[x = \\sqrt{\\left| g_i''(\\eta_i^*)\\right|} (\\eta_i-\\eta_i^*), ~~ d\\eta_i = \\frac{1}{\\sqrt{\\left| g_i''(\\eta_i^*)\\right|}} \\cdot dx \\tag{31}\\]\nHence, we obtain:\n\\[\\int \\exp\\left(-\\frac{1}{2} \\left| g_i''(\\eta_i^*)\\right| (\\eta_i-\\eta_i^*)^2 \\right) \\cdot d\\eta_i = \\int \\exp\\left(-\\frac{1}{2} x^2\\right) \\cdot \\frac{1}{\\sqrt{\\left| g_i''(\\eta_i^*)\\right|}} dx \\tag{32}\\]\nwhich evaluates to\n\\[\\frac{1}{\\sqrt{\\left| g_i''(\\eta_i^*)\\right|}} \\cdot \\sqrt{2 \\pi} = \\sqrt{2 \\pi \\cdot \\frac{1}{\\left| g_i''(\\eta_i^*)\\right|}} = \\sqrt{\\frac{2\\pi}{\\left| g_i''(\\eta_i^*)\\right|}} \\tag{33}\\]\nFinally the magic happens. With all the work we have invested we can finally harvest the fruits. The integral disappears and we are left with a simple expression.\n\\[\\int \\exp\\left( -\\frac{1}{2} \\left| g_i''(\\eta_i^*)\\right| (\\eta_i-\\eta_i^*)^2\\right) \\cdot d\\eta_i = \\sqrt{2\\pi\\cdot \\frac{1}{\\left| g_i''(\\eta_i^*)\\right|} } = \\sqrt{\\frac{2\\pi}{\\left| g_i''(\\eta_i^*)\\right|}} \\tag{34}\\]\nTo my understanding we have now achieved an important goal by turning a hard-to calculate integral into an analytical expression, which is easier to evaluate. The only challenge remaining is the calculation of the second derivative \\(g_i''(\\eta_i^*)\\), which is not very straightforward. Substituting the simplified integral part from Equation 34 back into our individual likelihood expression (Equation 27), we get:\n\\[L_i\\left(\\theta_{TVCL}, \\omega^2| y_{i}\\right) = \\left(\\prod_{j=1}^{m_i}  p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right) \\cdot p(\\eta_i^*|\\omega^2) \\cdot \\sqrt{\\frac{2\\pi}{\\left| g_i''(\\eta_i^*)\\right|}} \\tag{35}\\]\nAlso here, we need to be careful. As mentioned above, \\(g_i''(\\eta_i^*)\\) is expected to be negative, since the function is concave and has a negative curvature at their maximum. I saw in Wang (2007) that they write \\(-g_i''(\\eta_i^*)\\), however, this should be the same as \\(|g_i''(\\eta_i^*)|\\) and I am going to stick to the absolute value out of convenience and because I find it a bit easier to follow.\nNow we can move a step further and translate it into something which is more familiar to us: The objective function in NONMEM. In the next section, we are going to spell out the missing expressions and also define the actual equation for the second derivative \\(g_i''(\\eta_i^*)\\)."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-theory-of",
    "href": "posts/understanding_nlme_estimation/index.html#sec-theory-of",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "5.4 Defining the Objective Function",
    "text": "5.4 Defining the Objective Function\n\n5.4.1 General\nOkay, so our next task is to define the objective function in NONMEM. We know that NONMEM calculates the -2 log likelihood (Bauer 2020). To my understanding the main reason to use the log of the Likelihood is to make it numerically more stable and the main reason to take the -2 is to make it asymptotically chi-square distributed (and thus, it allows some statistical testing). The negative sign turns our maximization problem into a minimization problem, which is mathematically easier to solve. The -2 log likelihood for a single individual is defined as:\n\\[OF_i = -2\\ln L_i\\left(\\theta_{TVCL}, \\omega^2| y_{i}\\right) \\tag{36}\\]\nFor the sake of simplicity we are still focusing on a single individual \\(i\\) and its contribution to the objective function. In the end, we would have to sum up the individual contributions to the objective function \\(OF_i\\) to get the final objective function. When we substitute the individual likelihood from Equation 35 into Equation 36, we get:\n\\[OF_i = -2\\ln\\left(\\left(\\prod_{j=1}^{m_i}  p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right) \\cdot p(\\eta_i^*|\\omega^2) \\cdot \\sqrt{\\frac{2\\pi}{\\left| g_i''(\\eta_i^*)\\right|}}\\right) \\tag{37}\\]\nWe can apply the \\(-2\\ln()\\) operation to each element and are left with:\n\\[OF_i = -2 \\ln \\left(\\prod_{j=1}^{m_i}  p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right) -2 \\ln \\left(p(\\eta_i^*|\\omega^2)\\right) -2 \\ln \\left(\\sqrt{\\frac{2\\pi}{\\left| g_i''(\\eta_i^*)\\right|}}\\right) \\tag{38}\\]\nThis can be re-written as:\n\\[OF_i = \\left(\\sum_{j=1}^{m_i} -2  \\ln\\left(p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right)\\right) -2 \\ln \\left(p(\\eta_i^*|\\omega^2)\\right) -2 \\ln \\left(\\sqrt{\\frac{2\\pi}{\\left| g_i''(\\eta_i^*)\\right|}}\\right) \\tag{39}\\]\nFrom here on it makes sense to split up the terms in order to better understand what is going on. We can identify three terms in the expression given by Equation 39:\n\nFirst term: \\(\\left(\\sum_{j=1}^{m_i} -2 \\ln\\left(p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right)\\right)\\)\nSecond term: \\(-2 \\ln \\left(p(\\eta_i^*|\\omega^2)\\right)\\)\nThird term: \\(-2 \\ln \\left(\\sqrt{\\frac{2\\pi}{\\left| g_i''(\\eta_i^*)\\right|}}\\right)\\)\n\nLet’s spell out each of these terms in more detail and see how they can be calculated.\n\n\n5.4.2 Term 1\nLet’s tackle the first term of Equation 39. It is given by:\n\\[\\left(\\sum_{j=1}^{m_i} -2  \\ln\\left(p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right)\\right) \\tag{40}\\]\nThe question we are asking is: “How likely is it to observe a certain set of data points \\(y_i\\) (= a series of observations for an individual) given the individual (most likely) parameter \\(\\eta_i^*\\) and \\(\\theta_{TVCL}\\)?” Please note: It seems that this expression is independent of \\(\\omega^2\\), but it is not. Our most likely \\(\\eta_i^*\\) is the mode of the distribution characterized by \\(\\omega^2\\), so we have a hidden dependence here. Typically, the our first term would also involve the residual unexplained variance (RUV) given by \\(\\sigma^2\\), but in our simple example it is fixed to a certain variance and thus, we don’t have to estimate it.\nWe assume a normal distribution (given by the RUV) around our model predictions \\(f(x_{ij}; \\theta_i)\\). Please refer to Figure 7 for a refresher. Now we can illustrate this concept for one single data point of a single individual (e.g., after 10 h a concentration of 2 mg/L was measured) as a case example to understand the concept a bit better:\n\n\nCode\n# Set seed for reproducibility (optional)\nset.seed(123)\n\n# Define fixed parameters\nDose &lt;- 100            # Dose administered\nV_D &lt;- 10              # Volume of distribution\neta_i &lt;- 0             # Individual random effect\nsigma &lt;- sqrt(0.1)     # Residual unexplained variance (standard deviation)\n\n# Define population parameters (Theta_TVCL)\nTheta_TVCL_values &lt;- c(10, 15, 20)\n\n# Define fixed time point (t)\nt &lt;- 1  # You can change this as needed\n\n# Compute the structural model predictions for each Theta_TVCL\n# Using the formula: C(t) = (Dose / V_D) * exp(-Theta_TVCL / V_D * t)\nmodel_predictions &lt;- data.frame(\n  Theta_TVCL = Theta_TVCL_values,\n  C_t = (Dose / V_D) * exp(-Theta_TVCL_values / V_D * t)\n)\n\n# Define the observed data point y_i\ny_i &lt;- 2\n\n# Create a sequence of y values for plotting the PDFs\ny_values &lt;- seq(min(model_predictions$C_t) - 3*sigma, \n               max(model_predictions$C_t) + 3*sigma, \n               length.out = 1000)\n\n# Create a data frame with density values for each Theta_TVCL\ndensity_data &lt;- model_predictions |&gt; \n  rowwise() |&gt;\n  do(data.frame(\n    Theta_TVCL = .$Theta_TVCL,\n    y = y_values,\n    density = dnorm(y_values, mean = .$C_t, sd = sigma)\n  )) |&gt;\n  ungroup()\n\n# Calculate the likelihood and -2 log likelihood for the observed y_i\nlikelihood_data &lt;- model_predictions |&gt;\n  mutate(\n    likelihood = dnorm(y_i, mean = C_t, sd = sigma),\n    neg2_log_likelihood = -2 * log(likelihood)\n  )\n\n# Merge likelihood data with density_data for annotation purposes\ndensity_data &lt;- density_data |&gt;\n  left_join(likelihood_data, by = \"Theta_TVCL\")\n\n# Create a named vector for custom facet labels\nfacet_labels &lt;- setNames(\n  paste(\"TVCL =\", Theta_TVCL_values),\n  Theta_TVCL_values\n)\n\n# Start plotting\nggplot(density_data, aes(x = y, y = density)) +\n  # Plot the density curves\n  geom_line(color = \"blue\", size = 1) +\n  \n  # Add a vertical line for the observed data point y_i\n  geom_vline(xintercept = y_i, linetype = \"dashed\", color = \"red\") +\n  \n  # Facet the plot by Theta_TVCL\n  facet_wrap(~ Theta_TVCL, scales = \"free_y\", labeller = as_labeller(facet_labels), nrow=3) +\n  \n  # Add annotations for the likelihood and -2 log likelihood\n  geom_text(data = density_data |&gt; distinct(Theta_TVCL, likelihood, neg2_log_likelihood),\n            aes(x = Inf, y = Inf, \n                label = paste0(\"Likelihood: \", round(likelihood, 6),\n                               \"\\n-2 ln(L): \", round(neg2_log_likelihood, 3))),\n            hjust = 1.1, vjust = 1.1, size = 4, color = \"black\") +\n  \n  # Customize labels and theme\n  labs(\n    title = \"Likelihood for different TVCL values\",\n    subtitle = paste(\"Observed data point yi =\", y_i, \", eta_i* = 0\"),\n    x = \"Concentration (C(t))\",\n    y = \"Probability Density\"\n  ) +\n  theme_bw() +\n  scale_y_continuous(limits=c(NA, 2))+\n  theme(\n    strip.text = element_text(size = 12, face = \"bold\"),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 12)\n  )\n\n\n\n\n\n\n\n\nFigure 10: Likelihood of observing a concentration of 2 mg/L at different typical clearance values.\n\n\n\n\n\nFor simplicity we have assumed an \\(\\eta_i^*\\) of 0 to generate this plot. We can see that we have observed a concentration of 2 mg/L at a given time point and want to know how likely it is to observe this particular concentration given different values of \\(\\theta_{TVCL}\\) with a fixed \\(\\eta_i^*\\). It becomes apparent that it is much more likely to have a clearance of 15 L/h given our data than to have a clearance of 10 or 20 L/h. But how can we explicitly calculate that likelihood given by Equation 40?\nSince we deal with a normal distribution, the likelihood is given by its probability density function (PDF). The general form is given by:\n\\[pdf(\\text{obs}) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\, \\exp\\left(-\\frac{(\\text{obs} - \\mu)^2}{2\\sigma^2}\\right) \\tag{41}\\]\nIn our case we would re-write it as follows for a single observation \\(y_{ij}\\):\n\\[p(y_{ij} | \\eta_i^*; \\theta_{TVCL}) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_{ij} - f(x_{ij}, \\theta_{TVCL}, \\eta_i^*))^2}{2\\sigma^2}\\right) \\tag{42}\\]\nTaking the log leads to:\n\\[\\ln \\left(p(y_{ij} | \\eta_i^*; \\theta_{TVCL})\\right) = -\\frac{1}{2} \\ln(2\\pi\\sigma^2) - \\frac{(y_{ij} - f(x_{ij}, \\theta_{TVCL}, \\eta_i^*))^2}{2\\sigma^2} \\tag{43}\\]\nMultiplying with -2 and simplifying leads to:\n\\[-2 \\ln \\left(p(y_{ij}  | \\eta_i^*; \\theta_{TVCL})\\right) = \\ln(2\\pi\\sigma^2) + \\frac{(y_{ij}  - f(x_{ij}, \\theta_{TVCL}, \\eta_i^*))^2}{\\sigma^2} \\tag{44}\\]\nIn a last step, we would have to take the sum of all observations to consider the likelihood contributions of all datapoints:\n\\[\\left(\\sum_{j=1}^{m_i} -2  \\ln\\left(p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right)\\right) = \\sum_{j=1}^{m_i} \\left[\\ln(2\\pi\\sigma^2) + \\frac{(y_{ij}  - f(x_{ij}, \\theta_{TVCL}, \\eta_i^*))^2}{\\sigma^2}\\right] \\tag{45}\\]\nGreat! If someone would give us a \\(\\theta_{TVCL}\\) value and an \\(\\eta_i^*\\) value, we would be able to calculate the -2 log likelihood term for that particular data point. Actually, we would need to have the model prediction function at hand for this (or an ODE-solver). The model prediction \\(f(x_{ij}, \\theta_{TVCL}, \\eta_i^*)\\) was already defined in Equation 69 (see above). It will be used at various points of the final objective function.\n\n\n5.4.3 Term 2\nA very similar concept applies to the second term of Equation 39. It is given by:\n\\[-2 \\ln \\left(p(\\eta_i^*|\\omega^2)\\right) \\tag{46}\\]\nWe typically assume that \\(\\eta_i\\) is normally distributed with a mean of 0 with a variance \\(\\omega^2\\). We could do a similar illustration as above, where we have a look at a certain \\(\\eta_i^*\\) value and want to know how likely it is to observe this value given the population variance \\(\\omega^2\\) (which we have at the moment of evaluation):\n\n\nCode\n# Set seed for reproducibility (optional)\nset.seed(123)\n\n# Define fixed parameters\neta_i_star &lt;- 0.3          # Observed eta_i value\nomega_squared_values &lt;- c(0.01, 0.2, 0.5)  # Population variances\n\n# Define the mean for eta_i (assumed to be 0)\nmu_eta &lt;- 0\n\n# Compute the structural model predictions for each omega^2\n# Using the formula: eta_i ~ N(0, omega^2)\nmodel_predictions &lt;- data.frame(\n  omega_squared = omega_squared_values,\n  mean = mu_eta\n)\n\n# Create a sequence of eta values for plotting the PDFs\n# Extending the range to cover the tails adequately\neta_values &lt;- seq(\n  mu_eta - 4 * sqrt(max(omega_squared_values)), \n  mu_eta + 4 * sqrt(max(omega_squared_values)), \n  length.out = 1000\n)\n\n# Create a data frame with density values for each omega^2\ndensity_data &lt;- model_predictions |&gt;\n  rowwise() |&gt;\n  do(data.frame(\n    omega_squared = .$omega_squared,\n    eta = eta_values,\n    density = dnorm(eta_values, mean = .$mean, sd = sqrt(.$omega_squared))\n  )) |&gt;\n  ungroup()\n\n# Calculate the likelihood and -2 log likelihood for the observed eta_i*\nlikelihood_data &lt;- model_predictions |&gt;\n  mutate(\n    likelihood = dnorm(eta_i_star, mean = mean, sd = sqrt(omega_squared)),\n    neg2_log_likelihood = -2 * log(likelihood)\n  )\n\n# Merge likelihood data with density_data for annotation purposes\ndensity_data &lt;- density_data |&gt;\n  left_join(likelihood_data, by = \"omega_squared\")\n\n# Create a named vector for custom facet labels\nfacet_labels &lt;- setNames(\n  paste(\"ω² =\", omega_squared_values),\n  omega_squared_values\n)\n\n# Start plotting\nggplot(density_data, aes(x = eta, y = density)) +\n  # Plot the density curves\n  geom_line(color = \"blue\", size = 1) +\n  \n  # Add a vertical line for the observed eta_i*\n  geom_vline(xintercept = eta_i_star, linetype = \"dashed\", color = \"red\") +\n  \n  # Highlight the point (eta_i*, density at eta_i*)\n  geom_point(\n    data = density_data |&gt; filter(abs(eta - eta_i_star) &lt; 1e-6), \n    aes(x = eta, y = density), \n    color = \"darkgreen\", size = 3  \n  ) +\n  \n  # Facet the plot by omega_squared with custom labels\n  facet_wrap(\n    ~ omega_squared, \n    scales = \"free_y\",\n    labeller = as_labeller(facet_labels),\n    nrow=3\n  ) +\n  \n  # Add annotations for the likelihood and -2 log likelihood\n  geom_text(\n    data = likelihood_data,\n    aes(\n      x = Inf, y = Inf, \n      label = paste0(\n        \"Likelihood: \", round(likelihood, 4),\n        \"\\n-2 log(L): \", round(neg2_log_likelihood, 2)\n      )\n    ),\n    hjust = 1.1, vjust = 1.1, size = 4, color = \"black\"\n  ) +\n  \n  # Customize labels and theme\n  labs(\n    title = \"Likelihood term for different ω² values\",\n    subtitle = paste(\"etai* =\", eta_i_star),\n    x = expression(eta),\n    y = \"Probability Density\"\n  ) +\n  theme_bw() +\n  scale_y_continuous(limits=c(NA, 4.5))+\n  theme(\n    strip.text = element_text(size = 12, face = \"bold\"),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 12)\n  )\n\n\n\n\n\n\n\n\nFigure 11: Likelihood of observing ETA=0.3 for different variances of the inter-individual variability term.\n\n\n\n\n\nIn our little example we can observe that the likelihood of “observing”2 \\(\\eta_i^* = 0.3\\) is much higher when the IIV variance is 0.2 compared to 0.01 or 0.5. This is the information we want to capture with this second likelihood term. Similarly to the first term of Equation 39, we are applying the general form of the pdf (Equation 41), which leads to this expression:\n\\[p(\\eta_i^*|\\omega^2) = \\frac{1}{\\sqrt{2\\pi\\omega^2}} \\exp\\left(-\\frac{(\\eta_i^* - 0)^2}{2\\omega^2}\\right) \\tag{47}\\]\nPlease note that the normal distribution is centered around 0 (\\(\\mu = 0\\)), which is in contrast to the individual level where it was centered around the predicted value \\(f(x_{ij}, \\theta_{TVCL}, \\eta_i^*)\\) (see Equation 42). Because of \\(\\mu = 0\\), we can simplify this to:\n\\[p(\\eta_i^*|\\omega^2) = \\frac{1}{\\sqrt{2\\pi\\omega^2}} \\exp\\left(-\\frac{\\eta_i^{*2}}{2\\omega^2}\\right) \\tag{48}\\]\nTaking the log:\n\\[\\ln \\left(p(\\eta_i^*|\\omega^2)\\right) = -\\frac{1}{2} \\ln(2\\pi\\omega^2) - \\frac{\\eta_i^{*2}}{2\\omega^2} \\tag{49}\\]\nMultiplying with -2 and simplifying:\n\\[-2 \\ln \\left(p(\\eta_i^*|\\omega^2)\\right) = \\ln(2\\pi\\omega^2) + \\frac{\\eta_i^{*2}}{\\omega^2} \\tag{50}\\]\nVery good. We have our population-level likelihood term. Similar to Term 1, we are now able to calculate this expression if someone would give us a value for \\(\\eta_i^*\\) and \\(\\omega^2\\). The remaining term is part 3 of the objective function, which is a bit more tricky. Let’s move on to this term.\n\n\n5.4.4 Term 3\nThe derivative term is the third and last term of Equation 39 and is given by\n\\[-2 \\ln \\left(\\sqrt{\\frac{2\\pi}{\\left| g_i''(\\eta_i^*)\\right|}}\\right) \\tag{51}\\]\nWe can take the square root out of the logarithm:\n\\[-2 \\ln \\left(\\sqrt{\\frac{2\\pi}{\\left| g_i''(\\eta_i^*)\\right|}}\\right) = -2 \\cdot \\frac{1}{2} \\cdot \\ln \\left(\\frac{2\\pi}{\\left| g_i''(\\eta_i^*)\\right|}\\right) \\tag{52}\\]\nWhich can be simplified to:\n\\[-2 \\ln \\left(\\sqrt{\\frac{2\\pi}{\\left| g_i''(\\eta_i^*)\\right|}}\\right) = - \\ln \\left(\\frac{2\\pi}{\\left| g_i''(\\eta_i^*)\\right|}\\right) \\tag{53}\\]\nThis expression can be further simplified by applying log-rules to:\n\\[-2 \\ln \\left(\\sqrt{\\frac{2\\pi}{\\left| g_i''(\\eta_i^*)\\right|}}\\right) = - \\ln \\left(2\\pi\\right) + \\ln\\left(\\left|g_i''(\\eta_i^*)\\right|\\right) \\tag{54}\\]\nWe still need to find a way to calculate the second derivative of the individual model function \\(g_i(\\eta_i^*)\\). However, defining the second derivative \\(g_i''(\\eta_i^*)\\) evaluated at \\(\\eta_i^*\\) is not straightforward and in many cases (once we deal with more complex models), this would be approximated numerically (e.g., using finite difference methods). We have luckily chosen a very simple model and can actually calculate the second derivative of the closed form expression. However, it is still quite complicated and cumbersome (at least to me). Therefore, we will simply use WolframAlpha / Mathematica to find these derivatives and help us out with the derivation rules.\nThe function \\(g_i(\\eta_i)\\) was already defined in Equation 18. Given that, the second derivative is expressed as the second derivatives of its two terms:\n\\[g_i''(\\eta_i^*) = \\left[\\left(\\sum_{j=1}^{m_i}  \\ln \\left(p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right)\\right)\\right]'' +  \\left[ln\\left(p(\\eta_i^*|\\omega^2)\\right)\\right]'' \\tag{55}\\]\nWe will use WolframAlpha to calculate the second derivative for both parts.\n\n5.4.4.1 First part of term 3\nLet’s start with the first part, which is given by:\n\\[\\left[\\left(\\sum_{j=1}^{m_i}  \\ln \\left(p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right)\\right)\\right]'' \\tag{56}\\]\nIt doesn’t matter whether we differentiate each term of a sum individually and then add the results, or add the terms first and then differentiate the sum as a whole. The result remain the same due to the linearity property of differentiation. For this reason, we first define the second derivative of a single term (by ignoring the sum). Our expression for \\(\\ln \\left(p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right)\\) is given in Equation 43.\nAccording to WolframAlpha, the second derivative of Equation 43 is given by:\n\\[\\begin{multline}\n\\left[\\ln \\left(p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right)\\right]'' = \\\\\n\\frac{-(f'(x_{ij}, \\theta_{TVCL}, \\eta_i^*))^2 + \\left(y_{ij} - f(x_{ij}, \\theta_{TVCL}, \\eta_i^*)\\right) \\cdot f''(x_{ij}, \\theta_{TVCL}, \\eta_i^*)}{\\sigma^2}\n\\end{multline} \\tag{57}\\]\nSubsequently, we have to take the sum of these derivatives over all \\(j\\) (see above):\n\\[\\begin{multline} \\left[\\left(\\sum_{j=1}^{m_i}  \\ln \\left(p(y_{ij}| \\eta_i; \\theta_{TVCL})\\right)\\right)\\right]'' = \\\\ \\sum_{j=1}^{m_i} \\left[ \\frac{-(f'(x_{ij}, \\theta_{TVCL}, \\eta_i^*))^2 + \\left(y_{ij} - f(x_{ij}, \\theta_{TVCL}, \\eta_i^*)\\right) \\cdot f''(x_{ij}, \\theta_{TVCL}, \\eta_i^*)}{\\sigma^2} \\right] \\end{multline} \\tag{58}\\]\nThe closed form expression for the model prediction function is defined in Equation 69. However, we haven’t defined its first and second derivative yet. Similar, we are going to use WolframAlpha to define these derivatives:\n\\[f'(x_{ij}, \\theta_{TVCL}, \\eta_i^*) = -\\frac{\\theta_{TVCL} \\cdot t_{ij} \\cdot e^{\\eta_i^*}}{V_D} \\cdot f(x_{ij}, \\theta_{TVCL}, \\eta_i^*) \\tag{59}\\]\n\\[f''(x_{ij}, \\theta_{TVCL}, \\eta_i^*) = \\frac{\\theta_{TVCL} \\cdot t_{ij} \\cdot e^{\\eta_i^*}}{V_D} \\cdot \\left( \\frac{\\theta_{TVCL} \\cdot t_{ij} \\cdot e^{\\eta_i^*}}{V_D} -1\\right) \\cdot f(x_{ij}, \\theta_{TVCL}, \\eta_i^*) \\tag{60}\\]\nThis provides us with everything we need for the first part of term 3.\n\n\n5.4.4.2 Second part of term 3\nThe second part of the second derivative is given by:\n\\[\\left[ln\\left(p(\\eta_i^*|\\omega^2)\\right)\\right]'' \\tag{61}\\]\nIts full expression is given by Equation 49 and the second derivative is defined as:\n\\[\\left[ln\\left(p(\\eta_i^*|\\omega^2)\\right)\\right]'' = - \\frac{1}{\\omega^2} \\tag{62}\\]\n\n\n5.4.4.3 Combining both parts\nNow we can substitute the terms in Equation 55 to get the full expression for \\(\\left[g_i(\\eta_i^*)\\right]''\\):\n\\[\\begin{multline} \\left[g_i(\\eta_i^*)\\right]'' = \\\\ \\sum_{j=1}^{m_i} \\left[ \\frac{-(f'(x_{ij}, \\theta_{TVCL}, \\eta_i^*))^2 + \\left(y_{ij} - f(x_{ij}, \\theta_{TVCL}, \\eta_i^*)\\right) \\cdot f''(x_{ij}, \\theta_{TVCL}, \\eta_i^*)}{\\sigma^2} \\right]\n- \\frac{1}{\\omega^2} \\end{multline} \\tag{63}\\]\nwhich depends on our previous definitions for \\(f'(x_{ij}, \\theta_{TVCL}, \\eta_i^*)\\) in Equation 59 and \\(f''(x_{ij}, \\theta_{TVCL}, \\eta_i^*)\\) in Equation 60.\nNow we have to pluck our results back into Equation 54 to finalize term 3:\n\\[\\begin{multline}  -2 \\ln \\left(\\sqrt{\\frac{2\\pi}{\\left| g_i''(\\eta_i^*)\\right|}}\\right) = - \\ln \\left(2\\pi\\right) + \\\\  \\ln\\left(\\left| \\sum_{j=1}^{m_i} \\left[ \\frac{-(f'(x_{ij}, \\theta_{TVCL}, \\eta_i^*))^2 + \\left(y_{ij} - f(x_{ij}, \\theta_{TVCL}, \\eta_i^*)\\right) \\cdot f''(x_{ij}, \\theta_{TVCL}, \\eta_i^*)}{\\sigma^2} \\right]\n- \\frac{1}{\\omega^2}  \\right|\\right) \\end{multline} \\tag{64}\\]\nGreat! We have found the final expression for the second derivative of \\(g_i(\\eta_i^*)\\).\n\n\n\n5.4.5 Defining \\(\\eta_i^*\\)\nOur first task is to find the particular value of \\(\\eta_i\\) which maximizes the expression in Equation 18. This should be nothing else than a Bayesian maximum a-posteriori (MAP) estimation (or empirical Bayes estimation (EBE)). We can find the mode of \\(g_i(\\eta_i)\\) by:\n\\[\\eta_i^* = \\underset{\\eta_i}{\\mathrm{argmax}}g_i(\\eta_i) = \\underset{\\eta_i}{\\mathrm{argmax}}\\left[\\left(\\sum_{j=1}^{m_i}  \\ln \\left(p(y_{ij}| \\eta_i; \\theta_{TVCL})\\right)\\right) + \\ln \\left(p(\\eta_i | \\omega^2)\\right)\\right] \\tag{65}\\]\nThis means that we are searching over all possible values for \\(\\eta_i\\) (per individual) and try to find the value that maximizes our log likelihood function. We have already previously defined \\(\\ln\\left(p(y_{ij} | \\eta_i; \\theta_{TVCL})\\right)\\) and \\(ln\\left(p(\\eta_i|\\omega^2)\\right)\\) in Equation 43 and Equation 49, respectively3. We can substitute these expressions into Equation 65:\n\\[\\eta_i^* = \\underset{\\eta_i}{\\mathrm{argmax}} ~\\left[\\sum_{j=1}^{m_i} \\left[-\\frac{1}{2} \\ln(2\\pi\\sigma^2) - \\frac{(y_{ij} - f(x_{ij}, \\theta_{TVCL}, \\eta_i))^2}{2\\sigma^2}\\right] -\\frac{1}{2} \\ln(2\\pi\\omega^2) - \\frac{\\eta_i^{2}}{2\\omega^2}\\right] \\tag{66}\\]\nNow this is an optimization (maximization) problem, so we don’t care about the constants. This simplifies the expression to:\n\\[\\eta_i^* = \\underset{\\eta_i}{\\mathrm{argmax}} ~\\left[\\sum_{j=1}^{m_i} \\left[ - \\frac{(y_{ij} - f(x_{ij}, \\theta_{TVCL}, \\eta_i))^2}{2\\sigma^2}\\right]  - \\frac{\\eta_i^{2}}{2\\omega^2}\\right] \\tag{67}\\]\nMost numerical optimization algorithms are designed to minimize functions, because it is conceptually simpler to identify a minimum than a maximum. As we will later reproduce this function in R, we are already now turning the maximization problem into a minimization problem by negation:\n\\[\\eta_i^* = \\underset{\\eta_i}{\\mathrm{argmin}} ~\\left[\\sum_{j=1}^{m_i} \\left[\\frac{(y_{ij} - f(x_{ij}, \\theta_{TVCL}, \\eta_i))^2}{2\\sigma^2}\\right]  + \\frac{\\eta_i^{2}}{2\\omega^2}\\right] \\tag{68}\\]\nIn our simple example, we luckily do not need to rely on ODE solvers (as we have an analytical solution at hand) and can later replace \\(f(x_{ij}, \\theta_{TVCL}, \\eta_i)\\) with the closed form expression of a 1 cmt iv bolus model:\n\\[f(x_{ij}, \\theta_{TVCL}, \\eta_i) = \\frac{Dose}{V_D} \\cdot \\exp(-\\frac{\\theta_{TVCL} \\cdot e^{\\eta_{i}}}{V_D}t_{ij}) \\tag{69}\\]\nEquation 68 needs to be numerically optimized for each individual at each iteration of the algorithm. This is typically done by using a numerical optimization algorithm. The optimization algorithm will search the parameter space to numerically find the value of \\(\\eta_i^*\\) that maximizes the function.\n\n\n5.4.6 Putting the pieces together\nPreviously, we have taken the three pieces of Equation 39 and expressed them in an explicit way. Let’s put them together. After simplification, our objective function is represented by:\n\\[OF_i = a + b + c \\tag{70}\\]\nwhere \\(a\\), \\(b\\), and \\(c\\) are defined based on Equation 45, Equation 50, and Equation 64:\n\\[a  = \\sum_{j=1}^{m_i} \\left[\\ln(2\\pi\\sigma^2) + \\frac{(y_{ij}  - f(x_{ij}, \\theta_{TVCL}, \\eta_i^*))^2}{\\sigma^2}\\right] \\tag{71}\\]\n\\[b = \\ln(2\\pi\\omega^2) + \\frac{\\eta_i^{*2}}{\\omega^2} \\tag{72}\\]\n\\[\\begin{multline} c = - \\ln \\left(2\\pi\\right) + \\\\  \\ln\\left(\\left| \\sum_{j=1}^{m_i} \\left[ \\frac{-(f'(x_{ij}, \\theta_{TVCL}, \\eta_i^*))^2 + \\left(y_{ij} - f(x_{ij}, \\theta_{TVCL}, \\eta_i^*)\\right) \\cdot f''(x_{ij}, \\theta_{TVCL}, \\eta_i^*)}{\\sigma^2} \\right] - \\frac{1}{\\omega^2}  \\right|\\right) \\end{multline} \\tag{73}\\]\nleading to\n\\[\\begin{multline} OF_i = \\sum_{j=1}^{m_i} \\left[\\ln(2\\pi\\sigma^2) + \\frac{(y_{ij}  - f(x_{ij}, \\theta_{TVCL}, \\eta_i^*))^2}{\\sigma^2}\\right] + \\ln(2\\pi\\omega^2) + \\frac{\\eta_i^{*2}}{\\omega^2}  - \\ln \\left(2\\pi\\right) +   \\\\ \\ln\\left(\\left| \\sum_{j=1}^{m_i} \\left[ \\frac{-(f'(x_{ij}, \\theta_{TVCL}, \\eta_i^*))^2 + \\left(y_{ij} - f(x_{ij}, \\theta_{TVCL}, \\eta_i^*)\\right) \\cdot f''(x_{ij}, \\theta_{TVCL}, \\eta_i^*)}{\\sigma^2} \\right] - \\frac{1}{\\omega^2}  \\right|\\right) \\end{multline} \\tag{74}\\]\nIt seems that NONMEM ignores all constants during the optimization process (Bauer (2020) and Wang (2007)), which is why we can get rid of the \\(\\ln(2\\pi)\\) terms. We can simplify to:\n\\[\\begin{multline} OF_i = \\sum_{j=1}^{m_i} \\left[\\ln(\\sigma^2) + \\frac{(y_{ij}  - f(x_{ij}, \\theta_{TVCL}, \\eta_i^*))^2}{\\sigma^2}\\right] + \\ln(\\omega^2) + \\frac{\\eta_i^{*2}}{\\omega^2} +   \\\\ \\ln\\left(\\left| \\sum_{j=1}^{m_i} \\left[ \\frac{-(f'(x_{ij}, \\theta_{TVCL}, \\eta_i^*))^2 + \\left(y_{ij} - f(x_{ij}, \\theta_{TVCL}, \\eta_i^*)\\right) \\cdot f''(x_{ij}, \\theta_{TVCL}, \\eta_i^*)}{\\sigma^2} \\right] - \\frac{1}{\\omega^2}  \\right|\\right) \\end{multline} \\tag{75}\\]\nPlease note that the objective function \\(OF_i\\) is a function with multiple input parameters. Technically, we would have to write something like this:\n\\[OF_i\\left(\\theta_{TVCL}, \\omega^2, \\sigma^2, y_i, \\eta_i^*, x_i = \\left[DOSE_i, V_D, t_i\\right]\\right) = (...)\\]\nto clearly state the dependencies. However, for the sake of readability we are simply denoting it \\(OF_i\\). The last step would be now to calculate the objective function for the whole population, including n individuals:\n\\[OF = \\sum_{i=1}^{n} OF_i \\tag{76}\\]\nVery good. We have now set up the final equation for our objective function and we can now reproduce these function in R!"
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-rrepro-func",
    "href": "posts/understanding_nlme_estimation/index.html#sec-rrepro-func",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "6.1 Function definitions",
    "text": "6.1 Function definitions\nIn order to reproduce the iteration of the NONMEM algorithm, we need to define a couple of functions inside R. Technically, we need all equations which are part of Equation 75. In the next steps we are going to define these functions in R.\n\n6.1.1 Structural model function and its derivatives\nThe first function we are defining in R is Equation 69, which gives us a prediction for a given \\(x_{ij}, \\theta_{TVCL}, \\eta_i\\) based on our structural model.\n\n\n\nfunction: f_pred()\n\n# Define structural model prediction function\nf_pred &lt;- function(eta_i, dose, vd, theta_tvcl, t) {\n  exp_eta_i &lt;- exp(eta_i)\n  exponent &lt;- -1 * (theta_tvcl * exp_eta_i / vd) * t\n  result &lt;- (dose / vd) * exp(exponent)\n  return(result)\n}\n\n\nWe also have to define the derivatives of f_pred(). This is the definition of the first derivative function (reproduction of Equation 59):\n\n\n\nfunction: f_prime()\n\n# Define the first derivative of the structural model prediction function\nf_prime &lt;- function(eta_i, dose, vd, theta_tvcl, t) {\n  term &lt;- (theta_tvcl * t * exp(eta_i)) / vd\n  derivative &lt;- -term * f_pred(eta_i, dose, vd, theta_tvcl, t)\n  return(derivative)\n}\n\n\nThis is the definition of the second derivative function (reproduction of Equation 60):\n\n\n\nfunction: f_double_prime()\n\n# Define the second derivative of the structural model prediction function\nf_double_prime &lt;- function(eta_i, dose, vd, theta_tvcl, t) {\n  term &lt;- (theta_tvcl * t * exp(eta_i)) / vd\n  second_derivative &lt;- term * (term - 1) * f_pred(eta_i, dose, vd, theta_tvcl, t)\n  return(second_derivative)\n}\n\n\nWe now need to pay attention what R is actually returning in these functions. For all individuals we have multiple data points, so t is going to be a vector. This will propagate then throughout the calculations and the results/derivative/second_derivative object, which is being returned, will also be a vector.\n\n\n6.1.2 Function to find the mode \\(\\eta_i^*\\)\nNow we need to define two functions to reproduce Equation 68. Since finding the mode \\(\\eta_i^*\\) is an optimization problem, we need to define it’s own objective function for which we are going to numerically solve for the maximum:\n\n\n\nfunction: obj_fun_eta_i_star()\n\n# Objective function for optimization to find eta_i_star\nobj_fun_eta_i_star &lt;- function(eta_i, y_i, t, theta_tvcl, vd, dose, sigma2, omega2, f_pred) {\n  f_pred_value &lt;- f_pred(eta_i = eta_i, dose = dose, vd = vd, theta_tvcl = theta_tvcl, t = t)\n  term1 &lt;- sum((y_i - f_pred_value)^2 / (2 * sigma2))\n  term2 &lt;- eta_i^2 / (2 * omega2)\n  obj_value &lt;- term1 + term2 \n  return(obj_value)\n}\n\n\nSince our optimizer function (optim) will actually minimize the objective function, we already previously turned the objective function into a minimization problem by negation (see Section 5.4.5). In the end it doesn’t matter if we maximize the positive version of the term or if we minimize the negative version given by Equation 68. The actual optimizer is then being encoded in a separate function and takes as an argument the MAP objective function we have defined earlier:\n\n\n\nfunction: compute_eta_i_star()\n\n# Define optimizer function to find eta_i_star\ncompute_eta_i_star &lt;- function(y_i, t, theta_tvcl, vd, dose, sigma2, omega2, f_pred, eta_i_init = 0, obj_fun_eta_i_star) {\n  res &lt;- optim(\n    par = eta_i_init,\n    fn = obj_fun_eta_i_star,\n    y_i = y_i,\n    t = t,\n    theta_tvcl = theta_tvcl,\n    vd = vd,\n    dose = dose,\n    sigma2 = sigma2,\n    omega2 = omega2,\n    f_pred = f_pred,\n    method = \"BFGS\"\n  )\n  eta_i_star &lt;- res$par\n  return(eta_i_star)\n}\n\n\nPlease note that the compute_eta_i_star() function takes as arguments the functions which we have defined prior to that: f_pred() and obj_fun_eta_i_star(). Another hint: We need to find \\(\\eta_i^*\\) for each individual. That’s why we’ll apply our function compute_eta_i_star() on an individual level and it will return a single value of \\(\\eta_i^*\\). Please note that each individual has multiple observations and therefore obj_fun_eta_i_star() calculates the sum of the squared residual terms.\n\n\n6.1.3 Second derivative function\nNow we need to define an R function for the second derivative \\(g_i''(\\eta_i)\\), which is given by Equation 64. Again, we need to take the sum of the likelihood term to get a single value for each individual with multiple observations. Even with multiple observations, the prior term is only being accounted for once. This explains why with a larger number of observations the likelihood is going to dominate the prior.\n\n\n\nfunction: gi_double_prime()\n\n# Define second derivative of gi\ngi_double_prime &lt;- function(eta_i, y_i, t, theta_tvcl, vd, dose, sigma2, omega2) {\n  # Calculate f_pred, f_prime, and f_double_prime for all time points\n  f_pred_val &lt;- f_pred(eta_i, dose, vd, theta_tvcl, t)\n  f_prime_val &lt;- f_prime(eta_i, dose, vd, theta_tvcl, t)\n  f_double_prime_val &lt;- f_double_prime(eta_i, dose, vd, theta_tvcl, t)\n  \n  # Compute the numerator for each observation\n  numerator &lt;- (-f_prime_val^2 + (y_i - f_pred_val) * f_double_prime_val)\n  \n  # Divide by sigma^2 for each observation\n  term &lt;- numerator / sigma2\n  \n  # Sum the terms across all observations for the individual\n  sum_term &lt;- sum(term)\n  \n  # Subtract the term 1 / omega^2\n  second_derivative &lt;- sum_term - (1 / omega2)\n  \n  # return the second derivative\n  return(second_derivative)\n}\n\n\nFrom here on, we only need to define one last function: the objective function, which brings all the pieces together.\n\n\n6.1.4 Objective function\nWhen defining the objective function (see Equation 75), we are going to rely on all other functions defined above. Please note, that the ofv_function defined below will calculate the objective function value for a single individual. Later on we will have to loop over all individuals and sum up the OFV as defined in Equation 76.\n\n\n\nfunction: ofv_function()\n\n# Define the objective function OFV\nofv_function &lt;- function(y_i, t, theta_tvcl, vd, dose, sigma2, omega2, f_pred, gi_double_prime, compute_eta_i_star, obj_fun_eta_i_star, eta_i_init = 0, return_val = \"ofv\") {\n  \n  # Compute eta_i_star\n  eta_i_star &lt;- compute_eta_i_star(\n    y_i = y_i,\n    t = t,\n    theta_tvcl = theta_tvcl,\n    vd = vd,\n    dose = dose,\n    sigma2 = sigma2,\n    omega2 = omega2,\n    f_pred = f_pred,\n    eta_i_init = eta_i_init,\n    obj_fun_eta_i_star = obj_fun_eta_i_star\n  )\n  \n  # Compute f(eta_i_star)\n  f_pred_value &lt;- f_pred(eta_i_star, dose, vd, theta_tvcl, t)\n  \n  # Compute residual term\n  residual &lt;- y_i - f_pred_value\n  residual_squared &lt;- residual^2\n  \n  # Compute the first term: ln(sigma^2)\n  term1 &lt;- length(y_i) * log(sigma2)\n  \n  # Compute the second term: (y_i - f(eta_i_star))^2 / sigma2\n  term2 &lt;- sum(residual_squared) / sigma2\n  \n  # Compute the third term: ln(omega^2)\n  term3 &lt;- log(omega2)\n  \n  # Compute the fourth term: (eta_i_star)^2 / omega2\n  term4 &lt;- (eta_i_star^2) / omega2\n  \n  # Compute gi''(eta_i_star)\n  gi_double_prime_value &lt;- gi_double_prime(\n    eta_i = eta_i_star,\n    y_i = y_i,\n    t = t,\n    theta_tvcl = theta_tvcl,\n    vd = vd,\n    dose = dose,\n    sigma2 = sigma2,\n    omega2 = omega2\n  )\n  \n  # Compute term5: ln of absolute value of gi_double_prime_value \n  term5 &lt;- log(abs(gi_double_prime_value))\n  \n  # Sum all terms to compute the objective function value\n  ofv &lt;- term1 + term2 + term3 + term4 + term5\n  \n  # return by default ofv, but we can also request to return the eta_i_star\n  if(return_val == \"ofv\"){\n    return(ofv)\n  } else if(return_val == \"eta_i_star\"){\n    return(eta_i_star)\n  }\n}\n\n\nGreat! Now we are all set to reproduce the objective function values for each iteration."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-rrepro-ofvrepro",
    "href": "posts/understanding_nlme_estimation/index.html#sec-rrepro-ofvrepro",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "6.2 Reproducing OFVs for iterations",
    "text": "6.2 Reproducing OFVs for iterations\nLet’s start with a little recap. Our simulated concentration-time data was given by this data frame:\n\n\nCode\n# show simulated data\nsim_data_reduced |&gt; \n  select(ID, TIME, DV) |&gt; \n  head() |&gt; \n  mytbl()\n\n\n\n\n\nID\nTIME\nDV\n\n\n\n\n1\n0.01\n30.8160\n\n\n1\n3.00\n24.5520\n\n\n1\n6.00\n17.9250\n\n\n1\n12.00\n10.1100\n\n\n1\n24.00\n3.5975\n\n\n2\n0.01\n31.4550\n\n\n\n\n\n\n\nThe following .ext file contained the information about the objective function values, which we are trying to reproduce in this chapter:\n\n\nCode\n# show first row\next_file |&gt;\n  mytbl()\n\n\n\n\n\nITERATION\nCL\nV\nRUV_VAR\nIIV_VAR\nOBJ\n\n\n\n\n0\n0.100000\n3.15\n0.1\n0.1500000\n41.845437\n\n\n1\n0.168370\n3.15\n0.1\n0.4839690\n-2.743076\n\n\n2\n0.163689\n3.15\n0.1\n0.3881430\n-3.068198\n\n\n3\n0.216552\n3.15\n0.1\n0.1465500\n-11.297115\n\n\n4\n0.216552\n3.15\n0.1\n0.1465500\n-11.297115\n\n\n5\n0.253493\n3.15\n0.1\n0.0862428\n-12.418728\n\n\n6\n0.243287\n3.15\n0.1\n0.1171540\n-12.789118\n\n\n7\n0.246139\n3.15\n0.1\n0.1111950\n-12.823615\n\n\n8\n0.246715\n3.15\n0.1\n0.1099530\n-12.824589\n\n\n9\n0.246682\n3.15\n0.1\n0.1100200\n-12.824593\n\n\n10\n0.246682\n3.15\n0.1\n0.1100200\n-12.824593\n\n\n\n\n\n\n\nNow we are going to loop over all iterations and calculate the objective function values for each individual. We will then sum up the individual objective function values to get the total objective function value for each iteration.\n\n\nCode\n# define empty tibble\nofv_values &lt;- tibble(ITERATION = numeric(), OBJ_R = numeric())\n\n# define constants\nDOSE &lt;- 100\n\n# loop over iterations\nfor(iter in 1:nrow(ext_file)){\n  \n  # retrieve information from first row\n  TVCL &lt;- ext_file |&gt; slice(iter) |&gt; pull(CL)\n  VD &lt;- ext_file |&gt; slice(iter) |&gt; pull(V)\n  RUV_VAR &lt;- ext_file |&gt; slice(iter) |&gt; pull(RUV_VAR)\n  IIV_VAR &lt;- ext_file |&gt; slice(iter) |&gt; pull(IIV_VAR)\n  \n  # create empty vector for individual ofv values\n  ofv_values_iter &lt;- numeric()\n  \n  # Loop over individuals\n  for(i in unique(sim_data_reduced$ID)){\n    \n    # Filter data for individual i\n    data_i &lt;- sim_data_reduced |&gt;\n      filter(ID == i)\n    \n    # Extract individual parameters\n    y_i &lt;- data_i$DV\n    t &lt;- data_i$TIME\n    \n    # Compute the objective function value\n    ofv_i &lt;- ofv_function(\n      y_i = y_i,\n      t = t,\n      theta_tvcl = TVCL,\n      vd = VD,\n      dose = DOSE,\n      sigma2 = RUV_VAR,\n      omega2 = IIV_VAR,\n      f_pred = f_pred,\n      gi_double_prime = gi_double_prime,\n      compute_eta_i_star = compute_eta_i_star,\n      obj_fun_eta_i_star = obj_fun_eta_i_star\n    )\n    \n    # append to vector\n    ofv_values_iter &lt;- c(ofv_values_iter, ofv_i)\n  }\n  \n  # calculate sum of ofv_values\n  OFV_sum &lt;- sum(ofv_values_iter)\n  \n  # store in tibble with add row\n  ofv_values &lt;- ofv_values |&gt;\n    add_row(ITERATION = iter-1, OBJ_R = OFV_sum)\n}\n\n# add columns of ofv_values (join) to ext_file\next_file &lt;- ext_file |&gt;\n  left_join(ofv_values, by = \"ITERATION\")\n\n\nWith the last lines of code, we have joined our estimated objective function values to the .ext file and can now compare the reproduced objective function values with the original ones:\n\n\nCode\n# show ext_file\next_file |&gt; \n  mutate(DIFF = OBJ_R - OBJ) |&gt; \n  mytbl()\n\n\n\n\n\nITERATION\nCL\nV\nRUV_VAR\nIIV_VAR\nOBJ\nOBJ_R\nDIFF\n\n\n\n\n0\n0.100000\n3.15\n0.1\n0.1500000\n41.845437\n41.845437\n0.00e+00\n\n\n1\n0.168370\n3.15\n0.1\n0.4839690\n-2.743076\n-2.743092\n-1.64e-05\n\n\n2\n0.163689\n3.15\n0.1\n0.3881430\n-3.068198\n-3.068141\n5.70e-05\n\n\n3\n0.216552\n3.15\n0.1\n0.1465500\n-11.297115\n-11.297124\n-9.10e-06\n\n\n4\n0.216552\n3.15\n0.1\n0.1465500\n-11.297115\n-11.297124\n-9.10e-06\n\n\n5\n0.253493\n3.15\n0.1\n0.0862428\n-12.418728\n-12.418735\n-7.90e-06\n\n\n6\n0.243287\n3.15\n0.1\n0.1171540\n-12.789118\n-12.789115\n3.20e-06\n\n\n7\n0.246139\n3.15\n0.1\n0.1111950\n-12.823615\n-12.823615\n2.00e-07\n\n\n8\n0.246715\n3.15\n0.1\n0.1099530\n-12.824589\n-12.824589\n-2.00e-07\n\n\n9\n0.246682\n3.15\n0.1\n0.1100200\n-12.824593\n-12.824594\n-1.00e-07\n\n\n10\n0.246682\n3.15\n0.1\n0.1100200\n-12.824593\n-12.824594\n-1.00e-07\n\n\n\n\n\n\n\nIn this table, the OBJ column represents the values from the NONMEM output, while OBJ_R represents our reproduced values. We can now compare the two columns to see if we have done a good job in reproducing the objective function values!\nIn general, I would say the reproduced values match the original values quite well. There are some small differences, mainly associated to the fifth digit after the decimal point. Of course there is always the possibility that I have made a mistake in the implementation, but I rather expect the differences to be related to e.g., the difference between analytical solution of the second derivative and the numerical approximation in NONMEM."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-surface-grid",
    "href": "posts/understanding_nlme_estimation/index.html#sec-surface-grid",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "7.1 Define the grid",
    "text": "7.1 Define the grid\nIn order to get a nice surface plot, we will plot all possible combination for \\(\\theta_{TVCL}\\) and \\(\\omega^2\\) within a certain range. Here is the head of the respective grid table:\n\n\nCode\n# Define the grid\ntheta_tvcl_values &lt;- seq(0.01, 0.6, length.out = 100)\nomega2_values &lt;- seq(0.01, 0.8, length.out = 100)\ngrid &lt;- expand.grid(theta_tvcl = theta_tvcl_values, omega2 = omega2_values)\n\n# show grid\ngrid |&gt; \n  head() |&gt; \n  mytbl()\n\n\n\n\n\ntheta_tvcl\nomega2\n\n\n\n\n0.0100000\n0.01\n\n\n0.0159596\n0.01\n\n\n0.0219192\n0.01\n\n\n0.0278788\n0.01\n\n\n0.0338384\n0.01\n\n\n0.0397980\n0.01\n\n\n\n\n\n\n\nIn a next step, we can calculate the objective function value for each grid element."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-surface-gridofv",
    "href": "posts/understanding_nlme_estimation/index.html#sec-surface-gridofv",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "7.2 Calculate OFV for each grid element",
    "text": "7.2 Calculate OFV for each grid element\nNow, we will loop over all grid elements and calculate the objective function value for each combination of \\(\\theta_{TVCL}\\) and \\(\\omega^2\\). We will then store the results in a tibble and convert it to a matrix for plotting.\n\n\nCode\n# Define empty tibble for grid + OFV values\nofv_grid_values &lt;- tibble(THETA_TVCL = numeric(), OMEGA2 = numeric(), OBJ = numeric())\n\n# Loop over grid elements\nfor(i in 1:nrow(grid)){\n  \n  # Extract theta_tvcl and omega2\n  theta_tvcl &lt;- grid |&gt; slice(i) |&gt; pull(theta_tvcl)\n  omega2 &lt;- grid |&gt; slice(i) |&gt; pull(omega2)\n  \n  # create empty vector for individual ofv values\n  ofv_values_iter &lt;- numeric()\n  \n  # Loop over individuals\n  for(i in unique(sim_data_reduced$ID)){\n    \n    # Filter data for individual i\n    data_i &lt;- sim_data_reduced |&gt;\n      filter(ID == i)\n    \n    # Extract individual parameters\n    y_i &lt;- data_i$DV\n    t &lt;- data_i$TIME\n    \n    # Compute the objective function value\n    ofv_i &lt;- ofv_function(\n      y_i = y_i,\n      t = t,\n      theta_tvcl = theta_tvcl,\n      vd = VD,\n      dose = DOSE,\n      sigma2 = RUV_VAR,\n      omega2 = omega2,\n      f_pred = f_pred,\n      gi_double_prime = gi_double_prime,\n      compute_eta_i_star = compute_eta_i_star,\n      obj_fun_eta_i_star = obj_fun_eta_i_star\n    )\n    \n    # append to vector\n    ofv_values_iter &lt;- c(ofv_values_iter, ofv_i)\n  }\n  \n  # calculate sum of ofv_values\n  OFV_sum &lt;- sum(ofv_values_iter)\n  \n  # store in tibble with add row\n  ofv_grid_values &lt;- ofv_grid_values |&gt;\n    add_row(THETA_TVCL = theta_tvcl, OMEGA2 = omega2, OBJ = OFV_sum)\n}\n\n# Convert to matrix for z-values\nx_unique &lt;- unique(ofv_grid_values$THETA_TVCL)\ny_unique &lt;- unique(ofv_grid_values$OMEGA2)\nz_matrix &lt;- matrix(ofv_grid_values$OBJ, nrow = length(x_unique), byrow = TRUE)"
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-surface-vis",
    "href": "posts/understanding_nlme_estimation/index.html#sec-surface-vis",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "7.3 Visualizing the surface",
    "text": "7.3 Visualizing the surface\nGreat. Now we have everything available to plot the 3D surface and to illustrate the steps which the estimation algorithm has taken to find the maximum likelihood estimate:\n\n\nCode\n# with tracer\n# Create 3D surface plot using plotly\nplot_ly(\n  x = ~x_unique,\n  y = ~y_unique,\n  z = ~z_matrix,\n  type = \"surface\",\n  cmin = -20, cmax = 60, name = \"Surface\",\n  showscale = TRUE,\n  colorbar = list(title = \"OFV\")\n) |&gt; \n  add_trace(\n    data = ext_file,\n    x = ~CL,\n    y = ~IIV_VAR,\n    z = ~OBJ,\n    type = 'scatter3d',\n    mode = 'markers',\n    marker = list(symbol = 'cross', color = 'red', size = 8),\n    name = \"NONMEM\"\n  ) |&gt;\n  add_trace(\n    data = ext_file,\n    x = ~CL,\n    y = ~IIV_VAR,\n    z = ~OBJ,\n    type = 'scatter3d',\n    mode = 'lines',\n    line = list(color = 'black', width = 3),\n    connectgaps = TRUE,\n    name = \"NONMEM\"\n  ) |&gt;\n  layout(\n    scene = list(\n      xaxis = list(title = \"Typical Clearance\"),\n      yaxis = list(title = \"IIV Variance\"),\n      zaxis = list(title = \"Objective Function Value\", range = c(-20, 60))\n    )\n  )\n\n\n\n\n\n\nWe can nicely see the surface and also the location of the maximum likelihood estimate. The red crosses represent the individual iterations of the estimation algorithm. The black line connects the individual iterations in the order they were performed. Apparently the optimization algorithm in NONMEM was quite efficient in finding a suitable path to the maximum likelihood estimate."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#modify-objective-function",
    "href": "posts/understanding_nlme_estimation/index.html#modify-objective-function",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "8.1 Modify objective function",
    "text": "8.1 Modify objective function\nTo perform the estimation entirely in R, we will encapsulate much of our previous logic into a single objective function which takes the current set of parameters as a vector (starting with the initials) and is returning the sum of the objective function across all individuals. This sum is what we will ask optim() in R to minimize by searching the parameter space. Essentially, nothing changes but we have to wrap everything in one single function which can be passed to optim().\n\n\n\nfunction: ofv_function()\n\n# define objective function for r-based parameter optimization\nparam_est_obj &lt;- function(par, \n                          sim_data_reduced,\n                          VD,\n                          DOSE,\n                          RUV_VAR,\n                          f_pred,\n                          gi_double_prime,\n                          compute_eta_i_star,\n                          obj_fun_eta_i_star) {\n  \n  \n  # Extract theta_tvcl and omega2\n  theta_tvcl &lt;- par[[1]]\n  omega2 &lt;- par[[2]]\n  \n  # create empty vector for individual ofv values\n  ofv_values_iter &lt;- numeric()\n  \n  # Loop over individuals\n  for(i in unique(sim_data_reduced$ID)){\n    \n    # Filter data for individual i\n    data_i &lt;- sim_data_reduced |&gt;\n      filter(ID == i)\n    \n    # Extract individual parameters\n    y_i &lt;- data_i$DV\n    t &lt;- data_i$TIME\n    \n    # Compute the objective function value\n    ofv_i &lt;- ofv_function(\n      y_i = y_i,\n      t = t,\n      theta_tvcl = theta_tvcl,\n      vd = VD,\n      dose = DOSE,\n      sigma2 = RUV_VAR,\n      omega2 = omega2,\n      f_pred = f_pred,\n      gi_double_prime = gi_double_prime,\n      compute_eta_i_star = compute_eta_i_star,\n      obj_fun_eta_i_star = obj_fun_eta_i_star\n    )\n    \n    # append to vector\n    ofv_values_iter &lt;- c(ofv_values_iter, ofv_i)\n  }\n  \n  # calculate sum of ofv_values\n  OFV_sum &lt;- sum(ofv_values_iter)\n  \n  # return OFV sum\n  return(OFV_sum)\n}\n\n\nGreat! Now we have the param_est_obj() function ready and it can be passed to the optimizer to start our own estimation."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#perform-estimation",
    "href": "posts/understanding_nlme_estimation/index.html#perform-estimation",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "8.2 Perform estimation",
    "text": "8.2 Perform estimation\nTo perform the estimation, we will pass the param_est_obj() function to optim function in R. For this part, we will select initial values that are close to the actual parameter estimates and additionally apply lower and upper bounds using the L-BFGS-B method. While playing around with the initial values, it became evident that the optim function and our current setup are considerably more sensitive to initial values than NONMEM. When the initial values were too far from the true estimates or when the optimizer was run without boundaries, I often encountered numerical issues, leading to termination of the estimation process. In contrast, NONMEM proved to be significantly more robust, suggesting that its optimizer is finely tuned for the pharmacometric problems we typcially face. In contrast, optim is rather a general purpose optimizer.\n\n\nCode\n# Starting guesses for theta_tvcl and omega2\ninitials &lt;- c(0.2, 0.1) \n\n# perform estimation\noptim_res &lt;- optim(\n  par     = initials,\n  fn      = param_est_obj,\n  sim_data_reduced = sim_data_reduced,\n  VD      = ext_file$V |&gt; unique(),\n  DOSE    = DOSE,\n  RUV_VAR = ext_file$RUV_VAR |&gt; unique(),\n  f_pred  = f_pred,\n  gi_double_prime         = gi_double_prime,\n  compute_eta_i_star= compute_eta_i_star,\n  obj_fun_eta_i_star= obj_fun_eta_i_star,\n  method  = \"L-BFGS-B\",\n  lower   = rep(0.001, length(initials)),\n  upper   = rep(1000, length(initials)),\n  hessian = TRUE,\n  control = list(\n    maxit = 10000,           \n    factr = 1e-7,           \n    pgtol = 1e-7,           \n    trace = 1,              \n    REPORT = 1              \n  )\n)\n\n\niter    1 value -12.819995\niter    2 value -12.822838\niter    3 value -12.824104\niter    4 value -12.824484\niter    5 value -12.824594\niter    6 value -12.824594\niter    7 value -12.824594\niter    8 value -12.824594\nfinal  value -12.824594 \nconverged\n\n\nThe optimization successfully converged and we can also see the trace of the optimization problem. The optim_res object now contains all the information we need about the optimization:\n\n\nCode\n# show optim_res object\noptim_res\n\n\n$par\n[1] 0.2466737 0.1100256\n\n$value\n[1] -12.82459\n\n$counts\nfunction gradient \n      16       16 \n\n$convergence\n[1] 0\n\n$message\n[1] \"CONVERGENCE: NORM OF PROJECTED GRADIENT &lt;= PGTOL\"\n\n$hessian\n              [,1]          [,2]\n[1,]  2.979341e+03  -0.003861313\n[2,] -3.861313e-03 821.681234002\n\n\nGreat! We can now compare the parameter estimates against the NONMEM estimates to see how well we were doing."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#comparison-against-nonmem",
    "href": "posts/understanding_nlme_estimation/index.html#comparison-against-nonmem",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "8.3 Comparison against NONMEM",
    "text": "8.3 Comparison against NONMEM\nTo compare our results against NONMEM, we will use the final iteration from the .ext file to extract the final parameter estimates. We will then incorporate the information obtained from the optim object in R, allowing us to directly compare the two sets of parameter estimates.\n\n\nCode\n# retrieve final parameter estimates from NM\nnm_par_est &lt;- ext_file |&gt; \n  filter(ITERATION == max(ext_file$ITERATION)) |&gt; \n  select(CL, IIV_VAR, OBJ) |&gt; \n  rename(\n    \"CL_NM\" = CL,\n    \"IIV_VAR_NM\" = IIV_VAR,\n    \"OBJ_NM\" = OBJ\n  ) |&gt; \n  mutate(\n    CL_R = optim_res$par[[1]],\n    IIV_VAR_R = optim_res$par[[2]],\n    OBJ_R = optim_res$value,\n    CL_DIFF = CL_R - CL_NM,\n    IIV_VAR_DIFF = IIV_VAR_R - IIV_VAR_NM,\n    OBJ_DIFF = OBJ_NM - OBJ_R\n  ) |&gt; \n  select(\n    CL_NM, CL_R, CL_DIFF, IIV_VAR_NM, IIV_VAR_R, IIV_VAR_DIFF, OBJ_NM, OBJ_R, OBJ_DIFF\n  )\n\n# show\nnm_par_est |&gt; \n  mytbl()\n\n\n\n\n\nCL_NM\nCL_R\nCL_DIFF\nIIV_VAR_NM\nIIV_VAR_R\nIIV_VAR_DIFF\nOBJ_NM\nOBJ_R\nOBJ_DIFF\n\n\n\n\n0.246682\n0.2466737\n-8.3e-06\n0.11002\n0.1100256\n5.6e-06\n-12.82459\n-12.82459\n3e-07\n\n\n\n\n\n\n\nOur R-based implementation achieves convergence to parameter estimates that are very similar to those obtained with NONMEM."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#retrieving-the-hessian-matrix",
    "href": "posts/understanding_nlme_estimation/index.html#retrieving-the-hessian-matrix",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "9.1 Retrieving the Hessian matrix",
    "text": "9.1 Retrieving the Hessian matrix\nWe can directly retrieve the hessian matrix from the optim_res object by assessing it via $hessian. Please note, that we did request the hessian during optimization with hessian = TRUE. This is the output:\n\n\nCode\n# store hessian\nhessian &lt;- optim_res$hessian\n\n# show\nhessian\n\n\n              [,1]          [,2]\n[1,]  2.979341e+03  -0.003861313\n[2,] -3.861313e-03 821.681234002"
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#solving-the-matrix",
    "href": "posts/understanding_nlme_estimation/index.html#solving-the-matrix",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "9.2 Solving the matrix",
    "text": "9.2 Solving the matrix\nWe can now go ahead and solve the matrix to get its inverse. The inverse of the Hessian serves as an approximation to the covariance matrix.\n\n\nCode\n# Standard errors (square root of variances)\ninverse_hessian &lt;- optim_res$hessian |&gt; solve()\n\n# show inverse hessian\ninverse_hessian\n\n\n             [,1]         [,2]\n[1,] 3.356447e-04 1.577289e-09\n[2,] 1.577289e-09 1.217017e-03\n\n\nWe are mainly interested in the variances for now, so we only care about the diagonals. We are going to retrieve the diagonals and store them alongside the parameter estimates inside a tibble:\n\n\nCode\n# get diagonal elements\nvariances_vec &lt;- diag(inverse_hessian)\n\n# define row\nrow1 &lt;- tibble(\n  PAR = \"TVCL\",\n  EST_R = optim_res$par[[1]],\n  VAR_R = variances_vec[[1]]\n)\n\n# define row\nrow2 &lt;- tibble(\n  PAR = \"IIV_VAR\",\n  EST_R = optim_res$par[[2]],\n  VAR_R = variances_vec[[2]]\n)\n\n# bind rows for parameter precicsion table\npar_prec &lt;- bind_rows(row1, row2)\n\n# show\npar_prec |&gt; \n  mytbl()\n\n\n\n\n\nPAR\nEST_R\nVAR_R\n\n\n\n\nTVCL\n0.2466737\n0.0003356\n\n\nIIV_VAR\n0.1100256\n0.0012170\n\n\n\n\n\n\n\nWe now have the parameter estimates and the associated variances stored in a single tibble."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#calculating-standard-errors",
    "href": "posts/understanding_nlme_estimation/index.html#calculating-standard-errors",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "9.3 Calculating standard errors",
    "text": "9.3 Calculating standard errors\nNow we can go ahead to calculate the standard error. The standard errors are the square root of the variances, so we can add a column with SE_R:\n\n\nCode\n# calculate standard errors\npar_prec &lt;- par_prec |&gt; \n  mutate(\n    SE_R = sqrt(VAR_R)\n  )\n\n# show\npar_prec |&gt; \n  mytbl()\n\n\n\n\n\nPAR\nEST_R\nVAR_R\nSE_R\n\n\n\n\nTVCL\n0.2466737\n0.0003356\n0.0183206\n\n\nIIV_VAR\n0.1100256\n0.0012170\n0.0348858\n\n\n\n\n\n\n\nFinally, the relative standard errors (RSE) can be calculated based on the standard error and the estimate itself. We are going to add RSE%_R to the tibble:\n\n\nCode\n# Calculate relative standard error\npar_prec &lt;- par_prec |&gt; \n  mutate(\"RSE%_R\" = (SE_R / EST_R)*100)\n\n# show table\npar_prec |&gt; \n  mytbl()\n\n\n\n\n\nPAR\nEST_R\nVAR_R\nSE_R\nRSE%_R\n\n\n\n\nTVCL\n0.2466737\n0.0003356\n0.0183206\n7.427062\n\n\nIIV_VAR\n0.1100256\n0.0012170\n0.0348858\n31.706964\n\n\n\n\n\n\n\nApparently, the parameter precision for the typical value of clearance is much higher than the variance estimate for the inter-individual variability. However, both estimates would be within an acceptable range and parameter precision would not be a particular concern."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#comparison-against-nonmem-1",
    "href": "posts/understanding_nlme_estimation/index.html#comparison-against-nonmem-1",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "9.4 Comparison against NONMEM",
    "text": "9.4 Comparison against NONMEM\nIn the previous steps, we have assessed the parameter precision of our R-based optimization run. Now, we want to compare the results against our reference solution. In a first step, we are going to load in the data from the .cov file from NONMEM, which contains the variance-covariance matrix of the parameter estimates from the estimation step\n\n\nCode\n# load nonmem .cov output\nnm_cov &lt;- nonmem2R::covload(\n  model = paste0(base_path, \"models/estimation/1cmt_iv_est.cov\"), \n  theta.only = FALSE\n) |&gt; \n  as_tibble()\n\n# show\nnm_cov |&gt; \n  mytbl()\n\n\n\n\n\nTHETA1\nTHETA2\nOMEGA.1.1.\nSIGMA.1.1.\n\n\n\n\n0.0006715\n0\n0.0007977\n0\n\n\n0.0000000\n0\n0.0000000\n0\n\n\n0.0007977\n0\n0.0023749\n0\n\n\n0.0000000\n0\n0.0000000\n0\n\n\n\n\n\n\n\nThis is a representation of the covariance matrix from NONMEM. We are now going to perform the same steps as done above and then augment the results with the R-based results to compare both results.\n\n\nCode\n# augment table\npar_prec &lt;- par_prec |&gt; \n  mutate(\n    EST_NM = c(nm_par_est$CL_NM, nm_par_est$IIV_VAR_NM),\n    VAR_NM = c(nm_cov$THETA1[[1]], nm_cov$OMEGA.1.1.[[3]]),\n    SE_NM = sqrt(VAR_NM),\n    \"RSE%_NM\" = (SE_NM/EST_NM)*100,\n    \"RSE%_DIFF\" = `RSE%_R` - `RSE%_NM`\n  ) |&gt; \n  select(PAR, EST_R, EST_NM, VAR_R, VAR_NM, SE_R, SE_NM, `RSE%_R`, `RSE%_NM`, `RSE%_DIFF`)\n\n# show table\npar_prec |&gt; \n  mytbl()\n\n\n\n\n\nPAR\nEST_R\nEST_NM\nVAR_R\nVAR_NM\nSE_R\nSE_NM\nRSE%_R\nRSE%_NM\nRSE%_DIFF\n\n\n\n\nTVCL\n0.2466737\n0.246682\n0.0003356\n0.0006715\n0.0183206\n0.0259135\n7.427062\n10.50480\n-3.077739\n\n\nIIV_VAR\n0.1100256\n0.110020\n0.0012170\n0.0023749\n0.0348858\n0.0487325\n31.706964\n44.29425\n-12.587288\n\n\n\n\n\n\n\nWith our R-based reproduction, we can capture the general trend of higher parameter imprecision for $OMEGA estimates. However, we are unable to replicate the exact numerical values from the NONMEM output. The differences in relative standard errors (RSEs) are substantial, with R showing a consistent bias toward lower RSEs compared to NONMEM. It remains unclear whether these discrepancies indicate an error in my implementation or if they can be attributed solely to differences in how the Hessian matrix is approximated in the maximum likelihood estimate. I personally lack enough knowledge about the specific numerical approximation methods used by NONMEM versus the optim function in R, and I am unsure whether such differences are expected or not."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#reading-in-nonmems-individual-etas",
    "href": "posts/understanding_nlme_estimation/index.html#reading-in-nonmems-individual-etas",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "10.1 Reading in NONMEMs individual ETAs",
    "text": "10.1 Reading in NONMEMs individual ETAs\nIn a first step, we are going to read in the sdtab which contains the individual realization of the random variable, ETA1. We are then only keeping one distinct row per ID.\n\n\nCode\n# load nonmem output with posthoc ETA\nsdtab &lt;- read_nm_table(paste0(base_path, \"models/estimation/estim_out\"))\n\n# summarise nonmem eta_i values\nnm_eta_i_star_values &lt;- sdtab |&gt; \n  select(ID, ETA1) |&gt; \n  distinct() |&gt; \n  rename(ETA1_NM = ETA1)\n\n# show table\nnm_eta_i_star_values |&gt; \n  mytbl()\n\n\n\n\n\nID\nETA1_NM\n\n\n\n\n1\n0.173900\n\n\n2\n-0.110320\n\n\n3\n-0.043933\n\n\n4\n-0.153930\n\n\n5\n0.725750\n\n\n6\n-0.033077\n\n\n7\n-0.349690\n\n\n8\n-0.427840\n\n\n9\n-0.183560\n\n\n10\n0.402510"
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#reproducing-ebe-map-estimates",
    "href": "posts/understanding_nlme_estimation/index.html#reproducing-ebe-map-estimates",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "10.2 Reproducing EBE / MAP estimates",
    "text": "10.2 Reproducing EBE / MAP estimates\nNow it is time to reproduce the Empirical Bayes Estimates (EBE) or Maximum A Posteriori (MAP) step using the custom function we defined earlier when constructing the objective function. For this, we will use the final parameter estimates obtained at convergence during our estimation process.\n\n\nCode\n# define iter as maximum \niter &lt;- nrow(ext_file)\n\n# retrieve information from first row\nTVCL &lt;- ext_file |&gt; slice(iter) |&gt; pull(CL)\nVD &lt;- ext_file |&gt; slice(iter) |&gt; pull(V)\nRUV_VAR &lt;- ext_file |&gt; slice(iter) |&gt; pull(RUV_VAR)\nIIV_VAR &lt;- ext_file |&gt; slice(iter) |&gt; pull(IIV_VAR)\n\n# define constants\nDOSE &lt;- 100\n\n# define initials\neta_i_init &lt;- 0\n\n# create empty list for individual eta_i_star values\neta_i_star_list &lt;- list()\n\n# Loop over individuals\nfor(i in unique(sim_data_reduced$ID)){\n  \n  # Filter data for individual i\n  data_i &lt;- sim_data_reduced |&gt;\n    filter(ID == i)\n  \n  # Extract individual parameters\n  y_i &lt;- data_i$DV\n  t &lt;- data_i$TIME\n\n  # compute eta_i_star  \n  eta_i_star &lt;- compute_eta_i_star(\n    y_i = y_i,\n    t = t,\n    theta_tvcl = TVCL,\n    vd = VD,\n    dose = DOSE,\n    sigma2 = RUV_VAR,\n    omega2 = IIV_VAR,\n    f_pred = f_pred,\n    eta_i_init = eta_i_init,\n    obj_fun_eta_i_star = obj_fun_eta_i_star\n  )\n  \n  # append to vector\n  eta_i_star_list[[i]] &lt;- tibble(\n    ID = i,\n    ETA1_R = eta_i_star\n  )\n}\n\n# store in tibble with add row\neta_i_star_values &lt;-  bind_rows(eta_i_star_list)\n\n# show the results\neta_i_star_values |&gt; \n  mytbl()\n\n\n\n\n\nID\nETA1_R\n\n\n\n\n1\n0.1738965\n\n\n2\n-0.1103184\n\n\n3\n-0.0439348\n\n\n4\n-0.1539363\n\n\n5\n0.7257483\n\n\n6\n-0.0330794\n\n\n7\n-0.3496945\n\n\n8\n-0.4278383\n\n\n9\n-0.1835616\n\n\n10\n0.4025086"
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#merging-datasets-and-comparison",
    "href": "posts/understanding_nlme_estimation/index.html#merging-datasets-and-comparison",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "10.3 Merging datasets and comparison",
    "text": "10.3 Merging datasets and comparison\nNow that we have the estimates ready for both NONMEM and R, we can compare both and calculate the difference.\n\n\nCode\n# left_join\neta_i_star_values &lt;- eta_i_star_values |&gt; \n  left_join(nm_eta_i_star_values, by = \"ID\") |&gt; \n  mutate(DIFF = ETA1_R - ETA1_NM)\n\n# show tibble\neta_i_star_values |&gt; \n  mytbl()\n\n\n\n\n\nID\nETA1_R\nETA1_NM\nDIFF\n\n\n\n\n1\n0.1738965\n0.173900\n-3.5e-06\n\n\n2\n-0.1103184\n-0.110320\n1.6e-06\n\n\n3\n-0.0439348\n-0.043933\n-1.8e-06\n\n\n4\n-0.1539363\n-0.153930\n-6.3e-06\n\n\n5\n0.7257483\n0.725750\n-1.7e-06\n\n\n6\n-0.0330794\n-0.033077\n-2.4e-06\n\n\n7\n-0.3496945\n-0.349690\n-4.5e-06\n\n\n8\n-0.4278383\n-0.427840\n1.7e-06\n\n\n9\n-0.1835616\n-0.183560\n-1.6e-06\n\n\n10\n0.4025086\n0.402510\n-1.4e-06\n\n\n\n\n\n\n\nThe differences between the two implementations are tiny, only showing up in distant decimal places. This gives me confidence that our MAP objective function is working as it should."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#footnotes",
    "href": "posts/understanding_nlme_estimation/index.html#footnotes",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe only need to approximate \\(g_i()\\) if we cannot explicitly calculate that integral. If we would be in a linear setting (which we are not), we wouldn’t need any approximation and could directly work with it.↩︎\nRemember: We actually deal with a latent, unobserved variable.↩︎\nThey are defined for the case of \\(\\eta_i^*\\), but here we need the more general form of \\(\\eta_i\\). But the expression itself is the same.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Marian Klose",
    "section": "",
    "text": "Hi there! I’m Marian Klose, currently a PhD student at Freie Universitaet Berlin, and part of the PharMetrX graduate research training program.\nHow did I end up here? During my pharmacy studies I got into pharmacometrics pretty early on and loved the mix of computer science, pharmacy, and math. After my graduation, I headed to Florida for six months to work at the University of Florida’s Center for Pharmacometrics and Systems Pharmacology, and also did a three-month internship at Boehringer Ingelheim to see if my interest would remain steady. And I was still fascinated by pharmacometrics, so I decided to start my PhD and applied to the PharMetrX program — and here I am today!\nThankfully, there are many helpful blogs and articles available that break down complex topics into simpler, more accessible concepts. Whenever I come across a concept I don’t fully understand (which happens quite often), I try to explain it to myself using a Quarto or RMarkdown document. Since I’m anyways creating these documents for myself, I thought I might as well share them as little blog posts. My goal really isn’t to produce perfectly polished content, but rather to document my own learning process and potentially help others who might be facing the same challenges."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Marian Klose",
    "section": "",
    "text": "Hi there! I’m Marian Klose, currently a PhD student at Freie Universitaet Berlin, and part of the PharMetrX graduate research training program.\nHow did I end up here? During my pharmacy studies I got into pharmacometrics pretty early on and loved the mix of computer science, pharmacy, and math. After my graduation, I headed to Florida for six months to work at the University of Florida’s Center for Pharmacometrics and Systems Pharmacology, and also did a three-month internship at Boehringer Ingelheim to see if my interest would remain steady. And I was still fascinated by pharmacometrics, so I decided to start my PhD and applied to the PharMetrX program — and here I am today!\nThankfully, there are many helpful blogs and articles available that break down complex topics into simpler, more accessible concepts. Whenever I come across a concept I don’t fully understand (which happens quite often), I try to explain it to myself using a Quarto or RMarkdown document. Since I’m anyways creating these documents for myself, I thought I might as well share them as little blog posts. My goal really isn’t to produce perfectly polished content, but rather to document my own learning process and potentially help others who might be facing the same challenges."
  },
  {
    "objectID": "publications/publications.html",
    "href": "publications/publications.html",
    "title": "Publications",
    "section": "",
    "text": "Clinical trial simulation\n\n\nRShiny\n\n\nDuchenne muscular dystrophy\n\n\nMiddle author\n\n\n\n\n\n\nOct 3, 2024\n\n\nKim J, Morales JF, Kang S, Klose M, Willcocks RJ, Daniels MJ, Belfiore-Oshan R, Walter GA, Rooney WD, Vandenborne K, Kim S\n\n\nCPT: Pharmacometrics & Systems Pharmacology\n\n\n\n\n\n\n\n\n\n\n\n\nClinical trial simulation\n\n\nRShiny\n\n\nType 1 diabetes\n\n\nSecond author\n\n\n\n\n\n\nJul 3, 2024\n\n\nMorales JF, Klose M, Hoffert Y, Podichetty JT, Burton J, Schmidt S, Romero K, O’Doherty I, Martin F, Campbell-Thompson M, Haller MJ, Atkinson MA, Kim S.\n\n\nCPT: Pharmacometrics & Systems Pharmacology\n\n\n\n\n\n\n\n\n\n\n\n\nDoxorubicin\n\n\nTLD-1\n\n\nPEGylated liposomal doxorubicin\n\n\nNLME PK\n\n\nMiddle author\n\n\n\n\n\n\nJun 15, 2024\n\n\nMc Laughlin AM, Hess D, Michelet R, Colombo I, Haefliger S, Bastian S, Rabaglio M, Schwitter M, Fischer S, Eckhardt K, Hayoz S, Kopp C, Klose M, Sessa C, Stathis A, Halbherr S, Huisinga W, Joerger M, Kloft C.\n\n\nCancer Chemotherapy and Pharmacology\n\n\n\n\n\n\n\n\n\n\n\n\nPBPK\n\n\nOxycodone\n\n\nGDI\n\n\nDDI\n\n\nUGT2B7\n\n\nCYP2D6\n\n\nFirst author\n\n\n\n\n\n\nMar 1, 2024\n\n\nKlose M, Cristofoletti R, Silva CM, Mangal N, Turgeon J, Michaud V, Lesko LJ, Schmidt S.\n\n\nEuropean Journal of Pharmaceutical Sciences\n\n\n\n\n\n\n\n\n\n\n\n\nTLD-1\n\n\nPEGylated liposomal doxorubicin\n\n\nDoxorubicin\n\n\nSafety\n\n\nEfficacy\n\n\nNCA\n\n\nMiddle author\n\n\n\n\n\n\nFeb 2, 2024\n\n\nColombo I, Koster KL, Holer L, Haefliger S, Rabaglio M, Bastian S, Schwitter M, Eckhardt K, Hayoz S, Mc Laughlin AM, Kloft C, Klose M, Halbherr S, Baumgartner C, Sessa C, Stathis A, Hess D, Joerger M.\n\n\nEuropean Journal of Cancer\n\n\n\n\n\n\n\n\n\n\n\n\nMeropenem\n\n\nAntiinfectives\n\n\nNLME PK\n\n\nSecond author\n\n\n\n\n\n\nApr 20, 2021\n\n\nLiebchen U, Klose M, Paal M, Vogeser M, Zoller M, Schroeder I, Schmitt L, Huisinga W, Michelet R, Zander J, Scharf C, Weinelt FA, Kloft C.\n\n\nAntibiotics\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/publications.html#peer-reviewed-publications",
    "href": "publications/publications.html#peer-reviewed-publications",
    "title": "Publications",
    "section": "",
    "text": "Clinical trial simulation\n\n\nRShiny\n\n\nDuchenne muscular dystrophy\n\n\nMiddle author\n\n\n\n\n\n\nOct 3, 2024\n\n\nKim J, Morales JF, Kang S, Klose M, Willcocks RJ, Daniels MJ, Belfiore-Oshan R, Walter GA, Rooney WD, Vandenborne K, Kim S\n\n\nCPT: Pharmacometrics & Systems Pharmacology\n\n\n\n\n\n\n\n\n\n\n\n\nClinical trial simulation\n\n\nRShiny\n\n\nType 1 diabetes\n\n\nSecond author\n\n\n\n\n\n\nJul 3, 2024\n\n\nMorales JF, Klose M, Hoffert Y, Podichetty JT, Burton J, Schmidt S, Romero K, O’Doherty I, Martin F, Campbell-Thompson M, Haller MJ, Atkinson MA, Kim S.\n\n\nCPT: Pharmacometrics & Systems Pharmacology\n\n\n\n\n\n\n\n\n\n\n\n\nDoxorubicin\n\n\nTLD-1\n\n\nPEGylated liposomal doxorubicin\n\n\nNLME PK\n\n\nMiddle author\n\n\n\n\n\n\nJun 15, 2024\n\n\nMc Laughlin AM, Hess D, Michelet R, Colombo I, Haefliger S, Bastian S, Rabaglio M, Schwitter M, Fischer S, Eckhardt K, Hayoz S, Kopp C, Klose M, Sessa C, Stathis A, Halbherr S, Huisinga W, Joerger M, Kloft C.\n\n\nCancer Chemotherapy and Pharmacology\n\n\n\n\n\n\n\n\n\n\n\n\nPBPK\n\n\nOxycodone\n\n\nGDI\n\n\nDDI\n\n\nUGT2B7\n\n\nCYP2D6\n\n\nFirst author\n\n\n\n\n\n\nMar 1, 2024\n\n\nKlose M, Cristofoletti R, Silva CM, Mangal N, Turgeon J, Michaud V, Lesko LJ, Schmidt S.\n\n\nEuropean Journal of Pharmaceutical Sciences\n\n\n\n\n\n\n\n\n\n\n\n\nTLD-1\n\n\nPEGylated liposomal doxorubicin\n\n\nDoxorubicin\n\n\nSafety\n\n\nEfficacy\n\n\nNCA\n\n\nMiddle author\n\n\n\n\n\n\nFeb 2, 2024\n\n\nColombo I, Koster KL, Holer L, Haefliger S, Rabaglio M, Bastian S, Schwitter M, Eckhardt K, Hayoz S, Mc Laughlin AM, Kloft C, Klose M, Halbherr S, Baumgartner C, Sessa C, Stathis A, Hess D, Joerger M.\n\n\nEuropean Journal of Cancer\n\n\n\n\n\n\n\n\n\n\n\n\nMeropenem\n\n\nAntiinfectives\n\n\nNLME PK\n\n\nSecond author\n\n\n\n\n\n\nApr 20, 2021\n\n\nLiebchen U, Klose M, Paal M, Vogeser M, Zoller M, Schroeder I, Schmitt L, Huisinga W, Michelet R, Zander J, Scharf C, Weinelt FA, Kloft C.\n\n\nAntibiotics\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/publications.html#abstractsposterspresentations",
    "href": "publications/publications.html#abstractsposterspresentations",
    "title": "Publications",
    "section": "Abstracts/Posters/Presentations",
    "text": "Abstracts/Posters/Presentations\n\n\n\n\n\nTLD-1 – A novel liposomal doxorubicin formulation, in patients with advanced solid tumors: Final results of a multicenter, open-label, cross-over phase I study (SAKK 65/16)\n\n\n\n\n\n\nDoxorubicin\n\n\nTLD-1\n\n\nPegylated Liposomal Doxorubicin\n\n\nSecond author\n\n\n\n\n\n\nMar 3, 2025\n\n\nM. Joerger, M. Klose, I. Colombo, K. Koster, K. Gobat, S. Haefliger, M. Rabaglio, S. Bastian, M. Schwitter, K. Eckhardt, S. Hayoz, S. Halbherr, C. Sessa, R. Michelet, A. Mc Laughlin, D. Hess, C. Kloft, A. Stathis\n\n\nESMO TAT 2025\n\n\n\n\n\n\n\nMachine learning-driven flattening of model priors: A comparative simulation study across multiple compounds\n\n\n\n\n\n\nMachine learning\n\n\nBayes\n\n\nPrior Modifications\n\n\nSimulation study\n\n\nFirst author\n\n\n\n\n\n\nJun 26, 2024\n\n\nM. Klose, F. Thoma, L. Kovar, W. Huisinga, R. Michelet, C. Kloft\n\n\nPAGE 2024\n\n\n\n\n\n\n\nThe Impact of UGT2B7 and CYP2D6 Gene-Drug- and CYP-mediated Drug-Drug-Interactions on Oxycodone and Oxymorphone Pharmacokinetics using PBPK Modelling.\n\n\n\n\n\n\nPBPK\n\n\nOxycodone\n\n\nGDI\n\n\nDDI\n\n\nUGT2B7\n\n\nCYP2D6\n\n\nFirst author\n\n\n\n\n\n\nOct 30, 2022\n\n\nKlose M, Schmidt S, Cristofoletti R.\n\n\nACoP13\n\n\n\n\n\n\n\nUsing microdose-based activity measurement to individualise dosing of cytochrome P450 metabolised drugs: a case study with yohimbine and tamoxifen\n\n\n\n\n\n\nYohimbine\n\n\nTamoxifen\n\n\nCYP2D6\n\n\nMicrodosing\n\n\nMiddle author\n\n\n\n\n\n\nSep 2, 2021\n\n\nR. Michelet, F. Weinelt, M. Klose, A. M. Mc Laughlin, F. Kluwe, C. Montefusco-Pereira, M. Van Dyk, M. Vay, W. Huisinga, C. Kloft & G. Mikus\n\n\nPAGE 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/abstracts_docs/03-03-2025_tld_1_final_results_sakk6516.html",
    "href": "publications/abstracts_docs/03-03-2025_tld_1_final_results_sakk6516.html",
    "title": "TLD-1 – A novel liposomal doxorubicin formulation, in patients with advanced solid tumors: Final results of a multicenter, open-label, cross-over phase I study (SAKK 65/16)",
    "section": "",
    "text": "https://oncologypro.esmo.org/meeting-resources/esmo-targeted-anticancer-therapies-congress-2025/tld-1-a-novel-liposomal-doxorubicin-formulation-in-patients-with-advanced-solid-tumors-final-results-of-a-multicenter-open-label-cross-over-p"
  },
  {
    "objectID": "publications/abstracts_docs/03-03-2025_tld_1_final_results_sakk6516.html#url",
    "href": "publications/abstracts_docs/03-03-2025_tld_1_final_results_sakk6516.html#url",
    "title": "TLD-1 – A novel liposomal doxorubicin formulation, in patients with advanced solid tumors: Final results of a multicenter, open-label, cross-over phase I study (SAKK 65/16)",
    "section": "",
    "text": "https://oncologypro.esmo.org/meeting-resources/esmo-targeted-anticancer-therapies-congress-2025/tld-1-a-novel-liposomal-doxorubicin-formulation-in-patients-with-advanced-solid-tumors-final-results-of-a-multicenter-open-label-cross-over-p"
  },
  {
    "objectID": "publications/abstracts_docs/03-03-2025_tld_1_final_results_sakk6516.html#abstract",
    "href": "publications/abstracts_docs/03-03-2025_tld_1_final_results_sakk6516.html#abstract",
    "title": "TLD-1 – A novel liposomal doxorubicin formulation, in patients with advanced solid tumors: Final results of a multicenter, open-label, cross-over phase I study (SAKK 65/16)",
    "section": "Abstract",
    "text": "Abstract\n\nBackground\nTLD-1 is a novel liposomal doxorubicin formulation with a small hydrodynamic liposome diameter of 60 nm (90 nm for Caelyx) and a high lipid-to-drug ratio. We present the final results of SAKK 65/16, a phase 1 first-in-human study to define TLD-1’s safety, recommended phase 2 dose (RP2D), preliminary activity and pharmacokinetics (PK) in patients with advanced solid tumors.\n\n\nMethods\nWe included patients with advanced or recurrent solid tumors who failed standard treatment. TLD-1 was administered intravenously every 3 weeks up to a maximum of 9 cycles (6 cycles for anthracycline-pretreated patients) starting at 10 mg/m2, using accelerated dose escalation and a modified continual reassessment design. A subsequent comparative PK part with a randomized cross-over design used TLD-1 at the RP2D and assigned patients to a single cycle of Caelyx (first or second cycle), comparing the PK of TLD-1 and Caelyx.\n\n\nResults\n43 patients were enrolled between November 2018 and March 2023, incuding 30 patients in the dose-escalation part and 13 patients in the comparative PK part. Predominant tumor entities included breast cancer in 23 patients and gynecological cancers in 12 patients. TLD-1 RP2D was defined as 40 mg/m2 administered every 3 weeks. No treatment-related adverse events (TRAE) G4 or G5 were reported. TRAE G3 included cumulative hand-foot syndrome in 5 (12%) patients, stomatitis in 3 (7%) patients and anemia in 2 (5%) patients. Partial tumor response was documented in 4 (9%) patients. In the comparative PK part, median terminal half-life of encapsulated doxorubicin was 118 hours (TLD-1) and 70 hours (Caelyx), respectively, while median terminal half-life was 122 hours (TLD-1) and 81 hours (Caelyx) for unencapsulated doxorubicin, driven by the rate-limiting release from the liposomes.\n\n\nConclusions\nTLD-1 at 40 mg/m2 every 3 weeks was safe and showed preliminary activity in patients with heavily pretreated solid malignancies. TLD-1 exhibits a lower clearance and prolonged plasma half-life compared to conventional liposomal doxorubicin formulations such as Caelyx, potentially resulting from decreased degradation by the mononuclear phagocyte system.\n\n\nClinical trial identification\nNCT03387917.\n\n\nEditorial acknowledgement\nLegal entity responsible for the study Swiss Group for Clinical Cancer Research (SAKK).\n\n\nFunding\nInnomedica.\n\n\nDisclosure\nM. Joerger: Financial Interests, Institutional, Invited Speaker, Clinical study activity: Basilea, Bayer, BMS, Immunophotonics, Innomedica, MSD, Novartis, Roche; Financial Interests, Institutional, Other, Clinical study activity: DaiichySankyo; Financial Interests, Institutional, Invited Speaker: Anaveon; Non-Financial Interests, Personal, Advisory Role: Novartis, AstraZeneca, Basilea, Bayer, BMS, Debiopharm, MSD, Roche, Sanofi. I. Colombo: Financial Interests, Institutional, Expert Testimony: GSK, AstraZeneca, AbbVie; Financial Interests, Institutional, Advisory Board: GSK, MSD, AstraZeneca, Incyte; Financial Interests, Personal, Other, Travel grants: GSK; Financial Interests, Personal, Invited Speaker: GSK, MSD, Bayer, Vivesto, Incyte, AstraZeneca, Orion Pharma, Tolremo; Financial Interests, Personal, Other, Health Care Professional Consultancy: BionTech; Financial Interests, Personal, Full or part-time Employment, My husband is an employee of this biomedical company since 02.2022. His role is ‘Marketing Manager’ for the endoscopy division and he is in charge of the launchig program in the field of artificial intelligence applied to colonoscpy. I confirm that his job does not have any conflict of interest with my job and my role at ESMO: Medtronic; Financial Interests, Personal, Stocks/Shares: Medtronic; Non-Financial Interests, Personal, Leadership Role: Swiss Group for Clinical Cancer Research (SAKK); Non-Financial Interests, Personal, Advisory Role: European School of Oncology (ESO). S. Halbherr: Financial Interests, Personal, Stocks/Shares: Innomedica; Other, Personal, Full or part-time Employment: Innomedica. C. Sessa: Financial Interests, Personal, Other, ESO Consultant for gynaecological cancer: ESO (European School of Oncology); Financial Interests, Personal, Other, DMC member of the MK-3475-C93 studyDMC member of the MK-2870-005 study: Merck; Non-Financial Interests, Personal, Advisory Role, ESMO extended member women for oncology: ESMO; Non-Financial Interests, Personal, Advisory Role, Member of the compliance committee: ESMO. A. Mc Laughlin: Other, Personal, Full or part-time Employment: Pharmetheus AB. A. Stathis: Financial Interests, Institutional, Expert Testimony: Bayer, Eli Lilly; Financial Interests, Institutional, Advisory Board: Janssen, Roche, Incyte, Beigene; Financial Interests, Institutional, Other, Travel grant: AstraZeneca, Incyte; Financial Interests, Institutional, Other, Consultancy: Debiopharm, AstraZeneca, MSD; Financial Interests, Institutional, Invited Speaker: Janseen, Pfizer, Merck MSD, Roche, Novartis, ADC Therapeutics, AbbVie, Bayer, Philogen, Cellestia, AstraZeneca, Incyte, Amgen, Loxo Oncology, Debiopharm, BMS. All other authors have declared no conflicts of interest."
  },
  {
    "objectID": "posts/bayes_map_estimation_r/index.html",
    "href": "posts/bayes_map_estimation_r/index.html",
    "title": "Bayesian MAP estimation in R",
    "section": "",
    "text": "Code\n# load packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(kableExtra)\nlibrary(xpose4)\nlibrary(tidyr)"
  },
  {
    "objectID": "posts/bayes_map_estimation_r/index.html#starting-with-an-example",
    "href": "posts/bayes_map_estimation_r/index.html#starting-with-an-example",
    "title": "Bayesian MAP estimation in R",
    "section": "3.1 Starting with an example",
    "text": "3.1 Starting with an example\nI would say that most people in their daily life think Bayesian (although not everyone wants to admit it). The core idea of Bayesian statistics is to update your beliefs once new data or information becomes available. One simple example: Assume you commute to work every day by using your car. If someone would ask you “How long do you think you will need to get to work tomorrow?”, you would probably have a very good idea about how long it will take you, simply based on your experience. You might say that 30 minutes is a good estimate, but would you bet money that it will be exactly 30 minutes? Most likely not, it might be your best guess but there will be some uncertainly associated to it. You might be willing to bet money that it will be between 20 and 40 minutes. Now assuming that you get into your car the next morning and traffic is a state so that after 25 minutes you made it half-way through.\n[IMAGE]\nWould you still stick to your initial belief that it will take you 30 minutes? Probably not, you would update your belief. Would you then just multiply the 25 minutes by two and say that it will take you 50 minutes? Probably not, given that all your experience tells you that you nearly always made it within 40 minutes. So your updated belief will be something between 30 minutes (initial guess) and 50 minutes (what the data suggests). This updated belief (let’s say 38 minutes) is what Bayesian statistics is all about. And since we apply this Bayesian idea in our daily life, I personally find it quite an intuitive way of thinking statistics."
  },
  {
    "objectID": "posts/bayes_map_estimation_r/index.html#nomenclature",
    "href": "posts/bayes_map_estimation_r/index.html#nomenclature",
    "title": "Bayesian MAP estimation in R",
    "section": "3.2 Nomenclature",
    "text": "3.2 Nomenclature\nThe real-life example defined above allows us to define some Bayesian nomenclature:\n\nThe prior is the initial belief, in our example the 30 minutes and the uncertainty around it.\nThe likelihood represents the new data (or evidence), in our example the 25 minutes for 1/2 of the way.\nThe posterior is the updated belief after the data became available, in our example the 38 minutes.\n\nSo the Bayesian workflow is always based on the idea that you have a prior belief, which you update once new data (likelihood) becomes available. The updated belief is then called the posterior. There are some differences if we talk about MAP estimation and full posterior estimation:\n\nMAP: the maximum a-posteriori estimate, which is the mode of the posterior distribution\nfull Bayesian: the full posterior distribution and not only the mode / a point estimate\n\nMost pharmacometricians focus on the mode of the posterior (MAP/EBE) and neglect the uncertainty captured in the full distribution. The main reason for this is that the MAP estimates are easy to calculate while the full posterior is more challenging to obtain.\nIt goes without a saying that this is a simple example and everything is just based on a gut-feeling than on equations. While the core-idea remains the same, Bayesian statistics provide a framework to calculate the posterior in a more formal way. This is what we are tackling next."
  },
  {
    "objectID": "posts/bayes_map_estimation_r/index.html#the-bayesian-formula",
    "href": "posts/bayes_map_estimation_r/index.html#the-bayesian-formula",
    "title": "Bayesian MAP estimation",
    "section": "3.4 The Bayesian Formula",
    "text": "3.4 The Bayesian Formula\nAs described above, the main goal of a Bayesian attempt is to obtain the posterior distribution (either the mode or the full distribution). Often you find this formula to describe the Bayes theorem:\n\\[P(A|B) = \\frac{P(A) \\cdot P(B|A)}{P(B)}\\]\nHere, conditional probabilities are used and \\(P(A|B)\\) is the probability of event A given that event B has occurred. This remains quite theoretical. Let’s directly translate it into the context of a pharmacokinetic NLME model. Our aim is to indiv\n\\[p(\\theta|Y_{i}) = \\frac{p(\\theta) \\cdot p(Y_{i}|\\theta)}{p(Y_i)}\\]\nwith\n\n\\(p(\\theta|Y_{ij})\\): the posterior distribution of the parameter \\(\\theta\\) given the data of our ith individual \\(Y_{i}\\)\n\\(p(\\theta)\\): the prior distribution of the parameter \\(\\theta\\)\n\\(p(Y_{i}|\\theta)\\): the likelihood of the data \\(Y_{i}\\) given the parameter \\(\\theta\\)\n\\(p(Y_{i})\\): the marginal likelihood of the data \\(Y_{i}\\)\n\nThe marginal likelihood of the data \\(Y_{i}\\) is often neglected since it is just a scaling factor, does not depend on the parameter \\(\\theta\\) and is typically hard to calculate as it contains a high-dimensional integral. That is why you often see the formula written as:\n\\[p(\\theta|Y_{i}) \\propto p(\\theta) \\cdot p(Y_{i}|\\theta)\\]\nHere, we got rid of the marginal likelihood in the denominator. As we are now missing out the scaling factor, we now deal with an unnormalized posterior distribution and indicate this by using the proportional sign. Dealing with unnormalized posteriors is not a problem if we are only interested in the mode of the posterior distribution (MAP estimate) as the normalization factor is not needed for this."
  },
  {
    "objectID": "posts/bayes_map_estimation_r/index.html#disclaimer",
    "href": "posts/bayes_map_estimation_r/index.html#disclaimer",
    "title": "Bayesian MAP estimation in R",
    "section": "1.2 Disclaimer",
    "text": "1.2 Disclaimer\nI just share my personal thoughts and understanding of some Bayesian ideas and I can’t promise that you won’t find flaws in my explanations or equations."
  },
  {
    "objectID": "posts/bayes_map_estimation_r/index.html#our-goal-in-pharmacometrics",
    "href": "posts/bayes_map_estimation_r/index.html#our-goal-in-pharmacometrics",
    "title": "Bayesian MAP estimation in R",
    "section": "3.3 Our goal in pharmacometrics",
    "text": "3.3 Our goal in pharmacometrics\nBut what is now the goal of Bayesian stats in pharmacometrics? Actually not to better plan your daily commute to work. Instead, we typically use Bayesian estimation to individualize the model parameters for a given individual \\(i\\), which has a set of observations \\(Y_{i}\\). Now we want to use this individual data to find the best parameter estimates for this individual. On the one hand this happens during model building, at each iteration step and then after the actual estimation has finished, at the posthoc step. On the other hand, it also happens in so called model-informed precision dosing scenarios (MIPD) where we want to predict the future concentration-time profile and we have obtained some historic concentrations through therapeutic drug monitoring."
  },
  {
    "objectID": "posts/bayes_map_estimation_r/index.html#bayesian-theory",
    "href": "posts/bayes_map_estimation_r/index.html#bayesian-theory",
    "title": "Bayesian MAP estimation in R",
    "section": "4.4 Bayesian Theory",
    "text": "4.4 Bayesian Theory\n\n4.4.1 General form\nAs described above, the main goal of a Bayesian attempt is to obtain the posterior distribution (either the mode or the full distribution). Often you find this formula to describe the Bayes theorem:\n\\[P(A|B) = \\frac{P(A) \\cdot P(B|A)}{P(B)}\\]\nHere, conditional probabilities are used and \\(P(A|B)\\) is the probability of event A given that event B has occurred. This remains quite theoretical. Let’s directly translate it into the context of a pharmacokinetic NLME model.\n\n\n4.4.2 Pharmacometric context\nOur use-case in the world of pharmacometrics is to find the most likely parameter \\(\\eta_i\\) (which then translates to \\(CL_i\\)) given our individual’s concentration data \\(Y_{i}\\):\n\\[p(\\eta|Y_{i}) = \\frac{p(\\eta) \\cdot p(Y_{i}|\\eta)}{p(Y_i)}\\]\nwith\n\n\\(p(\\eta|Y_{ij})\\): the posterior distribution of the parameter \\(\\eta\\) given the data of our ith individual \\(Y_{i}\\)\n\\(p(\\eta)\\): the prior distribution of the parameter \\(\\eta\\)\n\\(p(Y_{i}|\\eta)\\): the likelihood of the data \\(Y_{i}\\) given the parameter \\(\\eta\\)\n\\(p(Y_{i})\\): the marginal likelihood of the data \\(Y_{i}\\)\n\nThe marginal likelihood of the data \\(Y_{i}\\) is often neglected since it is just a scaling factor, does not depend on the parameter \\(\\eta\\) and is typically hard to calculate as it contains a high-dimensional integral. That is why you often see the formula written as:\n\\[p(\\eta|Y_{i}) \\propto p(\\eta) \\cdot p(Y_{i}|\\eta)\\]\nHere, we got rid of the marginal likelihood in the denominator. As we are now missing out the scaling factor, we now deal with an unnormalized posterior distribution and indicate this by using the proportional sign. Dealing with unnormalized posteriors is not a problem if we are only interested in the mode of the posterior distribution (MAP estimate) as the normalization factor is not needed for this. But how can we calculate the MAP estimate for a given individual?\n\n\n4.4.3 MAP estimation\nIf we are interested to find the most likely parameter for our individual, we have to find the maximum of the posterior distribution (MAP estimate). Mathematically we can define this by finding the parameter \\(\\eta_i^*\\) that maximizes the posterior distribution:\n\\[\\eta_i^* = \\underset{\\eta_i}{\\mathrm{argmax}}~ p(\\eta_i|Y_{i}) = \\underset{\\eta_i}{\\mathrm{argmax}}~ p(\\eta_i) \\cdot \\prod_{j=1}^{m} p(Y_{ij}|\\eta_i)\\]\nDealing with the product of probabilities is often not very handy and we can simplify this by taking the log of the posterior distribution:\n\\[\\eta_i^* = \\underset{\\eta_i}{\\mathrm{argmax}}~ \\log(p(\\eta_i|Y_{i})) = \\underset{\\eta_i}{\\mathrm{argmax}}~ \\log(p(\\eta_i)) + \\sum_{j=1}^{m} \\log(p(Y_{ij}|\\eta_i))\\]\nIn the end, we would have to use a numerical optimizer function to search the parameter space of \\(\\eta_i\\) to find the maximum of the posterior distribution (\\(\\eta_i^*\\)). To write out the full equation we need to define both terms, \\(p(\\eta_i)\\) and \\(p(Y_{ij}|\\eta_i)\\). Let’s go through them step by step.\n\n4.4.3.1 Prior term\nThe prior distribution for \\(p(\\eta_i)\\) needs to incorporate our beliefs and uncertainty about \\(\\eta_i\\) before we have seen the data. Bayesian estimation actually allows us to incorporate our prior information through different distributions and parameters (which is sometimes being criticized being too subjective). In pharmacometrics, you will often see that people use the same distribution based on a previously estimates model. This is usually a normal distribution with mean 0 and a variance \\(\\omega^2_{CL}\\) estimated by the NLME model. What is the intuition behind this? When searching the space of \\(\\eta_i\\) during \\(\\underset{\\eta_i}{\\mathrm{argmax}}\\), we need to evaluate at each point how likely the given \\(\\eta_i\\) is given the prior distribution. This means that individual random effects at the boundaries of the prior distribution are discouraged while those in the center are encouraged. Or in other words: We only want to move away from the typical individual estimate when we get a substantial gain in explaining the data. To calculate the contribution of the prior term, we rely on the probability density function of the normal distribution:\n\\[p(\\eta_i) = \\frac{1}{\\sqrt{2\\pi\\omega^2}} \\cdot \\exp\\left(-\\frac{(\\eta_i-\\mu)^2}{2\\omega^2}\\right)\\]\nAs \\(\\mu\\) is typically 0, we can simplify this to:\n\\[p(\\eta_i) = \\frac{1}{\\sqrt{2\\pi\\omega^2}} \\cdot \\exp\\left(-\\frac{\\eta_i^2}{2\\omega^2}\\right)\\]\nTaking the log yields:\n\\[\\log(p(\\eta_i)) = -0.5 \\log(2\\pi\\omega^2) - \\frac{\\eta_i^2}{2\\omega^2} \\]\nNow we are able to calculate the contribution of the prior term for a given individual \\(\\eta_i\\), as we know the variance \\(\\omega^2_{CL}\\) (0.11) from our NLME model.\n\n\n4.4.3.2 Likelihood term\nThe likelihood term \\(p(Y_{ij}|\\eta_i)\\) is the probability of observing the data point \\(Y_{ij}\\) given the parameter \\(\\eta_i\\). As we usually also assume that the residuals of our model predictions are normally distributed, we can also use the probability density function of the normal distribution to calculate this term. The likelihood term is then:\n\\[p(Y_{ij}|\\eta_i) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\cdot \\exp\\left(-\\frac{(Y_{ij}-f(x_{ij}; \\eta_i))^2}{2\\sigma^2}\\right)\\] where\n\n\\(Y_{ij}\\) is the observed data point of the individual \\(i\\) at time \\(j\\)\n\\(f(x_{ij}; \\eta_i)\\) is the model prediction for the individual \\(i\\) at time \\(j\\) given the individual random effect \\(\\eta_i\\) (for CL)\n\\(\\sigma^2\\) is the variance of the residual unexplained variability of the model\n\nPlease note that in our case, where we have a combined additive and proportional RUV model, \\(\\sigma^2\\) is the sum of both variances at time \\(j\\). We can take the log and get:\n\\[\\log(p(Y_{ij}|\\eta_i)) = -0.5 \\log(2\\pi\\sigma^2) - \\frac{(Y_{ij}-f(x_{ij}; \\eta_i))^2}{2\\sigma^2}\\]\n\n\n\n4.4.4 Combining both terms\nNow we can combine both terms:\n\\[\\eta_i^* = \\underset{\\eta_i}{\\mathrm{argmax}}~\\left(\\left[-0.5 \\log(2\\pi\\omega^2) - \\frac{\\eta_i^2}{2\\omega^2}\\right] ~ + \\left[-0.5 \\log(2\\pi\\sigma^2) - \\frac{(Y_{ij}-f(x_{ij}; \\eta_i))^2}{2\\sigma^2}\\right]\\right)\\]\nThis is the equation for which our numerical optimizer needs to find the maximum (or minimum if negated) to obtain the individual MAP estimate. Let’s define some functions and reproduce our NONMEM reference solution in R."
  },
  {
    "objectID": "posts/bayes_map_estimation_r/index.html#why-bayesian-estimation-is-important-to-us",
    "href": "posts/bayes_map_estimation_r/index.html#why-bayesian-estimation-is-important-to-us",
    "title": "Bayesian MAP estimation in R",
    "section": "",
    "text": "Bayesian estimation is a commonly applied and powerful method in pharmacometrics. Often the calculation happens kind of automatically when we fit a pharmacometric model using e.g., NONMEM. So it is quite easy to use them without dealing the underlying concepts and the actual computation of e.g., the individual parameter estimates provided as an output in NONMEM: Given that diagnostics relying on IPRED (the individual predictions obtained through simulating with the MAP parameter estimate) are commonly reported, and individual parameters are often used for covariate screening, the importance of this topic is clear. The most commonly applied Bayesian tool are so called MAP or EBE estimates, which represent the mode of the posterior distribution (we will talk about the posterior later). However, also the full posterior distribution can be of interest, although it is harder to calculate. In this blogpost, I want to shortly describe my own understanding of the Bayesian ideas and concepts and then show how to reproduce the MAP estimates and its resulting individual predictions (IPRED) in R. An equation-based reproduction is typically the best way to understand and learn about the underlying concept."
  },
  {
    "objectID": "posts/bayes_map_estimation_r/index.html#why-bayesian-estimation-is-important",
    "href": "posts/bayes_map_estimation_r/index.html#why-bayesian-estimation-is-important",
    "title": "Bayesian MAP estimation in R",
    "section": "1.1 Why Bayesian estimation is important",
    "text": "1.1 Why Bayesian estimation is important\nBayesian estimation is a commonly applied and powerful method in pharmacometrics. Often the calculation happens kind of automatically when we fit a pharmacometric model using e.g., NONMEM. So it is quite easy to use them without dealing the underlying concepts and the actual computation of e.g., the individual parameter estimates provided as an output in NONMEM: Given that diagnostics relying on IPRED (the individual predictions obtained through simulating with the MAP parameter estimate) are commonly reported, and individual parameters are often used for covariate screening, the importance of this topic is clear. The most commonly applied Bayesian tool are so called MAP or EBE estimates, which represent the mode of the posterior distribution (we will talk about the posterior later). However, also the full posterior distribution can be of interest, although it is harder to calculate. In this blogpost, I want to shortly describe my own understanding of the Bayesian ideas and concepts and then show how to reproduce the MAP estimates and its resulting individual predictions (IPRED) in R. An equation-based reproduction is typically the best way to understand and learn about the underlying concept."
  },
  {
    "objectID": "posts/bayes_map_estimation_r/index.html#pharmacokinetic-nlme-model",
    "href": "posts/bayes_map_estimation_r/index.html#pharmacokinetic-nlme-model",
    "title": "Bayesian MAP estimation in R",
    "section": "4.1 Pharmacokinetic NLME model",
    "text": "4.1 Pharmacokinetic NLME model\nFor a little example and the reference solution, we will use the same simple one-compartment i.v. model with first order disposition processes we have fitted to some data here [REF]. This was the model structure:\n\n\n\nModel structure of our simple 1 cmt i.v. bolus model.\n\n\nWe assume to have an already fitted model and our only task is to individualize the parameters for our given individual. The model parameter estimates which we are using are taken from the previous blogpost [REF]:\n\n\\(CL\\) = 0.247 L/h\n\\(V\\) = 3.15 L\n\\(\\omega^2_{CL}\\) = 0.11\n\\(\\sigma^2_{RUV}\\) = 0.10"
  },
  {
    "objectID": "posts/bayes_map_estimation_r/index.html#data",
    "href": "posts/bayes_map_estimation_r/index.html#data",
    "title": "Bayesian MAP estimation in R",
    "section": "4.1 Data",
    "text": "4.1 Data\nWithout data no Bayesian parameter individualization. As we do this MAP estimation on an individual level, we are going to pick one single ID from the dataset we have defined in the previous blogpost [REF]and use this as as our example. Let’s first load the simulated concentration-time data and show the head of it:\n\n\nCode\n# read in simulated dataset from previous blogpost\nsim_data &lt;- read_csv(\"~/GitHub/personal-website/posts/understanding_nlme_estimation/data/output_from_sim/sim_data.csv\")\n\n# show data \nsim_data |&gt;  \n  head() |&gt; \n  kable() |&gt; \n  kable_styling()\n\n\n\n\n\nID\nTIME\nEVID\nAMT\nRATE\nDV\nMDV\n\n\n\n\n1\n0.00\n1\n100\n0\n0.0000\n1\n\n\n1\n0.01\n0\n0\n0\n30.8160\n0\n\n\n1\n3.00\n0\n0\n0\n24.5520\n0\n\n\n1\n6.00\n0\n0\n0\n17.9250\n0\n\n\n1\n12.00\n0\n0\n0\n10.1100\n0\n\n\n1\n24.00\n0\n0\n0\n3.5975\n0\n\n\n\n\n\n\n\nThese is the previously simulated data with which we have built the model. We will focus on one single ID (ID 5) which is rather at the lower end of the simulated concentrations. We did this as it allows us to better see the differences between the typical individual and the individualized one. Let’s plot the data to get a better understanding of the concentration-time profiles:\n\n\nCode\n# show individual profiles\nsim_data |&gt; \n  filter(EVID == 0) |&gt; \n  mutate(flag = if_else(ID == 5, \"Reference\", \"Others\")) |&gt;\n  ggplot(aes(x=TIME, y=DV, group=ID, color=as.factor(flag))) +\n  geom_point()+\n  geom_line()+\n  theme_bw()+\n  scale_y_continuous(limits=c(0,NA))+\n  labs(x=\"Time after last dose [h]\", y=\"Concentration [mg/L]\")+\n  scale_color_manual(\"Individual\", values=c(\"grey\", \"darkblue\"))+\n  ggtitle(\"Simulated data\")\n\n\n\n\n\n\n\n\nFigure 1: Simulated concentration-time profiles for 10 individuals with ID 5 being our examplaratory ID.\n\n\n\n\n\nWe will later need this data when running NONMEM, so we have to save it to file.\n\n\nCode\n# define sim_data with ID == 5\nsim_data_id5 &lt;- sim_data |&gt; \n  filter(ID == 5)\n\n# save data for NONMEM\nsim_data_id5 |&gt; \n  write_csv(\"~/GitHub/personal-website/posts/bayes_map_estimation_r/data/sim_data_ID5.csv\")"
  },
  {
    "objectID": "posts/bayes_map_estimation_r/index.html#nlme-model-structure",
    "href": "posts/bayes_map_estimation_r/index.html#nlme-model-structure",
    "title": "Bayesian MAP estimation in R",
    "section": "4.2 NLME model structure",
    "text": "4.2 NLME model structure\nFor a little example and the reference solution, we will use the same simple one-compartment i.v. model with first order disposition processes we have fitted to some data here [REF]. This was the model structure:\n\n\n\nModel structure of our simple 1 cmt i.v. bolus model.\n\n\nWe assume to have an already fitted model and our only task is to individualize the parameters for our given individual. The model parameter estimates which we are using are mainly taken from the previous blogpost [REF]:\n\n\\(CL\\) = 0.247 L/h\n\\(V\\) = 3.15 L\n\\(\\omega^2_{CL}\\) = 0.11\n\\(\\sigma^2_{RUV\\_ADD}\\) = 0.10\n\\(\\sigma^2_{RUV\\_PROP}\\) = 0.10\n\nThis is what we are now going to translate into a NONMEM model. Please note that we added a proportional error term to the previously used model to better make a point when comparing prior, likelihood and posterior."
  },
  {
    "objectID": "posts/bayes_map_estimation_r/index.html#nonmem-model",
    "href": "posts/bayes_map_estimation_r/index.html#nonmem-model",
    "title": "Bayesian MAP estimation in R",
    "section": "4.3 NONMEM model",
    "text": "4.3 NONMEM model\nThe NONMEM model is based on the previous blogpost, but we have made some minor edits:\n\n\n1cmt_iv_map_est.mod\n\n$PROBLEM 1cmt_iv_map_est\n\n$INPUT ID TIME EVID AMT RATE DV MDV\n\n$DATA C:\\Users\\maria\\Documents\\GitHub\\personal-website\\posts\\bayes_map_estimation_r\\data\\sim_data_ID5.csv IGNORE=@\n\n$SUBROUTINES ADVAN1 TRANS2\n\n$PK\n; define fixed effects parameters\nCL = THETA(1) * EXP(ETA(1))\nV = THETA(2)\n\n; scaling\nS1=V\n\n$THETA\n0.247 FIX               ; 1 TVCL\n3.15 FIX                ; 2 TVV\n\n$OMEGA \n0.11 FIX                ; 1 OM_CL\n\n$SIGMA\n0.10 FIX                ; 1 SIG_PROP\n0.10 FIX                ; 2 SIG_ADD\n\n$ERROR \n; add residual unexplained variability\nIPRED = F\nY = IPRED + IPRED * EPS(1) + EPS(2)\n\n$ESTIMATION METHOD=COND LAPLACIAN MAXEVAL=0 SIGDIG=3 PRINT=1 NOABORT POSTHOC\n\n$TABLE ID TIME EVID AMT RATE DV PRED IPRED MDV ETA1 CL NOAPPEND ONEHEADER NOPRINT FILE=map_estim_out\n\nWe have updated the initial parameter estimates to the estimates listed above. Furthermore, we run with MAXEVAL=0 as we don’t need a full estimation but only a MAP estimation. The POSTHOC option will force NONMEM to calculate the individual MAP estimates. We can now go ahead and run it.\nAfter executing the model, we can read in map_estim_out:\n\n\nCode\n# load simulated data\nnm_out &lt;- read_nm_table(\"~/GitHub/personal-website/posts/bayes_map_estimation_r/models/map_estim_out\")\n\n# show simulated data\nnm_out |&gt; \n  head() |&gt; \n  kable() |&gt; \n  kable_styling()\n\n\n\n\n\nID\nTIME\nEVID\nAMT\nRATE\nDV\nPRED\nIPRED\nMDV\nETA1\nCL\n\n\n\n\n5\n0.00\n1\n100\n0\n0.00000\n31.7460\n31.7460\n1\n0.47652\n0.39778\n\n\n5\n0.01\n0\n0\n0\n31.61900\n31.7210\n31.7060\n0\n0.47652\n0.39778\n\n\n5\n3.00\n0\n0\n0\n19.66000\n25.0920\n21.7350\n0\n0.47652\n0.39778\n\n\n5\n6.00\n0\n0\n0\n12.00700\n19.8320\n14.8810\n0\n0.47652\n0.39778\n\n\n5\n12.00\n0\n0\n0\n4.33360\n12.3890\n6.9755\n0\n0.47652\n0.39778\n\n\n5\n24.00\n0\n0\n0\n0.78148\n4.8349\n1.5327\n0\n0.47652\n0.39778\n\n\n\n\n\n\n\nETA1 (or \\(\\eta_i\\)) is the individual random effect for the individual and CL (better \\(CL_i\\)) represents the resulting individual MAP estimate for the clearance. CL_i can be obtained by\n\\[CL_i = \\theta_{TVCL} \\cdot \\exp(\\eta_i)\\]\nWe can confirm this by calculating:\n\\[CL_i = 0.247 \\cdot \\exp(0.4763) = 0.3977\\]\nor directly in R:\n\n# calculate individual MAP estimate\n0.247*exp(0.4763)\n\n[1] 0.3976962\n\n\nGreat! Now we have our reference solution for the parameter individualization. We not only got the individualized parameter estimates but also the individual predictions (IPRED). We can now compare both visually:\n\n\nCode\n# plot DV, PRED, IPRED\nnm_out |&gt; \n  filter(TIME &gt; 0) |&gt; \n  pivot_longer(cols=c(PRED, IPRED, DV), names_to=\"variable\", values_to=\"value\") |&gt;\n  ggplot(aes(x=TIME, y=value, group=variable, color=variable))+\n  geom_point()+\n  geom_line()+\n  labs(\n    y = \"Concentration [mg/L]\",\n    x = \"Time after last dose [h]\",\n    title = \"Comparison of DV, PRED and IPRED\"\n  ) +\n  theme_bw() \n\n\n\n\n\n\n\n\n\nWe can see at a first glance that our reference individual (red, DV) differs substantially from our typical individual (blue, PRED). The individual predictions (green, IPRED), which are based on our MAP estimate for CL (CL_i), are in between. Let’s have a closer look how to get there."
  },
  {
    "objectID": "posts/bayes_map_estimation_r/index.html#prior-term-1",
    "href": "posts/bayes_map_estimation_r/index.html#prior-term-1",
    "title": "Bayesian MAP estimation in R",
    "section": "5.1 Prior term",
    "text": "5.1 Prior term\nWe will start to define a function with the prior term\n\n\n\nfunction: prior_fun()\n\n# define prior term function\nprior_fun &lt;- function(eta_i, omega2){\n  # calculate probability\n  log_prob &lt;- - 0.5 * log(2*pi*omega2) - eta_i^2/(2*omega2)\n  \n  # return log probability\n  return(log_prob)\n}\n\n\nWe can now test this function and illustrate the prior term for a range of \\(\\eta_i\\) values:\n\n\nCode\n# define range\neta_i &lt;- seq(-2, 2, 0.01)\n\n# calculate prior term\nprior &lt;- prior_fun(eta_i = eta_i, omega2 = 0.11)\n\n# create tibble\nprior_tibble &lt;- tibble(\n  eta_i = eta_i,\n  exp_prior = exp(prior),\n  source = \"prior\"\n)\n\n# plot prior term\nprior_tibble |&gt; \n  ggplot(aes(x=eta_i, y=exp_prior))+\n  geom_line()+\n  labs(\n    x = expression(eta[i]),\n    y = expression(p(eta[i])),\n    title = \"Prior term (Random effect)\"\n  )+\n  theme_bw()\n\n\n\n\n\n\n\n\n\nand this is how the \\(\\eta_i\\) values translate to the Clearance domain:\n\n\nCode\n# plot prior term\nprior_tibble |&gt; \n  ggplot(aes(x=0.247*exp(eta_i), y=exp_prior))+\n  geom_line()+\n  labs(\n    x = expression(CL[i]),\n    y = expression(p(CL[i])),\n    title = \"Prior term (Clearance)\"\n  )+\n  theme_bw()\n\n\n\n\n\n\n\n\n\nWe can see that some values of \\(\\eta_i\\) and \\(CL_i\\) are more likely than others. Based on the results of the fitted NLME model, these distributions represent our prior beliefs before we see the data of the given individual."
  },
  {
    "objectID": "posts/bayes_map_estimation_r/index.html#why-bayesian-estimation-is-important-1",
    "href": "posts/bayes_map_estimation_r/index.html#why-bayesian-estimation-is-important-1",
    "title": "Bayesian MAP estimation in R",
    "section": "6.1 Why Bayesian estimation is important",
    "text": "6.1 Why Bayesian estimation is important\nBayesian estimation is a commonly applied and powerful method in pharmacometrics. Often the calculation happens kind of automatically when we fit a pharmacometric model using e.g., NONMEM. So it is quite easy to use them without dealing the underlying concepts and the actual computation of e.g., the individual parameter estimates provided as an output in NONMEM: Given that diagnostics relying on IPRED (the individual predictions obtained through simulating with the MAP parameter estimate) are commonly reported, and individual parameters are often used for covariate screening, the importance of this topic is clear. The most commonly applied Bayesian tool are so called MAP or EBE estimates, which represent the mode of the posterior distribution (we will talk about the posterior later). However, also the full posterior distribution can be of interest, although it is harder to calculate. In this blogpost, I want to shortly describe my own understanding of the Bayesian ideas and concepts and then show how to reproduce the MAP estimates and its resulting individual predictions (IPRED) in R. An equation-based reproduction is typically the best way to understand and learn about the underlying concept."
  },
  {
    "objectID": "posts/bayes_map_estimation_r/index.html#disclaimer-1",
    "href": "posts/bayes_map_estimation_r/index.html#disclaimer-1",
    "title": "Bayesian MAP estimation in R",
    "section": "6.2 Disclaimer",
    "text": "6.2 Disclaimer\nI just share my personal thoughts and understanding of some Bayesian ideas and I can’t promise that you won’t find flaws in my explanations or equations."
  },
  {
    "objectID": "posts/bayes_map_estimation_r/index.html#starting-with-an-example-1",
    "href": "posts/bayes_map_estimation_r/index.html#starting-with-an-example-1",
    "title": "Bayesian MAP estimation in R",
    "section": "8.1 Starting with an example",
    "text": "8.1 Starting with an example\nI would say that most people in their daily life think Bayesian (although not everyone wants to admit it). The core idea of Bayesian statistics is to update your beliefs once new data or information becomes available. One simple example: Assume you commute to work every day by using your car. If someone would ask you “How long do you think you will need to get to work tomorrow?”, you would probably have a very good idea about how long it will take you, simply based on your experience. You might say that 30 minutes is a good estimate, but would you bet money that it will be exactly 30 minutes? Most likely not, it might be your best guess but there will be some uncertainly associated to it. You might be willing to bet money that it will be between 20 and 40 minutes. Now assuming that you get into your car the next morning and traffic is a state so that after 25 minutes you made it half-way through.\n[IMAGE]\nWould you still stick to your initial belief that it will take you 30 minutes? Probably not, you would update your belief. Would you then just multiply the 25 minutes by two and say that it will take you 50 minutes? Probably not, given that all your experience tells you that you nearly always made it within 40 minutes. So your updated belief will be something between 30 minutes (initial guess) and 50 minutes (what the data suggests). This updated belief (let’s say 38 minutes) is what Bayesian statistics is all about. And since we apply this Bayesian idea in our daily life, I personally find it quite an intuitive way of thinking statistics."
  },
  {
    "objectID": "posts/bayes_map_estimation_r/index.html#nomenclature-1",
    "href": "posts/bayes_map_estimation_r/index.html#nomenclature-1",
    "title": "Bayesian MAP estimation in R",
    "section": "8.2 Nomenclature",
    "text": "8.2 Nomenclature\nThe real-life example defined above allows us to define some Bayesian nomenclature:\n\nThe prior is the initial belief, in our example the 30 minutes and the uncertainty around it.\nThe likelihood represents the new data (or evidence), in our example the 25 minutes for 1/2 of the way.\nThe posterior is the updated belief after the data became available, in our example the 38 minutes.\n\nSo the Bayesian workflow is always based on the idea that you have a prior belief, which you update once new data (likelihood) becomes available. The updated belief is then called the posterior. There are some differences if we talk about MAP estimation and full posterior estimation:\n\nMAP: the maximum a-posteriori estimate, which is the mode of the posterior distribution\nfull Bayesian: the full posterior distribution and not only the mode / a point estimate\n\nMost pharmacometricians focus on the mode of the posterior (MAP/EBE) and neglect the uncertainty captured in the full distribution. The main reason for this is that the MAP estimates are easy to calculate while the full posterior is more challenging to obtain.\nIt goes without a saying that this is a simple example and everything is just based on a gut-feeling than on equations. While the core-idea remains the same, Bayesian statistics provide a framework to calculate the posterior in a more formal way. This is what we are tackling next."
  },
  {
    "objectID": "posts/bayes_map_estimation_r/index.html#our-goal-in-pharmacometrics-1",
    "href": "posts/bayes_map_estimation_r/index.html#our-goal-in-pharmacometrics-1",
    "title": "Bayesian MAP estimation in R",
    "section": "8.3 Our goal in pharmacometrics",
    "text": "8.3 Our goal in pharmacometrics\nBut what is now the goal of Bayesian stats in pharmacometrics? Actually not to better plan your daily commute to work. Instead, we typically use Bayesian estimation to individualize the model parameters for a given individual \\(i\\), which has a set of observations \\(Y_{i}\\). Now we want to use this individual data to find the best parameter estimates for this individual. On the one hand this happens during model building, at each iteration step and then after the actual estimation has finished, at the posthoc step. On the other hand, it also happens in so called model-informed precision dosing scenarios (MIPD) where we want to predict the future concentration-time profile and we have obtained some historic concentrations through therapeutic drug monitoring."
  },
  {
    "objectID": "posts/bayes_map_estimation_r/index.html#data-1",
    "href": "posts/bayes_map_estimation_r/index.html#data-1",
    "title": "Bayesian MAP estimation in R",
    "section": "9.1 Data",
    "text": "9.1 Data\nWithout data no Bayesian parameter individualization. As we do this MAP estimation on an individual level, we are going to pick one single ID from the dataset we have defined in the previous blogpost [REF]and use this as as our example. Let’s first load the simulated concentration-time data and show the head of it:\nquarto-executable-code-5450563D\n# read in simulated dataset from previous blogpost\nsim_data &lt;- read_csv(\"~/GitHub/personal-website/posts/understanding_nlme_estimation/data/output_from_sim/sim_data.csv\")\n\n# show data \nsim_data |&gt;  \n  head() |&gt; \n  kable() |&gt; \n  kable_styling()\nThese is the previously simulated data with which we have built the model. We will focus on one single ID (ID 5) which is rather at the lower end of the simulated concentrations. We did this as it allows us to better see the differences between the typical individual and the individualized one. Let’s plot the data to get a better understanding of the concentration-time profiles:\nquarto-executable-code-5450563D\n#| label: fig-sim-vis\n#| fig-cap: \"Simulated concentration-time profiles for 10 individuals with ID 5 being our examplaratory ID.\"\n\n# show individual profiles\nsim_data |&gt; \n  filter(EVID == 0) |&gt; \n  mutate(flag = if_else(ID == 5, \"Reference\", \"Others\")) |&gt;\n  ggplot(aes(x=TIME, y=DV, group=ID, color=as.factor(flag))) +\n  geom_point()+\n  geom_line()+\n  theme_bw()+\n  scale_y_continuous(limits=c(0,NA))+\n  labs(x=\"Time after last dose [h]\", y=\"Concentration [mg/L]\")+\n  scale_color_manual(\"Individual\", values=c(\"grey\", \"darkblue\"))+\n  ggtitle(\"Simulated data\")\nWe will later need this data when running NONMEM, so we have to save it to file.\nquarto-executable-code-5450563D\n# save data for NONMEM\nsim_data |&gt; \n  filter(ID == 5) |&gt; \n  write_csv(\"~/GitHub/personal-website/posts/bayes_map_estimation_r/data/sim_data_ID5.csv\")"
  },
  {
    "objectID": "posts/bayes_map_estimation_r/index.html#nlme-model-structure-1",
    "href": "posts/bayes_map_estimation_r/index.html#nlme-model-structure-1",
    "title": "Bayesian MAP estimation in R",
    "section": "9.2 NLME model structure",
    "text": "9.2 NLME model structure\nFor a little example and the reference solution, we will use the same simple one-compartment i.v. model with first order disposition processes we have fitted to some data here [REF]. This was the model structure:\n\n\n\nModel structure of our simple 1 cmt i.v. bolus model.\n\n\nWe assume to have an already fitted model and our only task is to individualize the parameters for our given individual. The model parameter estimates which we are using are mainly taken from the previous blogpost [REF]:\n\n\\(CL\\) = 0.247 L/h\n\\(V\\) = 3.15 L\n\\(\\omega^2_{CL}\\) = 0.11\n\\(\\sigma^2_{RUV\\_ADD}\\) = 0.10\n\\(\\sigma^2_{RUV\\_PROP}\\) = 0.10\n\nThis is what we are now going to translate into a NONMEM model. Please note that we added a proportional error term to the previously used model to better make a point when comparing prior, likelihood and posterior."
  },
  {
    "objectID": "posts/bayes_map_estimation_r/index.html#nonmem-model-1",
    "href": "posts/bayes_map_estimation_r/index.html#nonmem-model-1",
    "title": "Bayesian MAP estimation in R",
    "section": "9.3 NONMEM model",
    "text": "9.3 NONMEM model\nThe NONMEM model is based on the previous blogpost, but we have made some minor edits:\n\n\n1cmt_iv_map_est.mod\n\n$PROBLEM 1cmt_iv_map_est\n\n$INPUT ID TIME EVID AMT RATE DV MDV\n\n$DATA C:\\Users\\maria\\Documents\\GitHub\\personal-website\\posts\\bayes_map_estimation_r\\data\\sim_data_ID5.csv IGNORE=@\n\n$SUBROUTINES ADVAN1 TRANS2\n\n$PK\n; define fixed effects parameters\nCL = THETA(1) * EXP(ETA(1))\nV = THETA(2)\n\n; scaling\nS1=V\n\n$THETA\n0.247 FIX               ; 1 TVCL\n3.15 FIX                ; 2 TVV\n\n$OMEGA \n0.11 FIX                ; 1 OM_CL\n\n$SIGMA\n0.10 FIX                ; 1 SIG_PROP\n0.10 FIX                ; 2 SIG_ADD\n\n$ERROR \n; add residual unexplained variability\nIPRED = F\nY = IPRED + IPRED * EPS(1) + EPS(2)\n\n$ESTIMATION METHOD=COND LAPLACIAN MAXEVAL=0 SIGDIG=3 PRINT=1 NOABORT POSTHOC\n\n$TABLE ID TIME EVID AMT RATE DV PRED IPRED MDV ETA1 CL NOAPPEND ONEHEADER NOPRINT FILE=map_estim_out\n\nWe have updated the initial parameter estimates to the estimates listed above. Furthermore, we run with MAXEVAL=0 as we don’t need a full estimation but only a MAP estimation. The POSTHOC option will force NONMEM to calculate the individual MAP estimates. We can now go ahead and run it.\nAfter executing the model, we can read in map_estim_out:\nquarto-executable-code-5450563D\n# load simulated data\nnm_out &lt;- read_nm_table(\"~/GitHub/personal-website/posts/bayes_map_estimation_r/models/map_estim_out\")\n\n# show simulated data\nnm_out |&gt; \n  head() |&gt; \n  kable() |&gt; \n  kable_styling()\nETA1 (or \\(\\eta_i\\)) is the individual random effect for the individual and CL (better \\(CL_i\\)) represents the resulting individual MAP estimate for the clearance. CL_i can be obtained by\n\\[CL_i = \\theta_{TVCL} \\cdot \\exp(\\eta_i)\\]\nWe can confirm this by calculating:\n\\[CL_i = 0.247 \\cdot \\exp(0.4763) = 0.3977\\]\nor directly in R:\nquarto-executable-code-5450563D\n#| code-fold: FALSE\n\n# calculate individual MAP estimate\n0.247*exp(0.4763)\nGreat! Now we have our reference solution for the parameter individualization. We not only got the individualized parameter estimates but also the individual predictions (IPRED). We can now compare both visually:\nquarto-executable-code-5450563D\n# plot DV, PRED, IPRED\nnm_out |&gt; \n  filter(TIME &gt; 0) |&gt; \n  pivot_longer(cols=c(PRED, IPRED, DV), names_to=\"variable\", values_to=\"value\") |&gt;\n  ggplot(aes(x=TIME, y=value, group=variable, color=variable))+\n  geom_point()+\n  geom_line()+\n  labs(\n    y = \"Concentration [mg/L]\",\n    x = \"Time after last dose [h]\",\n    title = \"Comparison of DV, PRED and IPRED\"\n  ) +\n  theme_bw() \nWe can see at a first glance that our reference individual (red, DV) differs substantially from our typical individual (blue, PRED). The individual predictions (green, IPRED), which are based on our MAP estimate for CL (CL_i), are in between. Let’s have a closer look how to get there."
  },
  {
    "objectID": "posts/bayes_map_estimation_r/index.html#bayesian-theory-1",
    "href": "posts/bayes_map_estimation_r/index.html#bayesian-theory-1",
    "title": "Bayesian MAP estimation in R",
    "section": "9.4 Bayesian Theory",
    "text": "9.4 Bayesian Theory\n\n9.4.1 General form\nAs described above, the main goal of a Bayesian attempt is to obtain the posterior distribution (either the mode or the full distribution). Often you find this formula to describe the Bayes theorem:\n\\[P(A|B) = \\frac{P(A) \\cdot P(B|A)}{P(B)}\\]\nHere, conditional probabilities are used and \\(P(A|B)\\) is the probability of event A given that event B has occurred. This remains quite theoretical. Let’s directly translate it into the context of a pharmacokinetic NLME model.\n\n\n9.4.2 Pharmacometric context\nOur use-case in the world of pharmacometrics is to find the most likely parameter \\(\\eta_i\\) (which then translates to \\(CL_i\\)) given our individual’s concentration data \\(Y_{i}\\):\n\\[p(\\eta|Y_{i}) = \\frac{p(\\eta) \\cdot p(Y_{i}|\\eta)}{p(Y_i)}\\]\nwith\n\n\\(p(\\eta|Y_{ij})\\): the posterior distribution of the parameter \\(\\eta\\) given the data of our ith individual \\(Y_{i}\\)\n\\(p(\\eta)\\): the prior distribution of the parameter \\(\\eta\\)\n\\(p(Y_{i}|\\eta)\\): the likelihood of the data \\(Y_{i}\\) given the parameter \\(\\eta\\)\n\\(p(Y_{i})\\): the marginal likelihood of the data \\(Y_{i}\\)\n\nThe marginal likelihood of the data \\(Y_{i}\\) is often neglected since it is just a scaling factor, does not depend on the parameter \\(\\eta\\) and is typically hard to calculate as it contains a high-dimensional integral. That is why you often see the formula written as:\n\\[p(\\eta|Y_{i}) \\propto p(\\eta) \\cdot p(Y_{i}|\\eta)\\]\nHere, we got rid of the marginal likelihood in the denominator. As we are now missing out the scaling factor, we now deal with an unnormalized posterior distribution and indicate this by using the proportional sign. Dealing with unnormalized posteriors is not a problem if we are only interested in the mode of the posterior distribution (MAP estimate) as the normalization factor is not needed for this. But how can we calculate the MAP estimate for a given individual?\n\n\n9.4.3 MAP estimation\nIf we are interested to find the most likely parameter for our individual, we have to find the maximum of the posterior distribution (MAP estimate). Mathematically we can define this by finding the parameter \\(\\eta_i^*\\) that maximizes the posterior distribution:\n\\[\\eta_i^* = \\underset{\\eta_i}{\\mathrm{argmax}}~ p(\\eta_i|Y_{i}) = \\underset{\\eta_i}{\\mathrm{argmax}}~ p(\\eta_i) \\cdot \\sum_{j=1}^{m} p(Y_{ij}|\\eta_i)\\]\nIn the end, we would have to use a numerical optimizer function to search the parameter space of \\(\\eta_i\\) to find the maximum of the posterior distribution (\\(\\eta_i^*\\)). To write out the full equation we need to define both terms, \\(p(\\eta_i)\\) and \\(p(Y_{ij}|\\eta_i)\\). Let’s go through them step by step.\n\n9.4.3.1 Prior term\nThe prior distribution for \\(p(\\eta_i)\\) needs to incorporate our beliefs and uncertainty about \\(\\eta_i\\) before we have seen the data. Bayesian estimation actually allows us to incorporate our prior information through different distributions and parameters (which is sometimes being criticized being too subjective). In pharmacometrics, you will often see that people use the same distribution based on a previously estimates model. This is usually a normal distribution with mean 0 and a variance \\(\\omega^2_{CL}\\) estimated by the NLME model. What is the intuition behind this? When searching the space of \\(\\eta_i\\) during \\(\\underset{\\eta_i}{\\mathrm{argmax}}\\), we need to evaluate at each point how likely the given \\(\\eta_i\\) is given the prior distribution. This means that individual random effects at the boundaries of the prior distribution are discouraged while those in the center are encouraged. Or in other words: We only want to move away from the typical individual estimate when we get a substantial gain in explaining the data. To calculate the contribution of the prior term, we rely on the probability density function of the normal distribution:\n\\[p(\\eta_i) = \\frac{1}{\\sqrt{2\\pi\\omega^2}} \\cdot \\exp\\left(-\\frac{(\\eta_i-\\mu)^2}{2\\omega^2}\\right)\\]\nAs \\(\\mu\\) is typically 0, we can simplify this to:\n\\[p(\\eta_i) = \\frac{1}{\\sqrt{2\\pi\\omega^2}} \\cdot \\exp\\left(-\\frac{\\eta_i^2}{2\\omega^2}\\right)\\]\nNow we are able to calculate the contribution of the prior term for a given individual \\(\\eta_i\\), as we know the variance \\(\\omega^2_{CL}\\) (0.11) from our NLME model.\n\n\n9.4.3.2 Likelihood term\nThe likelihood term \\(p(Y_{ij}|\\eta_i)\\) is the probability of observing the data point \\(Y_{ij}\\) given the parameter \\(\\eta_i\\). As we usually also assume that the residuals of our model predictions are normally distributed, we can also use the probability density function of the normal distribution to calculate this term. The likelihood term is then:\n\\[p(Y_{ij}|\\eta_i) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\cdot \\exp\\left(-\\frac{(Y_{ij}-f(x_{ij}; \\eta_i))^2}{2\\sigma^2}\\right)\\] where\n\n\\(Y_{ij}\\) is the observed data point of the individual \\(i\\) at time \\(j\\)\n\\(f(x_{ij}; \\eta_i)\\) is the model prediction for the individual \\(i\\) at time \\(j\\) given the individual random effect \\(\\eta_i\\) (for CL)\n\\(\\sigma^2\\) is the variance of the residual unexplained variability of the model\n\nPlease note that in our case, where we have a combined additive and proportional RUV model, \\(\\sigma^2\\) is the sum of both variances at time \\(j\\).\n\n\n\n9.4.4 Combining both terms\nNow we can combine both terms:\n\\[\\eta_i^* = \\underset{\\eta_i}{\\mathrm{argmax}}~\\left(\\left[\\frac{1}{\\sqrt{2\\pi\\omega^2}} \\cdot \\exp\\left(-\\frac{\\eta_i^2}{2\\omega^2}\\right)\\right] ~ \\cdot \\left[ \\sum_{j=1}^{m} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\cdot \\exp\\left(-\\frac{(Y_{ij}-f(x_{ij}; \\eta_i))^2}{2\\sigma^2}\\right)\\right]\\right)\\]\nThis is the equation for which our numerical optimizer needs to find the maximum (or minimum if negated) to obtain the individual MAP estimate. Let’s define some functions and reproduce our NONMEM reference solution in R."
  },
  {
    "objectID": "posts/bayes_map_estimation_r/index.html#prior-term-3",
    "href": "posts/bayes_map_estimation_r/index.html#prior-term-3",
    "title": "Bayesian MAP estimation in R",
    "section": "10.1 Prior term",
    "text": "10.1 Prior term\nWe will start to define a function with the prior term\n```{r filename=“function: prior_term()”} #| echo: true #| collapse: false #| code-fold: false"
  },
  {
    "objectID": "posts/bayes_map_estimation_r/index.html#likelihood-term-1",
    "href": "posts/bayes_map_estimation_r/index.html#likelihood-term-1",
    "title": "Bayesian MAP estimation in R",
    "section": "5.2 Likelihood term",
    "text": "5.2 Likelihood term\nFor the Likelihood term described in [IREF] we need to define the model prediction function. The simple 1 cmt model structure we are using allows us to use a closed-form expression of the model instead of relying on ODE-based numerical solutions (see [REF]). But the concepts remain the same and you could apply this also if you have an ODE-based system. Let’s first define the model prediction function:\n\n\n\nfunction: model_fun()\n\n# define model function\nmodel_fun &lt;- function(eta_i, dose, vd, theta_tvcl, t) {\n  exp_eta_i &lt;- exp(eta_i)\n  exponent &lt;- -1 * (theta_tvcl * exp_eta_i / vd) * t\n  result &lt;- (dose / vd) * exp(exponent)\n  return(result)\n}\n\n\nWe can now use the prediction function and build the likelihood term function:\n\n\n\nfunction: likelihood_fun()\n\n# define likelihood term function\nlikelihood_fun &lt;- function(eta_i, dose, vd, theta_tvcl, t, Y_i, sigma2_add, sigma2_prop){\n  \n  # get model predictions\n  f_i &lt;- model_fun(eta_i, dose, vd, theta_tvcl, t)\n  \n  # calculate resulting sigma2 for each timepoint (prop variance is scaled based on f_i^2)\n  sigma2 &lt;- sigma2_prop * f_i^2 + sigma2_add\n  \n  # calculate probability\n  log_lik &lt;- - 0.5 * log(2*pi*sigma2) - (Y_i - f_i)^2/(2*sigma2)\n  \n  # take sum of likelihoods\n  log_lik_sum &lt;- sum(log_lik)\n  \n  # return log likelihood\n  return(log_lik_sum)\n}\n\n\nWe can now test this function and illustrate the likelihood term for a range of \\(\\eta_i\\) values:\n\n\nCode\n# define range\neta_i &lt;- seq(-2, 2, 0.01)\n\n# empty list\nll_list &lt;- list()\n\n# loop over each element of eta_i\nfor(cur_eta_i in eta_i){\n  # calculate likelihood term\n  ll_list[[as.character(cur_eta_i)]] &lt;- likelihood_fun(\n    eta_i = cur_eta_i, \n    dose = nm_out |&gt; filter(EVID == 1) |&gt; pull(AMT), \n    vd = 3.15, \n    theta_tvcl = 0.247, \n    t = nm_out |&gt; filter(EVID == 0) |&gt; pull(TIME), \n    Y_i = nm_out |&gt; filter(EVID == 0) |&gt; pull(DV), \n    sigma2_add = 0.10, \n    sigma2_prop = 0.10\n  )\n}\n\n# convert to vector\nll_vector &lt;- ll_list |&gt; unlist() |&gt; unname()\n\n# create tibble\nlikelihood_tibble &lt;- tibble(\n  eta_i = eta_i,\n  likelihood = ll_vector,\n  source = \"likelihood\"\n)\n\n# plot likelihood term\nlikelihood_tibble |&gt; \n  ggplot(aes(x=eta_i, y=exp(likelihood)))+\n  geom_line()+\n  labs(\n    x = expression(eta[i]),\n    y = expression(p(Y[i] | eta[i])),\n    title = \"Likelihood term\"\n  )+\n  theme_bw()\n\n\n\n\n\n\n\n\n\nHere we can see which values of \\(\\eta_i\\) are more and less likely to describe the observed data \\(Y_i\\) of our example individual. Now we can go ahead and combine both terms to calculate the MAP estimate for our individual."
  },
  {
    "objectID": "posts/bayes_map_estimation_r/index.html#map-estimation-1",
    "href": "posts/bayes_map_estimation_r/index.html#map-estimation-1",
    "title": "Bayesian MAP estimation in R",
    "section": "5.3 MAP estimation",
    "text": "5.3 MAP estimation\nWe now have to define an objective function for the numerical optimizer which will maximize the posterior distribution as defined in [IREF]. We simply have to put together both terms:\n\n\n\nfunction: map_obj_fun()\n\n# define MAP estimation objective function\nmap_obj_fun &lt;- function(eta_i, omega2, dose, vd, theta_tvcl, t, Y_i, sigma2_add, sigma2_prop){\n  \n  # calculate log prior term\n  log_prior_term &lt;- prior_fun(\n    eta_i = eta_i, \n    omega2 = omega2\n  )\n  \n  # calculate log likelihood term\n  log_likelihood_term &lt;- likelihood_fun(\n    eta_i = eta_i, \n    dose = dose, \n    vd = vd, \n    theta_tvcl = theta_tvcl, \n    t = t, \n    Y_i = Y_i, \n    sigma2_add = sigma2_add, \n    sigma2_prop = sigma2_prop\n  )\n  \n  # combine both\n  log_posterior &lt;- log_prior_term + log_likelihood_term\n  \n  # return negative log posterior\n  return(-log_posterior)\n}\n\n\nPlease note that we are returning the negative log posterior as it is easier to minimize a function than to maximize it. We can now use this objective function to find the MAP estimate for our individual using the optim function in R. We want to find the particular value of \\(\\eta_i\\) which minimizes our map_obj_fun() function.\n\n\nCode\n# run optimization\nmap_est &lt;- optim(\n  par = 0, \n  fn = map_obj_fun, \n  omega2 = 0.11, \n  dose = nm_out |&gt; filter(EVID == 1) |&gt; pull(AMT), \n  vd = 3.15, \n  theta_tvcl = 0.247, \n  t = nm_out |&gt; filter(EVID == 0) |&gt; pull(TIME), \n  Y_i = nm_out |&gt; filter(EVID == 0) |&gt; pull(DV), \n  sigma2_add = 0.10, \n  sigma2_prop = 0.10,\n  method = \"BFGS\"\n)\n\n# show estimation results\nmap_est \n\n\n$par\n[1] 0.6869381\n\n$value\n[1] 11.6776\n\n$counts\nfunction gradient \n      30        7 \n\n$convergence\n[1] 0\n\n$message\nNULL"
  }
]