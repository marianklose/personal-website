[
  {
    "objectID": "publications/publications.html",
    "href": "publications/publications.html",
    "title": "Publications",
    "section": "",
    "text": "Middle author\n\n\nClinical trial simulation\n\n\nRShiny\n\n\nDuchenne muscular dystrophy\n\n\n\n\n\n\nOct 3, 2024\n\n\nKim J, Morales JF, Kang S, Klose M, Willcocks RJ, Daniels MJ, Belfiore-Oshan R, Walter GA, Rooney WD, Vandenborne K, Kim S\n\n\nCPT: Pharmacometrics & Systems Pharmacology\n\n\n\n\n\n\n\n\n\n\n\n\nSecond author\n\n\nClinical trial simulation\n\n\nRShiny\n\n\nType 1 diabetes\n\n\n\n\n\n\nJul 3, 2024\n\n\nMorales JF, Klose M, Hoffert Y, Podichetty JT, Burton J, Schmidt S, Romero K, O’Doherty I, Martin F, Campbell-Thompson M, Haller MJ, Atkinson MA, Kim S.\n\n\nCPT: Pharmacometrics & Systems Pharmacology\n\n\n\n\n\n\n\n\n\n\n\n\nMiddle author\n\n\nDoxorubicin\n\n\nTLD-1\n\n\nPEGylated liposomal doxorubicin\n\n\nNLME PK\n\n\n\n\n\n\nJun 15, 2024\n\n\nMc Laughlin AM, Hess D, Michelet R, Colombo I, Haefliger S, Bastian S, Rabaglio M, Schwitter M, Fischer S, Eckhardt K, Hayoz S, Kopp C, Klose M, Sessa C, Stathis A, Halbherr S, Huisinga W, Joerger M, Kloft C.\n\n\nCancer Chemotherapy and Pharmacology\n\n\n\n\n\n\n\n\n\n\n\n\nFirst author\n\n\nPBPK\n\n\nOxycodone\n\n\nGDI\n\n\nDDI\n\n\nUGT2B7\n\n\nCYP2D6\n\n\n\n\n\n\nMar 1, 2024\n\n\nKlose M, Cristofoletti R, Silva CM, Mangal N, Turgeon J, Michaud V, Lesko LJ, Schmidt S.\n\n\nEuropean Journal of Pharmaceutical Sciences\n\n\n\n\n\n\n\n\n\n\n\n\nMiddle author\n\n\nTLD-1\n\n\nPEGylated liposomal doxorubicin\n\n\nDoxorubicin\n\n\nSafety\n\n\nEfficacy\n\n\nNCA\n\n\n\n\n\n\nFeb 2, 2024\n\n\nColombo I, Koster KL, Holer L, Haefliger S, Rabaglio M, Bastian S, Schwitter M, Eckhardt K, Hayoz S, Mc Laughlin AM, Kloft C, Klose M, Halbherr S, Baumgartner C, Sessa C, Stathis A, Hess D, Joerger M.\n\n\nEuropean Journal of Cancer\n\n\n\n\n\n\n\n\n\n\n\n\nSecond author\n\n\nMeropenem\n\n\nAntiinfectives\n\n\nNLME PK\n\n\n\n\n\n\nApr 20, 2021\n\n\nLiebchen U, Klose M, Paal M, Vogeser M, Zoller M, Schroeder I, Schmitt L, Huisinga W, Michelet R, Zander J, Scharf C, Weinelt FA, Kloft C.\n\n\nAntibiotics\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/publications.html#section",
    "href": "publications/publications.html#section",
    "title": "Publications",
    "section": "2024",
    "text": "2024\n\n\n\n\n\n\nA model-informed clinical trial simulation tool with a graphical user interface for Duchenne muscular dystrophy\n\n\n\n\n\nJournal: CPT: Pharmacometrics & Systems Pharmacology\nPublication Date: 2024-10-03\nDOI: 10.1002/psp4.13246\nAuthors: Kim J, Morales JF, Kang S, Klose M, Willcocks RJ, Daniels MJ, Belfiore-Oshan R, Walter GA, Rooney WD, Vandenborne K, Kim S.\n\n\n\n\n\n\n\n\n\nType 1 diabetes prevention clinical trial simulator: Case reports of model-informed drug development tool\n\n\n\n\n\nJournal: CPT: Pharmacometrics & Systems Pharmacology\nPublication Date: 2024-07-03\nDOI: 10.1002/psp4.13193\nAuthors: Morales JF, Klose M, Hoffert Y, Podichetty JT, Burton J, Schmidt S, Romero K, O’Doherty I, Martin F, Campbell-Thompson M, Haller MJ, Atkinson MA, Kim S.\n\n\n\n\n\n\n\n\n\nPopulation pharmacokinetics of TLD-1, a novel liposomal doxorubicin, in a phase I trial\n\n\n\n\n\nJournal: Cancer Chemother Pharmacol\nPublication Date: 2024-09\nDOI: 10.1007/s00280-024-04679-z\nAuthors: Mc Laughlin AM, Hess D, Michelet R, Colombo I, Haefliger S, Bastian S, Rabaglio M, Schwitter M, Fischer S, Eckhardt K, Hayoz S, Kopp C, Klose M, Sessa C, Stathis A, Halbherr S, Huisinga W, Joerger M, Kloft C.\n\n\n\n\n\n\n\n\n\nMachine learning-driven flattening of model priors: A comparative simulation study across multiple compounds\n\n\n\n\n\nEvent: PAGE (2024) Publication Date: 2024-06-26\nLink: https://www.page-meeting.org/default.asp?abstract=10858\nAuthors: Klose M, Thoma F, Kovar L, Huisinga W, Michelet R, Kloft C.\n\n\n\n\n\n\n\n\n\nTLD-1, a novel liposomal doxorubicin, in patients with advanced solid tumors: Dose escalation and expansion part of a multicenter open-label phase I trial (SAKK 65/16)\n\n\n\n\n\nJournal: European Journal of Cancer\nPublication Date: 2024-04\nDOI: 10.1016/j.ejca.2024.113588\nAuthors: Colombo I, Koster KL, Holer L, Haefliger S, Rabaglio M, Bastian S, Schwitter M, Eckhardt K, Hayoz S, Mc Laughlin AM, Kloft C, Klose M, Halbherr S, Baumgartner C, Sessa C, Stathis A, Hess D, Joerger M.\n\n\n\n\n\n\n\n\n\nExploring the impact of CYP2D6 and UGT2B7 gene-drug interactions, and CYP-mediated DDI on oxycodone and oxymorphone pharmacokinetics using physiologically-based pharmacokinetic modeling and simulation\n\n\n\n\n\nJournal: Eur J Pharm Sci\nPublication Date: 2024-03-01\nDOI: 10.1016/j.ejps.2023.106689\nAuthors: Klose M, Cristofoletti R, Silva CM, Mangal N, Turgeon J, Michaud V, Lesko LJ, Schmidt S."
  },
  {
    "objectID": "publications/publications.html#section-1",
    "href": "publications/publications.html#section-1",
    "title": "Publications",
    "section": "2022",
    "text": "2022\n\n\n\n\n\n\nThe Impact of UGT2B7 and CYP2D6 Gene-Drug- and CYP-mediated Drug-Drug-Interactions on Oxycodone and Oxymorphone Pharmacokinetics using PBPK Modelling.\n\n\n\n\n\nEvent: ACoP13 (2022)\nPublication Date: 2022\nLink: www.go-acop.org/?abstract=413\nAuthors: Klose M, Schmidt S, Cristofoletti R."
  },
  {
    "objectID": "publications/publications.html#section-2",
    "href": "publications/publications.html#section-2",
    "title": "Publications",
    "section": "2021",
    "text": "2021\n\n\n\n\n\n\nEvaluation of the MeroRisk Calculator, A User-Friendly Tool to Predict the Risk of Meropenem Target Non-Attainment in Critically Ill Patients\n\n\n\n\n\nJournal: Antibiotics (Basel)\nPublication Date: 2021-04-20\nDOI: 10.3390/antibiotics10040468\nAuthors: Liebchen U, Klose M, Paal M, Vogeser M, Zoller M, Schroeder I, Schmitt L, Huisinga W, Michelet R, Zander J, Scharf C, Weinelt FA, Kloft C.\n\n\n\n\n\n\n\n\n\nUsing microdose-based activity measurement to individualise dosing of cytochrome P450 metabolised drugs: a case study with yohimbine and tamoxifen.\n\n\n\n\n\nEvent: PAGE (2021) Publication Date: 2021\nLink: https://www.page-meeting.org/default.asp?abstract=9807\nAuthors: Michelet R, Weinelt FA, Klose M, Mc Laughlin AM, Kluwe F, Montefusco-Pereira C, Van Dyk M, Vay M, Huisinga W, Kloft C, Mikus G."
  },
  {
    "objectID": "posts/2024-10-05/index.html",
    "href": "posts/2024-10-05/index.html",
    "title": "About this blog",
    "section": "",
    "text": "As someone involved in pharmacometrics, I’ve often found that many concepts are either poorly documented or explained in such technical language that they become hard to grasp (at least for me). This can be quite frustrating when you’re trying to learn or understand something new for which you don’t have a suitable mathematical and statistical background.\nThere are many nice blogs and articles in the field of pharmacometrics, such as\n\nDanielle Navarro\nPMX Solutions\nTingjie Guo’s NMHelp\nmrgsolve\n\nFor instance, Danielle Navarro’s post on population pharmacokinetic models in stan was incredibly helpful in my understanding of implementing pop-PK models in Stan. She just explained the concepts in a very easy-to-understand way and it is at the same time quite entertaining to read. Please check out her blog, if you haven’t already!\nI often explain new concepts to myself using Quarto or RMarkdown documents and I thought I might as well share them as little blog posts. While I don’t expect to match the quality of the blogs I’ve mentioned, I still hope that some posts can be helpful to others who are tackling similar challenges."
  },
  {
    "objectID": "posts/2024-10-05/index.html#motivation",
    "href": "posts/2024-10-05/index.html#motivation",
    "title": "About this blog",
    "section": "",
    "text": "As someone involved in pharmacometrics, I’ve often found that many concepts are either poorly documented or explained in such technical language that they become hard to grasp (at least for me). This can be quite frustrating when you’re trying to learn or understand something new for which you don’t have a suitable mathematical and statistical background.\nThere are many nice blogs and articles in the field of pharmacometrics, such as\n\nDanielle Navarro\nPMX Solutions\nTingjie Guo’s NMHelp\nmrgsolve\n\nFor instance, Danielle Navarro’s post on population pharmacokinetic models in stan was incredibly helpful in my understanding of implementing pop-PK models in Stan. She just explained the concepts in a very easy-to-understand way and it is at the same time quite entertaining to read. Please check out her blog, if you haven’t already!\nI often explain new concepts to myself using Quarto or RMarkdown documents and I thought I might as well share them as little blog posts. While I don’t expect to match the quality of the blogs I’ve mentioned, I still hope that some posts can be helpful to others who are tackling similar challenges."
  },
  {
    "objectID": "posts/2024-10-05/index.html#disclaimers",
    "href": "posts/2024-10-05/index.html#disclaimers",
    "title": "About this blog",
    "section": "Disclaimers",
    "text": "Disclaimers\nBefore diving in, a few things to note:\n\nI’m not an expert in everything I write about; I’m learning as I go and simply documenting my process. Expect some mistakes along the way!\nEnglish is not my first language, so I rely on tools like ChatGPT and DeepL to help with writing. I also use tools like ChatGPT to generate ideas or to find good ways to visualize concepts.\nI do my best to give credit where it’s due and cite sources when appropriate. Much of what I share builds on others’ work, and I’m not trying to reinvent the wheel."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Marian Klose",
    "section": "",
    "text": "Hi there! I’m Marian Klose, currently a PhD student at Freie Universitaet Berlin, and part of the PharMetrX graduate research training program. I am directly supervised by Prof. Charlotte Kloft from Freie Universitaet Berlin and co-supervised by Prof. Wilhelm Huisinga from University of Potsdam.\nHow did I end up here? After my A-levels I did a work-and-travel year and then started studying chemistry and biochemisty. I realized that I wanted to work in a field that was a bit more applied, so I switched to pharmacy. There I got into pharmacometrics pretty early on and loved the mix of computer science, pharmacy, and math. After my studies, I headed to Florida for six months to work at the University of Florida’s Center for Pharmacometrics and Systems Pharmacology under supervison of Prof. Stephan Schmidt and Prof. Rodrigo Cristofoletti, and also did a three-month internship at Boehringer Ingelheim to see if my interest would remain steady. And I was still fascinated by pharmacometrics, so I decided to start my PhD and applied to the PharMetrX program — and here I am today!\nIn the field of pharmacometrics, not everything is perfectly documented. Thankfully, there are many helpful blogs and articles available that break down complex topics into simpler, more accessible concepts. Whenever I come across a concept I don’t fully understand (which happens quite often), I try to explain it to myself using a Quarto or RMarkdown document. Since I’m anyways creating these documents for myself, I thought I might as well share them as little blog posts. My goal really isn’t to produce perfectly polished content, but rather to document my own learning process and potentially help others who might be facing the same challenges."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Marian Klose",
    "section": "",
    "text": "Hi there! I’m Marian Klose, currently a PhD student at Freie Universitaet Berlin, and part of the PharMetrX graduate research training program. I am directly supervised by Prof. Charlotte Kloft from Freie Universitaet Berlin and co-supervised by Prof. Wilhelm Huisinga from University of Potsdam.\nHow did I end up here? After my A-levels I did a work-and-travel year and then started studying chemistry and biochemisty. I realized that I wanted to work in a field that was a bit more applied, so I switched to pharmacy. There I got into pharmacometrics pretty early on and loved the mix of computer science, pharmacy, and math. After my studies, I headed to Florida for six months to work at the University of Florida’s Center for Pharmacometrics and Systems Pharmacology under supervison of Prof. Stephan Schmidt and Prof. Rodrigo Cristofoletti, and also did a three-month internship at Boehringer Ingelheim to see if my interest would remain steady. And I was still fascinated by pharmacometrics, so I decided to start my PhD and applied to the PharMetrX program — and here I am today!\nIn the field of pharmacometrics, not everything is perfectly documented. Thankfully, there are many helpful blogs and articles available that break down complex topics into simpler, more accessible concepts. Whenever I come across a concept I don’t fully understand (which happens quite often), I try to explain it to myself using a Quarto or RMarkdown document. Since I’m anyways creating these documents for myself, I thought I might as well share them as little blog posts. My goal really isn’t to produce perfectly polished content, but rather to document my own learning process and potentially help others who might be facing the same challenges."
  },
  {
    "objectID": "posts/2024-10-09/index.html",
    "href": "posts/2024-10-09/index.html",
    "title": "Coding RUV via $THETA in NONMEM",
    "section": "",
    "text": "Code\n# load packages\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(kableExtra)\nWhen I started my PhD in pharmacometrics, I wanted to try something fancy1: specifying a combined proportional and additive error model in NONMEM for one of my projects. A colleague kindly sent me a reference model, and to my confusion, the code included a novel way (at least to me) of defining residual unexplained variability (RUV):\nalternative way (combined)\n\n$THETA\n0.15        ; RUV_PROP\n0.23        ; RUV_ADD\n\n$ERROR\nIPRED = F\nSD_PROP = THETA(1)*IPRED\nSD_ADD = THETA(2)\nSD = SQRT(SD_PROP**2 + SD_ADD**2)\nY = IPRED + SD * EPS(1)\n\n$SIGMA\n1 FIX\nIt wasn’t immediately clear why it was set up this way, and I was left with some questions:\nIt seemed a bit odd to me. I was more familiar with defining RUV directly in the $SIGMA block, something like:\nclassical way v1\n\n$ERROR\nIPRED = F\nY = IPRED + IPRED * EPS(1) + EPS(2)\n\n$SIGMA\n0.0225\n0.0529\nor maybe in a slightly more elegant form:\nclassical way v2\n\nY = IPRED * (1 + EPS(1)) + EPS(2)\nSo, why use this “alternative”2 way of defining the error? Before we try to explain this way of writing a combined error model to ourselves, let’s break down the additive and proportional error model separately to understand what’s going on. Please note: most of this content can also be found elsewhere (Proost 2017)."
  },
  {
    "objectID": "posts/2024-10-09/index.html#additive-error-models",
    "href": "posts/2024-10-09/index.html#additive-error-models",
    "title": "Coding RUV via $THETA in NONMEM",
    "section": "Additive error models",
    "text": "Additive error models\nThe “classical” way (if I can call it that) of specifying an additive error model in NONMEM is as follows:\n\n\n\nclassical way (additive)\n\n$ERROR\nIPRED = F\nY = IPRED + EPS(1)\n\n$SIGMA\n0.0529\n\n\nIn this approach, RUV is defined directly in the $SIGMA block, where EPS(1) is assumed to be normally distributed with a mean of 0 and variance of 0.0529:\n\\[EPS(1) \\sim \\mathcal{N}(0,0.0529)\\]\nIt is quite important to note that we are specifying variances in $SIGMA. Now the alternative way (my colleague called it the Uppsala way3) of coding the additive error model looks like this:\n\n\n\nalternative way (additive)\n\n$THETA\n0.23        ; RUV_ADD\n\n$ERROR\nIPRED = F\nSD_ADD = THETA(1)\nY = IPRED + SD_ADD * EPS(1)\n\n$SIGMA\n1 FIX\n\n\nHere, $SIGMA is fixed so EPS(1) has a variance of 1, effectively making it a standard normal distribution:\n\\[EPS(1) \\sim \\mathcal{N}(0,1)\\]\nBut we then multiply this random variable EPS(1) by a scaling factor SD_ADD (which is being estimated as a THETA parameter) before the product is being added to the individual predicted IPRED value:\n\n\n\nalternative way (additive)\n\nY = IPRED + SD_ADD * EPS(1)\n\n\nI am not super familiar what happens if we multiply a random variable with a scaling factor. So maybe it is a good idea to visualize what happens when we fix $SIGMA to 1 and multiply it by SD = 0.23. Let’s start with plotting a standard normal distribution ($SIGMA 1 FIX):\n\n\nCode\n# sample from standard normal distribution\nx &lt;- rnorm(100000, mean = 0, sd = 1)\nstd_norm &lt;- tibble(x = x, source = \"unscaled\")\n\n# plot\nstd_norm |&gt; \n  ggplot(aes(x = x, fill = source)) +\n  geom_density(alpha=0.2)+\n  labs(title = \"Standard normal distribution\", x = \"\", y = \"Density\")+\n  scale_fill_manual(\n    \"Source\",\n    values = c(\n      \"unscaled\" = \"#003049\"\n    )\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThe resulting standard deviation should be 1, and since \\(1^2 = 1\\), the resulting variance should also be 1. Let’s be sure and check our empirical estimates (it is a simulation, after all) to confirm this:\n\n\nCode\n# summarize data and calculate sd and variance\nstd_norm |&gt; \n  group_by(source) |&gt;   \n  summarize(\n    sd = sd(x) |&gt; signif(digits = 3),\n    var = var(x) |&gt; signif(digits = 3)\n  ) |&gt; \n  rename(\n    \"Source\" = source,\n    \"Standard Deviation\" = sd,\n    \"Variance\" = var\n  ) |&gt;\n  kbl() |&gt; kable_styling()\n\n\n\n\n\nSource\nStandard Deviation\nVariance\n\n\n\n\nunscaled\n1\n0.999\n\n\n\n\n\n\n\nGood. But what happens now to this standard normal distribution if we multiply the random variable with some scaling parameter \\(SD = 0.23\\)? Let’s find out:\n\n\nCode\n# set a seed\nset.seed(123)\n\n# multiply with W\nSD &lt;- 0.23\nx_scaled &lt;- x * SD\nstd_norm_scaled &lt;- tibble(x = x_scaled, source = \"scaled\")\n\n# combine both\nstd_norm_combined &lt;- bind_rows(std_norm, std_norm_scaled)\n\n# plot\nstd_norm_combined |&gt; \n  ggplot(aes(x = x, fill = source)) +\n  geom_density(alpha = 0.2)+\n  labs(title = \"Normal distributions: Impact of scaling factor SD\", x = \"\", y = \"Density\")+\n  scale_fill_manual(\n    \"Source\",\n    values = c(\n      \"unscaled\" = \"#003049\",  # Blue color for original\n      \"scaled\" = \"#c1121f\"     # Orange color for scaled\n    )\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nLet’s compare the standard deviation and variance of both distributions:\n\n\nCode\n# summarize data and calculate sd and variance\nstd_norm_combined |&gt; \n  group_by(source) |&gt;   \n  summarize(\n    sd = sd(x) |&gt; signif(digits = 2),\n    var = var(x) |&gt; signif(digits = 2)\n  ) |&gt; \n  rename(\n    \"Source\" = source,\n    \"Standard Deviation\" = sd,\n    \"Variance\" = var\n  ) |&gt; \n  kbl() |&gt; \n  kable_styling()\n\n\n\n\n\nSource\nStandard Deviation\nVariance\n\n\n\n\nscaled\n0.23\n0.053\n\n\nunscaled\n1.00\n1.000\n\n\n\n\n\n\n\nFor the scaled distribution, we can see that the resulting standard deviation \\(\\sigma\\) is approximately equal to our scaling factor SD_ADD (which is 0.23) and the variance is \\(0.23^2 \\approx 0.053\\). This means that in our model code\n\n\n\nalternative way (additive)\n\nSD_ADD * EPS(1)\n\n\nthe SD_ADD parameter (specified via $THETA) is representing a standard deviation. Cool thing! Probably it’s not too surprising given my naming scheme, but anyways.4 Overall, both of these models should be equivalent:\n\n\n\nclassical way (additive)\n\n$SIGMA\n0.0529   ; variance\n\n\nand\n\n\n\nalternative way (additive)\n\n$THETA\n0.23   ; standard deviation\n\n$SIGMA\n1 FIX\n\n\nTo sum it up: We need to be careful with the units. If we use the classical way, we are estimating a variance via $SIGMA, but if we use the alternative way, we are estimating a standard deviation via $THETA and fix the $SIGMA to a standard normal. Typically, we would report the standard deviation (rather than the variance) if we use an additive model, and I think one of the advantages of the alternative way is that we directly read out the standard deviation from the parameter estimates (without the need to transform anything). Some also say that the estimation becomes more stable if we model the stochastic parts via $THETA, but I cannot judge if this is true or not.\n\n\n\n\n\n\nSpecifying additive RUV via $THETA gives us a standard deviation\n\n\n\nWhenever we have an additive error model and we specify the RUV in the $THETA block (the alternative way), the resulting estimate is a standard deviation."
  },
  {
    "objectID": "posts/2024-10-09/index.html#proportional-error-models",
    "href": "posts/2024-10-09/index.html#proportional-error-models",
    "title": "Coding RUV via $THETA in NONMEM",
    "section": "Proportional error models",
    "text": "Proportional error models\nNow, let’s look at proportional error models. The classical way of specifying the proportional error model looks like this:\n\n\n\nclassical way (proportional)\n\n$ERROR\nIPRED = F\nY = IPRED + IPRED * EPS(1)\n\n$SIGMA\n0.0225\n\n\nAnd the alternative way is:\n\n\n\nalternative way (proportional)\n\n$THETA\n0.15        ; RUV_PROP\n\n$ERROR\nIPRED = F\nSD_PROP = IPRED * THETA(1)\nY = IPRED + SD_PROP * EPS(1)\n\n$SIGMA\n1 FIX\n\n\nThe structure is similar to the additive model we discussed earlier, except that the standard deviation of the random noise around our prediction depends on the prediction itself. This is why we first calculate the standard deviation SD_PROP at the given prediction as:\n\n\n\nalternative way (proportional)\n\nSD_PROP = IPRED * THETA(1)\n\n\nThis already gives us an understanding of the units of THETA(1): it represents the coefficient of variation (CV) of the prediction IPRED. Why? A coefficient of variation represents the ratio of the standard deviation to the mean. This is why we end up with a standard deviation (SD_PROP) if we multiply the prediction (IPRED) with the CV (THETA(1)). So we always have a fraction of the prediction representing our standard deviation at that point.\n\nAn example\nSuppose we have a prediction (IPRED) of 10 mg/L and we want to show the resulting distribution. For the classical approach, we would specify a variance (EPS(1)) of 0.0225, and for the alternative way, we would specify a CV (THETA(1)) of 0.15. What do you think? Will this be equivalent or not? Let’s find out!\n\n\nCode\n# Set seed for reproducibility\nset.seed(123)\n\n# Parameters\nIPRED &lt;- 10         \nCV_percent &lt;- 0.15    \nSD_prop &lt;- CV_percent * IPRED  \nsd_classical &lt;- IPRED * sqrt(0.0225)  \n\n# Number of samples\nn &lt;- 100000\n\n# Classical way: Specify variance directly\neps_classical &lt;- rnorm(n, mean = 10, sd = sd_classical)  \n\n# Alternative way: Specify CV%\neps_alternative &lt;- rnorm(n, mean = 10, sd = 1 * SD_prop) \n\n# Create a tibble combining both distributions\nprop_models &lt;- tibble(\n  value = c(eps_classical, eps_alternative),\n  source = rep(c(\"Classical (Variance = 0.0225)\", \"Alternative (CV = 0.15)\"), each = n)\n)\n\n# Plot the density of both distributions\nprop_models |&gt; \n  ggplot(aes(x = value, fill = source)) +\n  geom_density(alpha = 0.4) +\n  labs(\n    title = \"Classical vs. alternative specification\",\n    x = \"Concentration [mg/L]\",\n    y = \"Density\"\n  ) +\n  scale_fill_manual(\n    \"Model Specification\",\n    values = c(\n      \"Classical (Variance = 0.0225)\" = \"#003049\",  # Blue\n      \"Alternative (CV = 0.15)\" = \"#c1121f\"      # Red\n    )\n  ) +\n  scale_x_continuous(breaks=c(4,6,8,10,12,14,16))+\n  theme_bw()\n\n\n\n\n\n\n\n\n\nBoth models end up with the same distribution. In the classical way, we are given a variance of 0.0225. To get the standard deviation, we take the square root of the variance:\n\\[\n\\sigma_{EPS} = \\sqrt{0.0225} = 0.15\n\\] This means, that our random variable EPS(1) has a standard deviation of 0.15 mg/L in our classical model:\n\n\n\nclassical way (proportional)\n\nY = IPRED + IPRED * EPS(1)\n\n\nBy multiplying this EPS(1) by the prediction (IPRED) of 10 mg/L, we are scaling this random variable to have the (desired) standard deviation of the prediction distribution (PRED):\n\\[\n\\sigma_{Y} = 0.15 \\times 10 = 1.5 \\, \\text{mg/L}\n\\]\nIn the alternative way, we are directly estimating the coefficient of variation (CV) as 0.15.\n\n\n\nalternative way (proportional)\n\nSD_PROP = IPRED * THETA(1)\nY = IPRED + SD_PROP * EPS(1)\n\n\nWe are first calculating the respective standard deviation (SD_PROP) by multiplying CV with IPRED. We then turn this standard deviation into a random variable with this standard deviation by multiplying it with a random variable from a standard normal (EPS(1)). Also here, the respective standard deviation of the prediction distribution (PRED) is 1.5 mg/L:\n\\[\n\\sigma_{Y} = 0.15 \\times 10 = 1.5 \\, \\text{mg/L}\n\\]\nIn both cases, the resulting variability is the same, meaning both approaches lead to the same standard deviation of 1.5 mg/L. Again, it is a bit more convenient to specify the CV directly, as it is more intuitive and easier to interpret. And if the stability argument is true (see above), we would also make our estimation more robust this way.\n\n\n\n\n\n\nSpecifying proportional RUV in $THETA gives us a coefficient of variation\n\n\n\nWhenever we have a proportional error model and we specify the RUV in the $THETA block, the resulting estimate is a coefficient of variation."
  },
  {
    "objectID": "posts/2024-10-09/index.html#combined-proportional-and-additive-error-models",
    "href": "posts/2024-10-09/index.html#combined-proportional-and-additive-error-models",
    "title": "Coding RUV via $THETA in NONMEM",
    "section": "Combined proportional and additive error models",
    "text": "Combined proportional and additive error models\nFinally, let’s combine our knowledge to understand the alternative way of specifying a combined proportional and additive error model:\n\n\n\nalternative way (combined)\n\n$THETA\n0.15        ; RUV_PROP\n0.23        ; RUV_ADD\n\n$ERROR\nIPRED = F\nSD_PROP = THETA(1)*IPRED\nSD_ADD = THETA(2)\nSD = SQRT(SD_PROP**2 + SD_ADD**2)\nY = IPRED + SD * EPS(1)\n\n$SIGMA\n1 FIX\n\n\nTwo parts should already be familiar:\n\n\n\nalternative way (combined)\n\nSD_PROP = THETA(1)*IPRED\nSD_ADD = THETA(2)\n\n\nIn the first part, we calculate SD_PROP, representing the resulting standard deviation of the proportional part (as THETA(1) is a CV). The second part, SD_ADD, gives us the standard deviation of the additive part. Now we want to find the joint standard deviation SD at the given concentration. But how do we combine these components?\n\n\n\nalternative way (combined)\n\nSD = SQRT(SD_PROP**2 + SD_ADD**2)\n\n\nWe can see that we first square both terms, then add them together, then take the square root. Sounds complicated - why not just add them directly together? This is because variances are additive when combining independent random variables, while standard deviations are not (Soch 2020). Written a bit more formally for two independent random variables (we typically assume the covariance to be 0 when modelling RUV):\n\\[\\mathrm{Var}(X + Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y)\\] In our case, SD_PROP and SD_ADD are standard deviations, so we must first square them to get the variances and then add them. However, we want to go back to a standard deviation before we multiply SD with EPS(1) (being fixed to 1). Therefore, we take the square root in the end.\nThis operation has always confused me a bit, but once I understood that I can sum up variances, but not standard deviations 5 it made more sense to me.\n\n\n\n\n\n\nCombined error models\n\n\n\nWhen specifying a combined error model, the estimates in the $THETA block represent a standard deviation for the additive part and a coefficient of variation for the proportional part."
  },
  {
    "objectID": "posts/2024-10-09/index.html#conclusion",
    "href": "posts/2024-10-09/index.html#conclusion",
    "title": "Coding RUV via $THETA in NONMEM",
    "section": "Conclusion",
    "text": "Conclusion\nThis is a somewhat lengthy explanation of why and how we code the alternative approach in NONMEM. Personally, I wasn’t very familiar with how distributions behave when its random variable is being multiplying by a factor, and I didn’t realize that while variances are additive when combining two random processes, standard deviations are not. If you have a stronger background in statistics, this might have been obvious, but I hope this explanation was still helpful for some others."
  },
  {
    "objectID": "posts/2024-10-09/index.html#footnotes",
    "href": "posts/2024-10-09/index.html#footnotes",
    "title": "Coding RUV via $THETA in NONMEM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYeah, I know, not really fancy. But that’s how it feels when you touch a combined error model for the first time.↩︎\nFor many of you, this is likely quite standard. The naming reflects my perspective.↩︎\nI’m not sure if this was initially introduced by one of the Uppsala groups or if this is just some hearsay.↩︎\nSome people also code it with W instead of SD but it’s always a good idea to find descriptive variable names.↩︎\nProbably something you would tackle in the first semester of your statistics studies. But not if you study pharmacy ;)↩︎"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Expressing RUV as $THETA in NONMEM\n\n\n\nRUV\n\n\nError\n\n\nNONMEM\n\n\n\nHave you ever been confused why some people use the $THETA block to code their RUV in NONMEM? You are not alone!\n\n\n\nMarian Klose\n\n\nOct 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbout this blog\n\n\n\nMotivation\n\n\nDisclaimer\n\n\n\nSome motivations and disclaimers\n\n\n\nMarian Klose\n\n\nOct 5, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/publications_docs/pub.html",
    "href": "publications/publications_docs/pub.html",
    "title": "A model-informed clinical trial simulation tool with a graphical user interface for Duchenne muscular dystrophy",
    "section": "",
    "text": "Quantitative model-based clinical trial simulation tools play a critical role in informing study designs through simulation before actual execution. These tools help drug developers explore various trial scenarios in silico to select a clinical trial design to detect therapeutic effects more efficiently, therefore reducing time, expense, and participants’ burden. To increase the usability of the tools, user-friendly and interactive platforms should be developed to navigate various simulation scenarios. However, developing such tools challenges researchers, requiring expertise in modeling and interface development. This tutorial aims to address this gap by guiding developers in creating tailored R Shiny apps, using an example of a model-based clinical trial simulation tool that we developed for Duchenne muscular dystrophy (DMD). In this tutorial, the structural framework, essential controllers, and visualization techniques for analysis are described, along with key code examples such as criteria selection and power calculation. A virtual population was created using a machine learning algorithm to enlarge the available sample size to simulate clinical trial scenarios in the presented tool. In addition, external validation of the simulated outputs was conducted using a placebo arm of a recently published DMD trial. This tutorial will be particularly useful for developing clinical trial simulation tools based on DMD progression models for other end points and biomarkers. The presented strategies can also be applied to other diseases."
  },
  {
    "objectID": "publications/publications_docs/pub.html#abstract",
    "href": "publications/publications_docs/pub.html#abstract",
    "title": "A model-informed clinical trial simulation tool with a graphical user interface for Duchenne muscular dystrophy",
    "section": "",
    "text": "Quantitative model-based clinical trial simulation tools play a critical role in informing study designs through simulation before actual execution. These tools help drug developers explore various trial scenarios in silico to select a clinical trial design to detect therapeutic effects more efficiently, therefore reducing time, expense, and participants’ burden. To increase the usability of the tools, user-friendly and interactive platforms should be developed to navigate various simulation scenarios. However, developing such tools challenges researchers, requiring expertise in modeling and interface development. This tutorial aims to address this gap by guiding developers in creating tailored R Shiny apps, using an example of a model-based clinical trial simulation tool that we developed for Duchenne muscular dystrophy (DMD). In this tutorial, the structural framework, essential controllers, and visualization techniques for analysis are described, along with key code examples such as criteria selection and power calculation. A virtual population was created using a machine learning algorithm to enlarge the available sample size to simulate clinical trial scenarios in the presented tool. In addition, external validation of the simulated outputs was conducted using a placebo arm of a recently published DMD trial. This tutorial will be particularly useful for developing clinical trial simulation tools based on DMD progression models for other end points and biomarkers. The presented strategies can also be applied to other diseases."
  },
  {
    "objectID": "publications/publications.html#manuscripts",
    "href": "publications/publications.html#manuscripts",
    "title": "Publications",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nType\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\nCategories\n\n\n\n\n\n\nPublication\n\n\n\n\n\nMar 10, 2024\n\n\nA model-informed clinical trial simulation tool with a graphical user interface for Duchenne muscular dystrophy\n\n\nKim J, Morales JF, Kang S, Klose M, Willcocks RJ, Daniels MJ, Belfiore-Oshan R, Walter GA, Rooney WD, Vandenborne K, Kim S\n\n\nMiddle author, Clinical trial simulation, RShiny\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/publications.html#publications",
    "href": "publications/publications.html#publications",
    "title": "Publications",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n\n\n\nMar 10, 2024\n\n\nA model-informed clinical trial simulation tool with a graphical user interface for Duchenne muscular dystrophy\n\n\nKim J, Morales JF, Kang S, Klose M, Willcocks RJ, Daniels MJ, Belfiore-Oshan R, Walter GA, Rooney WD, Vandenborne K, Kim S\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/publications.html#peer-reviewed-publications",
    "href": "publications/publications.html#peer-reviewed-publications",
    "title": "Publications",
    "section": "",
    "text": "Middle author\n\n\nClinical trial simulation\n\n\nRShiny\n\n\nDuchenne muscular dystrophy\n\n\n\n\n\n\nOct 3, 2024\n\n\nKim J, Morales JF, Kang S, Klose M, Willcocks RJ, Daniels MJ, Belfiore-Oshan R, Walter GA, Rooney WD, Vandenborne K, Kim S\n\n\nCPT: Pharmacometrics & Systems Pharmacology\n\n\n\n\n\n\n\n\n\n\n\n\nSecond author\n\n\nClinical trial simulation\n\n\nRShiny\n\n\nType 1 diabetes\n\n\n\n\n\n\nJul 3, 2024\n\n\nMorales JF, Klose M, Hoffert Y, Podichetty JT, Burton J, Schmidt S, Romero K, O’Doherty I, Martin F, Campbell-Thompson M, Haller MJ, Atkinson MA, Kim S.\n\n\nCPT: Pharmacometrics & Systems Pharmacology\n\n\n\n\n\n\n\n\n\n\n\n\nMiddle author\n\n\nDoxorubicin\n\n\nTLD-1\n\n\nPEGylated liposomal doxorubicin\n\n\nNLME PK\n\n\n\n\n\n\nJun 15, 2024\n\n\nMc Laughlin AM, Hess D, Michelet R, Colombo I, Haefliger S, Bastian S, Rabaglio M, Schwitter M, Fischer S, Eckhardt K, Hayoz S, Kopp C, Klose M, Sessa C, Stathis A, Halbherr S, Huisinga W, Joerger M, Kloft C.\n\n\nCancer Chemotherapy and Pharmacology\n\n\n\n\n\n\n\n\n\n\n\n\nFirst author\n\n\nPBPK\n\n\nOxycodone\n\n\nGDI\n\n\nDDI\n\n\nUGT2B7\n\n\nCYP2D6\n\n\n\n\n\n\nMar 1, 2024\n\n\nKlose M, Cristofoletti R, Silva CM, Mangal N, Turgeon J, Michaud V, Lesko LJ, Schmidt S.\n\n\nEuropean Journal of Pharmaceutical Sciences\n\n\n\n\n\n\n\n\n\n\n\n\nMiddle author\n\n\nTLD-1\n\n\nPEGylated liposomal doxorubicin\n\n\nDoxorubicin\n\n\nSafety\n\n\nEfficacy\n\n\nNCA\n\n\n\n\n\n\nFeb 2, 2024\n\n\nColombo I, Koster KL, Holer L, Haefliger S, Rabaglio M, Bastian S, Schwitter M, Eckhardt K, Hayoz S, Mc Laughlin AM, Kloft C, Klose M, Halbherr S, Baumgartner C, Sessa C, Stathis A, Hess D, Joerger M.\n\n\nEuropean Journal of Cancer\n\n\n\n\n\n\n\n\n\n\n\n\nSecond author\n\n\nMeropenem\n\n\nAntiinfectives\n\n\nNLME PK\n\n\n\n\n\n\nApr 20, 2021\n\n\nLiebchen U, Klose M, Paal M, Vogeser M, Zoller M, Schroeder I, Schmitt L, Huisinga W, Michelet R, Zander J, Scharf C, Weinelt FA, Kloft C.\n\n\nAntibiotics\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/publications_docs/cts_duchenne_muscular_dystrophy.html",
    "href": "publications/publications_docs/cts_duchenne_muscular_dystrophy.html",
    "title": "A model-informed clinical trial simulation tool with a graphical user interface for Duchenne muscular dystrophy",
    "section": "",
    "text": "Quantitative model-based clinical trial simulation tools play a critical role in informing study designs through simulation before actual execution. These tools help drug developers explore various trial scenarios in silico to select a clinical trial design to detect therapeutic effects more efficiently, therefore reducing time, expense, and participants’ burden. To increase the usability of the tools, user-friendly and interactive platforms should be developed to navigate various simulation scenarios. However, developing such tools challenges researchers, requiring expertise in modeling and interface development. This tutorial aims to address this gap by guiding developers in creating tailored R Shiny apps, using an example of a model-based clinical trial simulation tool that we developed for Duchenne muscular dystrophy (DMD). In this tutorial, the structural framework, essential controllers, and visualization techniques for analysis are described, along with key code examples such as criteria selection and power calculation. A virtual population was created using a machine learning algorithm to enlarge the available sample size to simulate clinical trial scenarios in the presented tool. In addition, external validation of the simulated outputs was conducted using a placebo arm of a recently published DMD trial. This tutorial will be particularly useful for developing clinical trial simulation tools based on DMD progression models for other end points and biomarkers. The presented strategies can also be applied to other diseases."
  },
  {
    "objectID": "publications/publications_docs/cts_duchenne_muscular_dystrophy.html#abstract",
    "href": "publications/publications_docs/cts_duchenne_muscular_dystrophy.html#abstract",
    "title": "A model-informed clinical trial simulation tool with a graphical user interface for Duchenne muscular dystrophy",
    "section": "",
    "text": "Quantitative model-based clinical trial simulation tools play a critical role in informing study designs through simulation before actual execution. These tools help drug developers explore various trial scenarios in silico to select a clinical trial design to detect therapeutic effects more efficiently, therefore reducing time, expense, and participants’ burden. To increase the usability of the tools, user-friendly and interactive platforms should be developed to navigate various simulation scenarios. However, developing such tools challenges researchers, requiring expertise in modeling and interface development. This tutorial aims to address this gap by guiding developers in creating tailored R Shiny apps, using an example of a model-based clinical trial simulation tool that we developed for Duchenne muscular dystrophy (DMD). In this tutorial, the structural framework, essential controllers, and visualization techniques for analysis are described, along with key code examples such as criteria selection and power calculation. A virtual population was created using a machine learning algorithm to enlarge the available sample size to simulate clinical trial scenarios in the presented tool. In addition, external validation of the simulated outputs was conducted using a placebo arm of a recently published DMD trial. This tutorial will be particularly useful for developing clinical trial simulation tools based on DMD progression models for other end points and biomarkers. The presented strategies can also be applied to other diseases."
  },
  {
    "objectID": "posts/2024-10-10/index.html",
    "href": "posts/2024-10-10/index.html",
    "title": "My attempt to understand NLME estimation algorithms in NONMEM",
    "section": "",
    "text": "In our little example we assume to have a (simple) hierarchical nonlinear mixed-effects (NLME) model, for which we want to conduct a parameter estimation step. To my understanding the hierarchical structure is given by having variability defined on a population (=parameter) level and an individual (=observation) level, where the individual level depends on the parameter level.\n\n\nThe population level is represented by an inter-individual variability (IIV) term, which assumes a log-normal distribution around a typical parameter value. In this simplified example we only consider IIV on clearance and do not consider any other random effects. The population (or parameter) level can be defined as follows:\n\\[CL = \\theta_{TVCL} \\cdot e^{\\eta_{i}},~~~~~\\eta_{i} \\sim N(0, \\omega^2)\\]\nHere, the clearance (\\(CL\\)) is modeled as a random variable following a log-normal distribution centered around the typical clearance value (\\(\\theta_{\\text{TVCL}}\\)) with variance \\(\\omega^2\\). The random effect \\(\\eta_i\\) itself follows a normal distribution \\(N(0, \\omega^2)\\).\n\n\n\nThe individual level on the other hand is defined by the observed concentrations for each subject and the predicted concentrations. The predictions are based on the structural model and dependent on the individual parameters. If I am not mistaken, this dependency is a representation of the hierarchical and nested nature of our NLME model. The individual level also incorporates residual unexplained variability (RUV), which is an important piece since it enables us to define the likelihood function in the end. The individual level can be defined by:\n\\[Y_{ij} = f(x_{ij}; CL_i) + \\epsilon_{ij},~~~~~\\epsilon_{ij} \\sim N(0, \\sigma^2)\\] where we can note that:\n\n\\(Y_{ij}\\) is the observed concentration for the \\(i^{th}\\) individual at the \\(j^{th}\\) time point, which would be one row (observation) of our dataset.\n\\(f(x_{ij}; CL_i)\\) is the predicted concentration.\n\nIt contains the function \\(f()\\), which represents e.g., the set of ODEs that describe the PK model.\nThe function \\(f()\\) depends on the individual clearance (\\(CL_i\\), can be seen as a realization of the random variable \\(CL\\)) and the variable \\(x_{ij}\\).\nThis \\(x_{ij}\\) contains all the information about covariates (if we would have any), dosing and sampling events for the \\(i^{th}\\) individual at the \\(j^{th}\\) time point.\n\n\\(\\epsilon_{ij}\\) is the residual unexplained variability for the \\(i^{th}\\) individual at the \\(j^{th}\\) time point. It typically follows a normal distribution \\(N(0, \\sigma^2)\\)\n\nIn our example we have two random variables, \\(Y_{ij}\\) and \\(CL_i\\), with parameters \\(\\beta := (\\theta_{TVCL}, \\omega^2, \\sigma^2)\\). In our example we just want to estimate the typical clearance \\(\\theta_{TVCL}\\) and the IIV on clearance \\(\\omega^2\\). The residual unexplained variability \\(\\sigma^2\\) is assumed to be known and fixed. Why do we do that? Just that we can plot the surface of our objective function value in 3D and better undestand how it looks like.\n\n\n\n\nIn our simple case example (with fixed \\(V_D\\) and fixed residual unexplained variability \\(\\sigma^2\\)), we have only two parameters to estimate: \\(\\theta_{TVCL}\\) and \\(\\omega^2\\). The overall goal? To infer the parameters of interest \\((\\hat{\\theta}_{TVCL}, \\hat{\\omega^2})\\) from our observed data \\(y_{1:n}\\) by maximizing the log-likelihood (\\(ln ~L\\)) function:\n\\[(\\hat{\\theta}_{TVCL}, \\hat{\\omega^2})_{ML} = \\underset{\\theta_{TVCL},~ \\omega^2}{\\mathrm{argmax}}~ln~L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}, CL_{i:n}\\right)\\]\nTo align more with the notation in Wang (2007), we can rather deal with the \\(\\eta_i\\) values instead of the realization in the parameter space. We can write the likelihood as:\n\\[(\\hat{\\theta}_{TVCL}, \\hat{\\omega^2})_{ML} = \\underset{\\theta_{TVCL},~ \\omega^2}{\\mathrm{argmax}}~ln~L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}, \\eta_{i:n}\\right)\\]\n\nHowever, the individual \\(\\eta_i\\) values are so called unobserved latent variables. We can only directly observe the \\(y_i\\) values, not the \\(\\eta_i\\) values. Therefore, we cannot easily compute the joint likelihood.\nAnsatz: We want to get rid of the dependence on \\(\\eta_i\\) by working with the marginal likelihood.\nThe marginal likelihood is calculated by integrating out the individual parameters \\(\\eta_i\\) (“marginalizing out \\(\\eta_i\\)”):\n\n\\[(\\hat{\\theta}_{TVCL}, \\hat{\\omega^2})_{ML} = \\underset{\\theta_{TVCL}, \\omega^2}{\\mathrm{argmax}}~ln~L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right)\\]\nwith\n\\[L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right) = p(y_{1:n}; \\theta_{TVCL}, \\omega^2) = \\prod_{i=1}^n p(y_{i}; \\theta_{TVCL},~ \\omega^2)\\]\n\\[L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right) = \\prod_{i=1}^n \\int p(y_{i}, \\eta_i; \\theta_{TVCL}, \\omega^2) \\cdot d\\eta_i \\]\n\nBy integrating over all possible values of \\(\\eta_i\\) we got rid of the dependence and are now left with teh marginal likelihood.\nWe can further split the marginal likelihood equation by using the chain rule of probability:\n\n\\[L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right) = \\prod_{i=1}^n \\int p(y_{i}| \\eta_i; \\theta_{TVCL}, \\omega^2) \\cdot p(\\eta_i | \\theta_{TVCL}, \\omega^2) \\cdot d\\eta_i\\] As \\(p(\\eta_i | \\theta_{TVCL}, \\omega^2)\\) does actually not depend on \\(\\theta_{TVCL}\\), and \\(p(y_{i}| \\eta_i; \\theta_{TVCL}, \\omega^2)\\) does not actually depend on \\(\\omega^2\\), we can simplify the equation to:\n\\[L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right) = \\prod_{i=1}^n \\int p(y_{i}| \\eta_i; \\theta_{TVCL}) \\cdot p(\\eta_i | \\omega^2) \\cdot d\\eta_i\\]\n\nThe integral now contains two parts: The individual level \\(p(y_{i}| \\eta_i; \\theta_{TVCL})\\) and the population level \\(p(\\eta_i | \\omega^2)\\). The intuition behind this can be seen as follows: For a given \\(\\eta_i\\) within the integral, the population term \\(p(\\eta_i |\\omega^2)\\) tells us how likely it is to observe this \\(\\eta_i\\) value in the population. The individual term \\(p(y_{i}| \\eta_i; \\theta_{TVCL})\\) tells us how likely it is to observe the \\(y_i\\) value given that particular \\(\\eta_i\\) value. The Likelihood will be maximal when the product of both terms is maximal.\nHowever, solving the marginal likelihood is much harder due to the integral. The question to answer is: “How can we maximize the marginal log likelihood function?”"
  },
  {
    "objectID": "posts/2024-10-10/index.html#statistical-model",
    "href": "posts/2024-10-10/index.html#statistical-model",
    "title": "My attempt to understand NLME estimation algorithms in NONMEM",
    "section": "",
    "text": "In our little example we assume to have a (simple) hierarchical nonlinear mixed-effects (NLME) model, for which we want to conduct a parameter estimation step. To my understanding the hierarchical structure is given by having variability defined on a population (=parameter) level and an individual (=observation) level, where the individual level depends on the parameter level.\n\n\nThe population level is represented by an inter-individual variability (IIV) term, which assumes a log-normal distribution around a typical parameter value. In this simplified example we only consider IIV on clearance and do not consider any other random effects. The population (or parameter) level can be defined as follows:\n\\[CL = \\theta_{TVCL} \\cdot e^{\\eta_{i}},~~~~~\\eta_{i} \\sim N(0, \\omega^2)\\]\nHere, the clearance (\\(CL\\)) is modeled as a random variable following a log-normal distribution centered around the typical clearance value (\\(\\theta_{\\text{TVCL}}\\)) with variance \\(\\omega^2\\). The random effect \\(\\eta_i\\) itself follows a normal distribution \\(N(0, \\omega^2)\\).\n\n\n\nThe individual level on the other hand is defined by the observed concentrations for each subject and the predicted concentrations. The predictions are based on the structural model and dependent on the individual parameters. If I am not mistaken, this dependency is a representation of the hierarchical and nested nature of our NLME model. The individual level also incorporates residual unexplained variability (RUV), which is an important piece since it enables us to define the likelihood function in the end. The individual level can be defined by:\n\\[Y_{ij} = f(x_{ij}; CL_i) + \\epsilon_{ij},~~~~~\\epsilon_{ij} \\sim N(0, \\sigma^2)\\] where we can note that:\n\n\\(Y_{ij}\\) is the observed concentration for the \\(i^{th}\\) individual at the \\(j^{th}\\) time point, which would be one row (observation) of our dataset.\n\\(f(x_{ij}; CL_i)\\) is the predicted concentration.\n\nIt contains the function \\(f()\\), which represents e.g., the set of ODEs that describe the PK model.\nThe function \\(f()\\) depends on the individual clearance (\\(CL_i\\), can be seen as a realization of the random variable \\(CL\\)) and the variable \\(x_{ij}\\).\nThis \\(x_{ij}\\) contains all the information about covariates (if we would have any), dosing and sampling events for the \\(i^{th}\\) individual at the \\(j^{th}\\) time point.\n\n\\(\\epsilon_{ij}\\) is the residual unexplained variability for the \\(i^{th}\\) individual at the \\(j^{th}\\) time point. It typically follows a normal distribution \\(N(0, \\sigma^2)\\)\n\nIn our example we have two random variables, \\(Y_{ij}\\) and \\(CL_i\\), with parameters \\(\\beta := (\\theta_{TVCL}, \\omega^2, \\sigma^2)\\). In our example we just want to estimate the typical clearance \\(\\theta_{TVCL}\\) and the IIV on clearance \\(\\omega^2\\). The residual unexplained variability \\(\\sigma^2\\) is assumed to be known and fixed. Why do we do that? Just that we can plot the surface of our objective function value in 3D and better undestand how it looks like."
  },
  {
    "objectID": "posts/2024-10-10/index.html#maximum-likelihood-estimation",
    "href": "posts/2024-10-10/index.html#maximum-likelihood-estimation",
    "title": "My attempt to understand NLME estimation algorithms in NONMEM",
    "section": "",
    "text": "In our simple case example (with fixed \\(V_D\\) and fixed residual unexplained variability \\(\\sigma^2\\)), we have only two parameters to estimate: \\(\\theta_{TVCL}\\) and \\(\\omega^2\\). The overall goal? To infer the parameters of interest \\((\\hat{\\theta}_{TVCL}, \\hat{\\omega^2})\\) from our observed data \\(y_{1:n}\\) by maximizing the log-likelihood (\\(ln ~L\\)) function:\n\\[(\\hat{\\theta}_{TVCL}, \\hat{\\omega^2})_{ML} = \\underset{\\theta_{TVCL},~ \\omega^2}{\\mathrm{argmax}}~ln~L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}, CL_{i:n}\\right)\\]\nTo align more with the notation in Wang (2007), we can rather deal with the \\(\\eta_i\\) values instead of the realization in the parameter space. We can write the likelihood as:\n\\[(\\hat{\\theta}_{TVCL}, \\hat{\\omega^2})_{ML} = \\underset{\\theta_{TVCL},~ \\omega^2}{\\mathrm{argmax}}~ln~L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}, \\eta_{i:n}\\right)\\]\n\nHowever, the individual \\(\\eta_i\\) values are so called unobserved latent variables. We can only directly observe the \\(y_i\\) values, not the \\(\\eta_i\\) values. Therefore, we cannot easily compute the joint likelihood.\nAnsatz: We want to get rid of the dependence on \\(\\eta_i\\) by working with the marginal likelihood.\nThe marginal likelihood is calculated by integrating out the individual parameters \\(\\eta_i\\) (“marginalizing out \\(\\eta_i\\)”):\n\n\\[(\\hat{\\theta}_{TVCL}, \\hat{\\omega^2})_{ML} = \\underset{\\theta_{TVCL}, \\omega^2}{\\mathrm{argmax}}~ln~L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right)\\]\nwith\n\\[L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right) = p(y_{1:n}; \\theta_{TVCL}, \\omega^2) = \\prod_{i=1}^n p(y_{i}; \\theta_{TVCL},~ \\omega^2)\\]\n\\[L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right) = \\prod_{i=1}^n \\int p(y_{i}, \\eta_i; \\theta_{TVCL}, \\omega^2) \\cdot d\\eta_i \\]\n\nBy integrating over all possible values of \\(\\eta_i\\) we got rid of the dependence and are now left with teh marginal likelihood.\nWe can further split the marginal likelihood equation by using the chain rule of probability:\n\n\\[L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right) = \\prod_{i=1}^n \\int p(y_{i}| \\eta_i; \\theta_{TVCL}, \\omega^2) \\cdot p(\\eta_i | \\theta_{TVCL}, \\omega^2) \\cdot d\\eta_i\\] As \\(p(\\eta_i | \\theta_{TVCL}, \\omega^2)\\) does actually not depend on \\(\\theta_{TVCL}\\), and \\(p(y_{i}| \\eta_i; \\theta_{TVCL}, \\omega^2)\\) does not actually depend on \\(\\omega^2\\), we can simplify the equation to:\n\\[L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right) = \\prod_{i=1}^n \\int p(y_{i}| \\eta_i; \\theta_{TVCL}) \\cdot p(\\eta_i | \\omega^2) \\cdot d\\eta_i\\]\n\nThe integral now contains two parts: The individual level \\(p(y_{i}| \\eta_i; \\theta_{TVCL})\\) and the population level \\(p(\\eta_i | \\omega^2)\\). The intuition behind this can be seen as follows: For a given \\(\\eta_i\\) within the integral, the population term \\(p(\\eta_i |\\omega^2)\\) tells us how likely it is to observe this \\(\\eta_i\\) value in the population. The individual term \\(p(y_{i}| \\eta_i; \\theta_{TVCL})\\) tells us how likely it is to observe the \\(y_i\\) value given that particular \\(\\eta_i\\) value. The Likelihood will be maximal when the product of both terms is maximal.\nHowever, solving the marginal likelihood is much harder due to the integral. The question to answer is: “How can we maximize the marginal log likelihood function?”"
  },
  {
    "objectID": "publications/publications_docs/xx-xx-xxxx_short_title.html",
    "href": "publications/publications_docs/xx-xx-xxxx_short_title.html",
    "title": "Exploring the impact of CYP2D6 and UGT2B7 gene-drug interactions, and CYP-mediated DDI on oxycodone and oxymorphone pharmacokinetics using physiologically-based pharmacokinetic modeling and simulation",
    "section": "",
    "text": "Oxycodone is one of the most commonly used opioids to treat moderate to severe pain. It is metabolized mainly by CYP3A4 and CYP2D6, while only a small fraction of the dose is excreted unchanged into the urine. Oxymorphone, the metabolite primarily formed by CYP2D6, has a 40- to 60-fold higher mu-opioid receptor affinity than the parent compound. While CYP2D6-mediated gene-drug-interactions (GDIs) and drug-drug interactions (DDIs) are well-studied, they only account for a portion of the variability in oxycodone and oxymorphone exposure. The combined impact of CYP2D6-mediated GDIs and DDIs, CYP3A4-mediated DDIs, and UGT2B7 GDIs is not fully understood yet and hard to study in head-to-head clinical trials given the relatively large number of scenarios. Instead, we propose the use of a physiologically-based pharmacokinetic model that integrates available information on oxycodone’s metabolism to characterize and predict the impact of DDIs and GDIs on the exposure of oxycodone and its major, pharmacologically-active metabolite oxymorphone. To this end, we first developed and verified a PBPK model for oxycodone and its metabolites using published clinical data. The verified model was then applied to determine the dose-exposure relationship of oxycodone and oxymorphone stratified by CYP2D6 and UGT2B7 phenotypes respectively, and administered perpetrators of CYP-based drug interactions. Our simulations demonstrate that the combination of CYP2D6 UM and a UGT2B7Y (268) mutation may lead to a 2.3-fold increase in oxymorphone exposure compared to individuals who are phenotyped as CYP2D6 NM / UGT2B7 NM. The extent of oxymorphone exposure increases up to 3.2-fold in individuals concurrently taking CYP3A4 inhibitors, such as ketoconazole. Inhibition of the CYP3A4 pathway results in a relative increase in the partial metabolic clearance of oxycodone to oxymorphone. Oxymorphone is impacted to a higher extent by GDIs and DDIs than oxycodone. We predict oxymorphone exposure to be highest in CYP2D6 UMs/UGT2B7 PMs in the presence of ketoconazole (strong CYP3A4 index inhibitor) and lowest in CYP2D6 PMs/UGT2B7 NMs in the presence of rifampicin (strong CYP3A4 index inducer) covering a 55-fold exposure range."
  },
  {
    "objectID": "publications/publications_docs/xx-xx-xxxx_short_title.html#abstract",
    "href": "publications/publications_docs/xx-xx-xxxx_short_title.html#abstract",
    "title": "Exploring the impact of CYP2D6 and UGT2B7 gene-drug interactions, and CYP-mediated DDI on oxycodone and oxymorphone pharmacokinetics using physiologically-based pharmacokinetic modeling and simulation",
    "section": "",
    "text": "Oxycodone is one of the most commonly used opioids to treat moderate to severe pain. It is metabolized mainly by CYP3A4 and CYP2D6, while only a small fraction of the dose is excreted unchanged into the urine. Oxymorphone, the metabolite primarily formed by CYP2D6, has a 40- to 60-fold higher mu-opioid receptor affinity than the parent compound. While CYP2D6-mediated gene-drug-interactions (GDIs) and drug-drug interactions (DDIs) are well-studied, they only account for a portion of the variability in oxycodone and oxymorphone exposure. The combined impact of CYP2D6-mediated GDIs and DDIs, CYP3A4-mediated DDIs, and UGT2B7 GDIs is not fully understood yet and hard to study in head-to-head clinical trials given the relatively large number of scenarios. Instead, we propose the use of a physiologically-based pharmacokinetic model that integrates available information on oxycodone’s metabolism to characterize and predict the impact of DDIs and GDIs on the exposure of oxycodone and its major, pharmacologically-active metabolite oxymorphone. To this end, we first developed and verified a PBPK model for oxycodone and its metabolites using published clinical data. The verified model was then applied to determine the dose-exposure relationship of oxycodone and oxymorphone stratified by CYP2D6 and UGT2B7 phenotypes respectively, and administered perpetrators of CYP-based drug interactions. Our simulations demonstrate that the combination of CYP2D6 UM and a UGT2B7Y (268) mutation may lead to a 2.3-fold increase in oxymorphone exposure compared to individuals who are phenotyped as CYP2D6 NM / UGT2B7 NM. The extent of oxymorphone exposure increases up to 3.2-fold in individuals concurrently taking CYP3A4 inhibitors, such as ketoconazole. Inhibition of the CYP3A4 pathway results in a relative increase in the partial metabolic clearance of oxycodone to oxymorphone. Oxymorphone is impacted to a higher extent by GDIs and DDIs than oxycodone. We predict oxymorphone exposure to be highest in CYP2D6 UMs/UGT2B7 PMs in the presence of ketoconazole (strong CYP3A4 index inhibitor) and lowest in CYP2D6 PMs/UGT2B7 NMs in the presence of rifampicin (strong CYP3A4 index inducer) covering a 55-fold exposure range."
  },
  {
    "objectID": "publications/publications_docs/03-10-2024_cts_duchenne_muscular_dystrophy.html",
    "href": "publications/publications_docs/03-10-2024_cts_duchenne_muscular_dystrophy.html",
    "title": "A model-informed clinical trial simulation tool with a graphical user interface for Duchenne muscular dystrophy",
    "section": "",
    "text": "Quantitative model-based clinical trial simulation tools play a critical role in informing study designs through simulation before actual execution. These tools help drug developers explore various trial scenarios in silico to select a clinical trial design to detect therapeutic effects more efficiently, therefore reducing time, expense, and participants’ burden. To increase the usability of the tools, user-friendly and interactive platforms should be developed to navigate various simulation scenarios. However, developing such tools challenges researchers, requiring expertise in modeling and interface development. This tutorial aims to address this gap by guiding developers in creating tailored R Shiny apps, using an example of a model-based clinical trial simulation tool that we developed for Duchenne muscular dystrophy (DMD). In this tutorial, the structural framework, essential controllers, and visualization techniques for analysis are described, along with key code examples such as criteria selection and power calculation. A virtual population was created using a machine learning algorithm to enlarge the available sample size to simulate clinical trial scenarios in the presented tool. In addition, external validation of the simulated outputs was conducted using a placebo arm of a recently published DMD trial. This tutorial will be particularly useful for developing clinical trial simulation tools based on DMD progression models for other end points and biomarkers. The presented strategies can also be applied to other diseases."
  },
  {
    "objectID": "publications/publications_docs/03-10-2024_cts_duchenne_muscular_dystrophy.html#abstract",
    "href": "publications/publications_docs/03-10-2024_cts_duchenne_muscular_dystrophy.html#abstract",
    "title": "A model-informed clinical trial simulation tool with a graphical user interface for Duchenne muscular dystrophy",
    "section": "",
    "text": "Quantitative model-based clinical trial simulation tools play a critical role in informing study designs through simulation before actual execution. These tools help drug developers explore various trial scenarios in silico to select a clinical trial design to detect therapeutic effects more efficiently, therefore reducing time, expense, and participants’ burden. To increase the usability of the tools, user-friendly and interactive platforms should be developed to navigate various simulation scenarios. However, developing such tools challenges researchers, requiring expertise in modeling and interface development. This tutorial aims to address this gap by guiding developers in creating tailored R Shiny apps, using an example of a model-based clinical trial simulation tool that we developed for Duchenne muscular dystrophy (DMD). In this tutorial, the structural framework, essential controllers, and visualization techniques for analysis are described, along with key code examples such as criteria selection and power calculation. A virtual population was created using a machine learning algorithm to enlarge the available sample size to simulate clinical trial scenarios in the presented tool. In addition, external validation of the simulated outputs was conducted using a placebo arm of a recently published DMD trial. This tutorial will be particularly useful for developing clinical trial simulation tools based on DMD progression models for other end points and biomarkers. The presented strategies can also be applied to other diseases."
  },
  {
    "objectID": "publications/template/xx-xx-xxxx_short_title.html",
    "href": "publications/template/xx-xx-xxxx_short_title.html",
    "title": "TITLE",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet."
  },
  {
    "objectID": "publications/template/xx-xx-xxxx_short_title.html#abstract",
    "href": "publications/template/xx-xx-xxxx_short_title.html#abstract",
    "title": "TITLE",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet."
  },
  {
    "objectID": "publications/publications_docs/07-03-2024_t1d_cts.html",
    "href": "publications/publications_docs/07-03-2024_t1d_cts.html",
    "title": "Type 1 diabetes prevention clinical trial simulator: Case reports of model-informed drug development tool",
    "section": "",
    "text": "Clinical trials seeking to delay or prevent the onset of type 1 diabetes (T1D) face a series of pragmatic challenges. Despite more than 100 years since the discovery of insulin, teplizumab remains the only FDA-approved therapy to delay progression from Stage 2 to Stage 3 T1D. To increase the efficiency of clinical trials seeking this goal, our project sought to inform T1D clinical trial designs by developing a disease progression model-based clinical trial simulation tool. Using individual-level data collected from the TrialNet Pathway to Prevention and The Environmental Determinants of Diabetes in the Young natural history studies, we previously developed a quantitative joint model to predict the time to T1D onset. We then applied trial-specific inclusion/exclusion criteria, sample sizes in treatment and placebo arms, trial duration, assessment interval, and dropout rate. We implemented a function for presumed drug effects. To increase the size of the population pool, we generated virtual populations using multivariate normal distribution and ctree machine learning algorithms. As an output, power was calculated, which summarizes the probability of success, showing a statistically significant difference in the time distribution until the T1D diagnosis between the two arms. Using this tool, power curves can also be generated through iterations. The web-based tool is publicly available: https://app.cop.ufl.edu/t1d/. Herein, we briefly describe the tool and provide instructions for simulating a planned clinical trial with two case studies. This tool will allow for improved clinical trial designs and accelerate efforts seeking to prevent or delay the onset of T1D."
  },
  {
    "objectID": "publications/publications_docs/07-03-2024_t1d_cts.html#abstract",
    "href": "publications/publications_docs/07-03-2024_t1d_cts.html#abstract",
    "title": "Type 1 diabetes prevention clinical trial simulator: Case reports of model-informed drug development tool",
    "section": "",
    "text": "Clinical trials seeking to delay or prevent the onset of type 1 diabetes (T1D) face a series of pragmatic challenges. Despite more than 100 years since the discovery of insulin, teplizumab remains the only FDA-approved therapy to delay progression from Stage 2 to Stage 3 T1D. To increase the efficiency of clinical trials seeking this goal, our project sought to inform T1D clinical trial designs by developing a disease progression model-based clinical trial simulation tool. Using individual-level data collected from the TrialNet Pathway to Prevention and The Environmental Determinants of Diabetes in the Young natural history studies, we previously developed a quantitative joint model to predict the time to T1D onset. We then applied trial-specific inclusion/exclusion criteria, sample sizes in treatment and placebo arms, trial duration, assessment interval, and dropout rate. We implemented a function for presumed drug effects. To increase the size of the population pool, we generated virtual populations using multivariate normal distribution and ctree machine learning algorithms. As an output, power was calculated, which summarizes the probability of success, showing a statistically significant difference in the time distribution until the T1D diagnosis between the two arms. Using this tool, power curves can also be generated through iterations. The web-based tool is publicly available: https://app.cop.ufl.edu/t1d/. Herein, we briefly describe the tool and provide instructions for simulating a planned clinical trial with two case studies. This tool will allow for improved clinical trial designs and accelerate efforts seeking to prevent or delay the onset of T1D."
  },
  {
    "objectID": "publications/publications_docs/10-03-2024_cts_duchenne_muscular_dystrophy.html",
    "href": "publications/publications_docs/10-03-2024_cts_duchenne_muscular_dystrophy.html",
    "title": "A model-informed clinical trial simulation tool with a graphical user interface for Duchenne muscular dystrophy",
    "section": "",
    "text": "Quantitative model-based clinical trial simulation tools play a critical role in informing study designs through simulation before actual execution. These tools help drug developers explore various trial scenarios in silico to select a clinical trial design to detect therapeutic effects more efficiently, therefore reducing time, expense, and participants’ burden. To increase the usability of the tools, user-friendly and interactive platforms should be developed to navigate various simulation scenarios. However, developing such tools challenges researchers, requiring expertise in modeling and interface development. This tutorial aims to address this gap by guiding developers in creating tailored R Shiny apps, using an example of a model-based clinical trial simulation tool that we developed for Duchenne muscular dystrophy (DMD). In this tutorial, the structural framework, essential controllers, and visualization techniques for analysis are described, along with key code examples such as criteria selection and power calculation. A virtual population was created using a machine learning algorithm to enlarge the available sample size to simulate clinical trial scenarios in the presented tool. In addition, external validation of the simulated outputs was conducted using a placebo arm of a recently published DMD trial. This tutorial will be particularly useful for developing clinical trial simulation tools based on DMD progression models for other end points and biomarkers. The presented strategies can also be applied to other diseases."
  },
  {
    "objectID": "publications/publications_docs/10-03-2024_cts_duchenne_muscular_dystrophy.html#abstract",
    "href": "publications/publications_docs/10-03-2024_cts_duchenne_muscular_dystrophy.html#abstract",
    "title": "A model-informed clinical trial simulation tool with a graphical user interface for Duchenne muscular dystrophy",
    "section": "",
    "text": "Quantitative model-based clinical trial simulation tools play a critical role in informing study designs through simulation before actual execution. These tools help drug developers explore various trial scenarios in silico to select a clinical trial design to detect therapeutic effects more efficiently, therefore reducing time, expense, and participants’ burden. To increase the usability of the tools, user-friendly and interactive platforms should be developed to navigate various simulation scenarios. However, developing such tools challenges researchers, requiring expertise in modeling and interface development. This tutorial aims to address this gap by guiding developers in creating tailored R Shiny apps, using an example of a model-based clinical trial simulation tool that we developed for Duchenne muscular dystrophy (DMD). In this tutorial, the structural framework, essential controllers, and visualization techniques for analysis are described, along with key code examples such as criteria selection and power calculation. A virtual population was created using a machine learning algorithm to enlarge the available sample size to simulate clinical trial scenarios in the presented tool. In addition, external validation of the simulated outputs was conducted using a placebo arm of a recently published DMD trial. This tutorial will be particularly useful for developing clinical trial simulation tools based on DMD progression models for other end points and biomarkers. The presented strategies can also be applied to other diseases."
  },
  {
    "objectID": "publications/publications_docs/06-15-2024_nlme_pk_tld1.html",
    "href": "publications/publications_docs/06-15-2024_nlme_pk_tld1.html",
    "title": "Population pharmacokinetics of TLD-1, a novel liposomal doxorubicin, in a phase I trial",
    "section": "",
    "text": "TLD-1 is a novel pegylated liposomal doxorubicin (PLD) formulation aiming to optimise the PLD efficacy-toxicity ratio. We aimed to characterise TLD-1’s population pharmacokinetics using non-compartmental analysis and nonlinear mixed-effects modelling.\n\n\n\nThe PK of TLD-1 was analysed by performing a non-compartmental analysis of longitudinal doxorubicin plasma concentration measurements obtained from a clinical trial in 30 patients with advanced solid tumours across a 4.5-fold dose range. Furthermore, a joint parent-metabolite PK model of doxorubicinentrapped, doxorubicinfree, and metabolite doxorubicinol was developed. Interindividual and interoccasion variability around the typical PK parameters and potential covariates to explain parts of this variability were explored.\n\n\n\nMedians standard deviations of dose-normalised doxorubicinentrapped+free Cmax and AUC0−∞ were 0.342 0.134 mg/L and 40.1 18.9 mg·h/L, respectively. The median half-life (95 h) was 23.5 h longer than the half-life of currently marketed PLD. The novel joint parent-metabolite model comprised a one-compartment model with linear release (doxorubicinentrapped), a two-compartment model with linear elimination (doxorubicinfree), and a one-compartment model with linear elimination for doxorubicinol. Body surface area on the volumes of distribution for free doxorubicin was the only significant covariate.\n\n\n\nThe population PK of TLD-1, including its release and main metabolite, were successfully characterised using non-compartmental and compartmental analyses. Based on its long half-life, TLD-1 presents a promising candidate for further clinical development. The PK characteristics form the basis to investigate TLD-1 exposure-response (i.e., clinical efficacy) and exposure-toxicity relationships in the future. Once such relationships have been established, the developed population PK model can be further used in model-informed precision dosing strategies.\nClinical trial registration ClinicalTrials.gov–NCT03387917–January 2, 2018"
  },
  {
    "objectID": "publications/publications_docs/06-15-2024_nlme_pk_tld1.html#abstract",
    "href": "publications/publications_docs/06-15-2024_nlme_pk_tld1.html#abstract",
    "title": "Population pharmacokinetics of TLD-1, a novel liposomal doxorubicin, in a phase I trial",
    "section": "",
    "text": "TLD-1 is a novel pegylated liposomal doxorubicin (PLD) formulation aiming to optimise the PLD efficacy-toxicity ratio. We aimed to characterise TLD-1’s population pharmacokinetics using non-compartmental analysis and nonlinear mixed-effects modelling.\n\n\n\nThe PK of TLD-1 was analysed by performing a non-compartmental analysis of longitudinal doxorubicin plasma concentration measurements obtained from a clinical trial in 30 patients with advanced solid tumours across a 4.5-fold dose range. Furthermore, a joint parent-metabolite PK model of doxorubicinentrapped, doxorubicinfree, and metabolite doxorubicinol was developed. Interindividual and interoccasion variability around the typical PK parameters and potential covariates to explain parts of this variability were explored.\n\n\n\nMedians standard deviations of dose-normalised doxorubicinentrapped+free Cmax and AUC0−∞ were 0.342 0.134 mg/L and 40.1 18.9 mg·h/L, respectively. The median half-life (95 h) was 23.5 h longer than the half-life of currently marketed PLD. The novel joint parent-metabolite model comprised a one-compartment model with linear release (doxorubicinentrapped), a two-compartment model with linear elimination (doxorubicinfree), and a one-compartment model with linear elimination for doxorubicinol. Body surface area on the volumes of distribution for free doxorubicin was the only significant covariate.\n\n\n\nThe population PK of TLD-1, including its release and main metabolite, were successfully characterised using non-compartmental and compartmental analyses. Based on its long half-life, TLD-1 presents a promising candidate for further clinical development. The PK characteristics form the basis to investigate TLD-1 exposure-response (i.e., clinical efficacy) and exposure-toxicity relationships in the future. Once such relationships have been established, the developed population PK model can be further used in model-informed precision dosing strategies.\nClinical trial registration ClinicalTrials.gov–NCT03387917–January 2, 2018"
  },
  {
    "objectID": "publications/publications.html#abstractsposters",
    "href": "publications/publications.html#abstractsposters",
    "title": "Publications",
    "section": "Abstracts/Posters",
    "text": "Abstracts/Posters\n\n\n\n\n\nMachine learning-driven flattening of model priors: A comparative simulation study across multiple compounds\n\n\n\n\n\n\nFirst author\n\n\nMachine learning\n\n\nBayes\n\n\nPrior Modifications\n\n\nSimulation study\n\n\n\n\n\n\nJun 26, 2024\n\n\nM. Klose, F. Thoma, L. Kovar, W. Huisinga, R. Michelet, C. Kloft\n\n\nPAGE 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/abstracts_docs/06-26-2024_ml_driven_flattening_model_priors.html",
    "href": "publications/abstracts_docs/06-26-2024_ml_driven_flattening_model_priors.html",
    "title": "Machine learning-driven flattening of model priors: A comparative simulation study across multiple compounds",
    "section": "",
    "text": "https://www.page-meeting.org/default.asp?abstract=10858"
  },
  {
    "objectID": "publications/abstracts_docs/06-26-2024_ml_driven_flattening_model_priors.html#introduction",
    "href": "publications/abstracts_docs/06-26-2024_ml_driven_flattening_model_priors.html#introduction",
    "title": "Machine learning-driven flattening of model priors: A comparative simulation study across multiple compounds",
    "section": "Introduction",
    "text": "Introduction\nModel-informed precision dosing (MIPD) supports clinical decision-making by leveraging mathematical models and individual drug or biomarker measurements [1]. A typical MIPD workflow involves estimating individual maximum a posteriori (MAP) values for parameters which include interindividual variability (IIV). MAP values represent the mode of the posterior distribution and are obtained by maximising the product of the likelihood of the data and the parameter’s prior distribution [2]. The selected prior distribution needs to properly reflect the uncertainty about the parameter and is often defined by a distribution around the covariate-adjusted typical parameter value with a variance given by the model’s IIV estimate. However, this representation of uncertainty is only accurate when the model closely represents the clinical population (CP). In reality, deviations between model and CP are expected [3]. This deviation gave rise to the method of machine learning (ML)-assisted flattening of model priors [4], where the variance of the prior is selectively increased to account for these deviations. While this method has shown considerable improvement in predictive performance for vancomycin [4], its potential benefit across other drugs has not been studied."
  },
  {
    "objectID": "publications/abstracts_docs/06-26-2024_ml_driven_flattening_model_priors.html#objectives",
    "href": "publications/abstracts_docs/06-26-2024_ml_driven_flattening_model_priors.html#objectives",
    "title": "Machine learning-driven flattening of model priors: A comparative simulation study across multiple compounds",
    "section": "Objectives",
    "text": "Objectives\nTo assess the impact of ML-driven flattening of priors on the predictive performance for five compounds which are commonly applied in Therapeutic Drug Monitoring (TDM) programs by conducting a simulation study."
  },
  {
    "objectID": "publications/abstracts_docs/06-26-2024_ml_driven_flattening_model_priors.html#methods",
    "href": "publications/abstracts_docs/06-26-2024_ml_driven_flattening_model_priors.html#methods",
    "title": "Machine learning-driven flattening of model priors: A comparative simulation study across multiple compounds",
    "section": "Methods",
    "text": "Methods\nFive compounds were investigated: vancomycin (VAN), meropenem (MEM), methotrexate (MTX), infliximab (IFX), and tacrolimus (TAC). To have a plausible representation of the differences between model and CP to which the model is applied, two published models per drug [5-14] were selected and split into a data-generating model (DM) and an applied model (AM) used for Bayesian forecasting.\nA plausible TDM dataset was simulated with the DM (n=5000) using mrgsolve [15]. The PK-Sim® [16] population builder was used to generate demographically realistic virtual patients. All other covariates were drawn from lognormal distributions based on reported point estimates. Two administrations, each followed by a single drug concentration measurement, were simulated for each individual. RUV was included to account for the expected noise in clinical readouts.\nMAP estimation using the AM was performed with prior weights λ=1 and λ=1/8 for standard priors (SP) and flattened priors (FP), respectively. While the first concentration was included for MAP estimation, the second was used as a reference to evaluate the predictive performance. For each patient and λ, the residual between prediction and withheld datapoint was calculated.\nThe labeled dataset was then split into a training/test cohort (75%/25%) to build a ML model which predicts whether FP or SP should be applied for a given individual. All ML steps were conducted using the tidymodels [17] framework in R. The XGBoost (XGB) algorithm [18] was employed, using a 5-fold cross-validation with 5 repeats and a grid search for hyperparameter tuning. Optimal hyperparameters were selected based on precision.\nTo assess the impact of the method on the predictive performance of the model, the test dataset was bootstrapped (n=1000). For each sample, RMSE and MPE along with their relative improvement compared to SP were calculated across patients."
  },
  {
    "objectID": "publications/abstracts_docs/06-26-2024_ml_driven_flattening_model_priors.html#results",
    "href": "publications/abstracts_docs/06-26-2024_ml_driven_flattening_model_priors.html#results",
    "title": "Machine learning-driven flattening of model priors: A comparative simulation study across multiple compounds",
    "section": "Results",
    "text": "Results\nResults from the simulation study, presented as the median relative improvement (-) or deterioration (+) compared to SP, were:\n\nVAN\n\nRMSE: -16.0% (-11 to -22% [4])\nMPE: -40.8% (-42 to -74% [4]) -MEM\nRMSE: -0.12%\nMPE: -2.89%\n\nMTX\n\nRMSE: +1.06%\nMPE: -4.15%\n\nIFX\n\nRMSE: -11.8%\nMPE: +1.69%\n\nTAC\n\nRMSE: -32.6%\nMPE: +18.0% Overall, the simulated impact ranged from -40.8% to +1.06% (RMSE) and -16.0% to +18.0% (MPE). Precision for the test dataset was 72.1% (VAN), 25.8% (MEM), 32.8% (MTX), 37.9% (IFX), and 71.3% (TAC)."
  },
  {
    "objectID": "publications/abstracts_docs/06-26-2024_ml_driven_flattening_model_priors.html#conclusion",
    "href": "publications/abstracts_docs/06-26-2024_ml_driven_flattening_model_priors.html#conclusion",
    "title": "Machine learning-driven flattening of model priors: A comparative simulation study across multiple compounds",
    "section": "Conclusion",
    "text": "Conclusion\nThe predicted improvements in RMSE and MPE for VAN are in agreement with the reported values. However, the impact of ML driven-flattening of priors on the predictive performance for other drugs varied substantially: The largest reduction in RMSE/MPE was indeed found for VAN with heterogenous improvements and even deteriorations for the other compounds. Missing impact of modified priors was also previously reported [19]. Moving forward, we plan to extend the investigation to identify variables which potentially explain the observed differences."
  },
  {
    "objectID": "publications/abstracts_docs/06-26-2024_ml_driven_flattening_model_priors.html#references",
    "href": "publications/abstracts_docs/06-26-2024_ml_driven_flattening_model_priors.html#references",
    "title": "Machine learning-driven flattening of model priors: A comparative simulation study across multiple compounds",
    "section": "References",
    "text": "References\n[1] F. Kluwe, R. Michelet, A. Mueller‐Schoell et al. Perspectives on Model‐Informed Precision Dosing in the Digital Health Era: Challenges, Opportunities, and Recommendations. Clin Pharmacol Ther 109: 29–36 (2021). [2] L.B. Sheiner, B. Rosenberg, K.L. Melmon. Modelling of individual pharmacokinetics for computer-aided drug dosage. Computers and Biomedical Research 5: 441–459 (1972). [3] R.J. Keizer, R. ter Heine, A. Frymoyer et al. Model‐Informed Precision Dosing at the Bedside: Scientific Challenges and Opportunities. CPT Pharmacometrics Syst Pharmacol 7: 785–787 (2018). [4] J.H. Hughes, R.J. Keizer. A hybrid machine learning/pharmacokinetic approach outperforms maximum a posteriori Bayesian estimation by selectively flattening model priors. CPT Pharmacometrics Syst Pharmacol 10: 1150–1160 (2021). [5] E. Dreesen, S. Berends, D. Laharie et al. Modelling of the relationship between infliximab exposure, faecal calprotectin and endoscopic remission in patients with Crohn’s disease. Br J Clin Pharmacol 87: 106–118 (2021). [6] A.A. Fasanmade, O.J. Adedokun, M. Blank et al. Pharmacokinetic Properties of Infliximab in Children and Adults with Crohn’s Disease: A Retrospective Analysis of Data from 2 Phase III Clinical Trials. Clin Ther 33: 946–964 (2011). [7] A. Boonpeng, S. Jaruratanasirikul, M. Jullangkoon et al. Population Pharmacokinetics/Pharmacodynamics and Clinical Outcomes of Meropenem in Critically Ill Patients. Antimicrob Agents Chemother 66: (2022). [8] J. Lan, Z. Wu, X. Wang et al. Population Pharmacokinetics Analysis and Dosing Simulations Of Meropenem in Critically Ill Patients with Pulmonary Infection. J Pharm Sci 111: 1833–1842 (2022). [9] F. Gallais, L. Oberic, S. Faguer et al. Body Surface Area Dosing of High-Dose Methotrexate Should Be Reconsidered, Particularly in Overweight, Adult Patients. Ther Drug Monit 43: 408–415 (2021). [10] D.W. Faltaos, J.S. Hulot, S. Urien et al. Population pharmacokinetic study of methotrexate in patients with lymphoid malignancy. Cancer Chemother Pharmacol 58: 626–633 (2006). [11] K. Ogasawara, S.D. Chitnis, R.Y. Gohh et al. Multidrug Resistance-Associated Protein 2 (MRP2/ABCC2) Haplotypes Significantly Affect the Pharmacokinetics of Tacrolimus in Kidney Transplant Recipients. Clin Pharmacokinet 52: 751–762 (2013). [12] J.-B. Woillard, B.C.M. de Winter, N. Kamar et al. Population pharmacokinetic model and Bayesian estimator for two tacrolimus formulations - twice daily Prograf® and once daily Advagraf®. Br J Clin Pharmacol 71: 391–402 (2011). [13] V. Goti, A. Chaturvedula, M.J. Fossler et al. Hospitalized Patients With and Without Hemodialysis Have Markedly Different Vancomycin Pharmacokinetics: A Population Pharmacokinetic Model-Based Analysis. Ther Drug Monit 40: 212–221 (2018). [14] A.H. Thomson, C.E. Staatz, C.M. Tobin et al. Development and evaluation of vancomycin dosage guidelines designed to achieve new target concentrations. Journal of Antimicrobial Chemotherapy 63: 1050–1057 (2009). [15] Kyle T Baron. mrgsolve: Simulate from ODE-Based Models. R package version 1.4.1, https://github.com/metrumresearchgroup/mrgsolve. (2024). [16] S. Willmann, J. Lippert, M. Sevestre et al. PK-Sim®: a physiologically based pharmacokinetic ‘whole-body’ model. BIOSILICO 1: 121–124 (2003). [17] M. Kuhn, H. Wickham. Tidymodels: a collection of packages for modeling and machine learning using tidyverse principles. (2020). [18] T. Chen, C. Guestrin. XGBoost: A Scalable Tree Boosting System. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining doi: 10.1145/2939672.2939785 (2016). [19] F. Le Louedec, PAGE 31 (2023) Abstr 10296 [www.page- eeting.org/?abstract=10296]. (2023)."
  },
  {
    "objectID": "publications/abstracts_docs/06-26-2024_ml_driven_flattening_model_priors.html#url",
    "href": "publications/abstracts_docs/06-26-2024_ml_driven_flattening_model_priors.html#url",
    "title": "Machine learning-driven flattening of model priors: A comparative simulation study across multiple compounds",
    "section": "",
    "text": "https://www.page-meeting.org/default.asp?abstract=10858"
  },
  {
    "objectID": "publications/publications_docs/02-02-2024_pk_safety_efficacy_tld1.html",
    "href": "publications/publications_docs/02-02-2024_pk_safety_efficacy_tld1.html",
    "title": "TLD-1, a novel liposomal doxorubicin, in patients with advanced solid tumors: Dose escalation and expansion part of a multicenter open-label phase I trial (SAKK 65/16)",
    "section": "",
    "text": "TLD-1 is a novel liposomal doxorubicin that compared favorably to conventional doxorubicin liposomal formulations in preclinical models. This phase I first-in-human study aimed to define the maximum tolerated dose (MTD), recommended phase 2 dose (RP2D), safety and preliminary activity of TLD-1 in patients with advanced solid tumors.\n\n\n\nWe recruited patients with advanced solid tumors who failed standard therapy and received up to 3 prior lines of palliative systemic chemotherapy. TLD-1 was administered intravenously every 3 weeks up to a maximum of 9 cycles (6 for patients with prior anthracyclines) from a starting dose of 10 mg/m2, according to an accelerated titration design followed by a modified continual reassessment method.\n\n\n\n30 patients were enrolled between November 2018 and May 2021. No dose-limiting toxicities (DLT) were observed. Maximum administered dose of TLD-1 was 45 mg/m2, RP2D was defined at 40 mg/m2. Most frequent treatment-related adverse events (TRAE) of any grade included palmar-plantar erythrodysesthesia (PPE) (50% of patients), oral mucositis (50%), fatigue (30%) and skin rash (26.7%). Most common G3 TRAE included PPE in 4 patients (13.3%) and oral mucositis in 2 (6.7%). Overall objective response rate was 10% in the whole population and 23.1% among 13 patients with breast cancer; median time-to-treatment failure was 2.7 months. TLD-1 exhibit linear pharmacokinetics, with a median terminal half-life of 95 h.\n\n\n\nThe new liposomal doxorubicin formulation TLD-1 showed a favourable safety profile and antitumor activity, particularly in breast cancer. RP2D was defined at 40 mg/m2 administered every 3 weeks. (NCT03387917)"
  },
  {
    "objectID": "publications/publications_docs/02-02-2024_pk_safety_efficacy_tld1.html#abstract",
    "href": "publications/publications_docs/02-02-2024_pk_safety_efficacy_tld1.html#abstract",
    "title": "TLD-1, a novel liposomal doxorubicin, in patients with advanced solid tumors: Dose escalation and expansion part of a multicenter open-label phase I trial (SAKK 65/16)",
    "section": "",
    "text": "TLD-1 is a novel liposomal doxorubicin that compared favorably to conventional doxorubicin liposomal formulations in preclinical models. This phase I first-in-human study aimed to define the maximum tolerated dose (MTD), recommended phase 2 dose (RP2D), safety and preliminary activity of TLD-1 in patients with advanced solid tumors.\n\n\n\nWe recruited patients with advanced solid tumors who failed standard therapy and received up to 3 prior lines of palliative systemic chemotherapy. TLD-1 was administered intravenously every 3 weeks up to a maximum of 9 cycles (6 for patients with prior anthracyclines) from a starting dose of 10 mg/m2, according to an accelerated titration design followed by a modified continual reassessment method.\n\n\n\n30 patients were enrolled between November 2018 and May 2021. No dose-limiting toxicities (DLT) were observed. Maximum administered dose of TLD-1 was 45 mg/m2, RP2D was defined at 40 mg/m2. Most frequent treatment-related adverse events (TRAE) of any grade included palmar-plantar erythrodysesthesia (PPE) (50% of patients), oral mucositis (50%), fatigue (30%) and skin rash (26.7%). Most common G3 TRAE included PPE in 4 patients (13.3%) and oral mucositis in 2 (6.7%). Overall objective response rate was 10% in the whole population and 23.1% among 13 patients with breast cancer; median time-to-treatment failure was 2.7 months. TLD-1 exhibit linear pharmacokinetics, with a median terminal half-life of 95 h.\n\n\n\nThe new liposomal doxorubicin formulation TLD-1 showed a favourable safety profile and antitumor activity, particularly in breast cancer. RP2D was defined at 40 mg/m2 administered every 3 weeks. (NCT03387917)"
  },
  {
    "objectID": "publications/publications_docs/03-01-2024_oxycodone_pbpk_ddi_gdi.html",
    "href": "publications/publications_docs/03-01-2024_oxycodone_pbpk_ddi_gdi.html",
    "title": "Exploring the impact of CYP2D6 and UGT2B7 gene-drug interactions, and CYP-mediated DDI on oxycodone and oxymorphone pharmacokinetics using physiologically-based pharmacokinetic modeling and simulation",
    "section": "",
    "text": "Oxycodone is one of the most commonly used opioids to treat moderate to severe pain. It is metabolized mainly by CYP3A4 and CYP2D6, while only a small fraction of the dose is excreted unchanged into the urine. Oxymorphone, the metabolite primarily formed by CYP2D6, has a 40- to 60-fold higher mu-opioid receptor affinity than the parent compound. While CYP2D6-mediated gene-drug-interactions (GDIs) and drug-drug interactions (DDIs) are well-studied, they only account for a portion of the variability in oxycodone and oxymorphone exposure. The combined impact of CYP2D6-mediated GDIs and DDIs, CYP3A4-mediated DDIs, and UGT2B7 GDIs is not fully understood yet and hard to study in head-to-head clinical trials given the relatively large number of scenarios. Instead, we propose the use of a physiologically-based pharmacokinetic model that integrates available information on oxycodone’s metabolism to characterize and predict the impact of DDIs and GDIs on the exposure of oxycodone and its major, pharmacologically-active metabolite oxymorphone. To this end, we first developed and verified a PBPK model for oxycodone and its metabolites using published clinical data. The verified model was then applied to determine the dose-exposure relationship of oxycodone and oxymorphone stratified by CYP2D6 and UGT2B7 phenotypes respectively, and administered perpetrators of CYP-based drug interactions. Our simulations demonstrate that the combination of CYP2D6 UM and a UGT2B7Y (268) mutation may lead to a 2.3-fold increase in oxymorphone exposure compared to individuals who are phenotyped as CYP2D6 NM / UGT2B7 NM. The extent of oxymorphone exposure increases up to 3.2-fold in individuals concurrently taking CYP3A4 inhibitors, such as ketoconazole. Inhibition of the CYP3A4 pathway results in a relative increase in the partial metabolic clearance of oxycodone to oxymorphone. Oxymorphone is impacted to a higher extent by GDIs and DDIs than oxycodone. We predict oxymorphone exposure to be highest in CYP2D6 UMs/UGT2B7 PMs in the presence of ketoconazole (strong CYP3A4 index inhibitor) and lowest in CYP2D6 PMs/UGT2B7 NMs in the presence of rifampicin (strong CYP3A4 index inducer) covering a 55-fold exposure range."
  },
  {
    "objectID": "publications/publications_docs/03-01-2024_oxycodone_pbpk_ddi_gdi.html#abstract",
    "href": "publications/publications_docs/03-01-2024_oxycodone_pbpk_ddi_gdi.html#abstract",
    "title": "Exploring the impact of CYP2D6 and UGT2B7 gene-drug interactions, and CYP-mediated DDI on oxycodone and oxymorphone pharmacokinetics using physiologically-based pharmacokinetic modeling and simulation",
    "section": "",
    "text": "Oxycodone is one of the most commonly used opioids to treat moderate to severe pain. It is metabolized mainly by CYP3A4 and CYP2D6, while only a small fraction of the dose is excreted unchanged into the urine. Oxymorphone, the metabolite primarily formed by CYP2D6, has a 40- to 60-fold higher mu-opioid receptor affinity than the parent compound. While CYP2D6-mediated gene-drug-interactions (GDIs) and drug-drug interactions (DDIs) are well-studied, they only account for a portion of the variability in oxycodone and oxymorphone exposure. The combined impact of CYP2D6-mediated GDIs and DDIs, CYP3A4-mediated DDIs, and UGT2B7 GDIs is not fully understood yet and hard to study in head-to-head clinical trials given the relatively large number of scenarios. Instead, we propose the use of a physiologically-based pharmacokinetic model that integrates available information on oxycodone’s metabolism to characterize and predict the impact of DDIs and GDIs on the exposure of oxycodone and its major, pharmacologically-active metabolite oxymorphone. To this end, we first developed and verified a PBPK model for oxycodone and its metabolites using published clinical data. The verified model was then applied to determine the dose-exposure relationship of oxycodone and oxymorphone stratified by CYP2D6 and UGT2B7 phenotypes respectively, and administered perpetrators of CYP-based drug interactions. Our simulations demonstrate that the combination of CYP2D6 UM and a UGT2B7Y (268) mutation may lead to a 2.3-fold increase in oxymorphone exposure compared to individuals who are phenotyped as CYP2D6 NM / UGT2B7 NM. The extent of oxymorphone exposure increases up to 3.2-fold in individuals concurrently taking CYP3A4 inhibitors, such as ketoconazole. Inhibition of the CYP3A4 pathway results in a relative increase in the partial metabolic clearance of oxycodone to oxymorphone. Oxymorphone is impacted to a higher extent by GDIs and DDIs than oxycodone. We predict oxymorphone exposure to be highest in CYP2D6 UMs/UGT2B7 PMs in the presence of ketoconazole (strong CYP3A4 index inhibitor) and lowest in CYP2D6 PMs/UGT2B7 NMs in the presence of rifampicin (strong CYP3A4 index inducer) covering a 55-fold exposure range."
  },
  {
    "objectID": "publications/publications.html#abstractsposterspresentations",
    "href": "publications/publications.html#abstractsposterspresentations",
    "title": "Publications",
    "section": "Abstracts/Posters/Presentations",
    "text": "Abstracts/Posters/Presentations\n\n\n\n\n\nMachine learning-driven flattening of model priors: A comparative simulation study across multiple compounds\n\n\n\n\n\n\nFirst author\n\n\nMachine learning\n\n\nBayes\n\n\nPrior Modifications\n\n\nSimulation study\n\n\n\n\n\n\nJun 26, 2024\n\n\nM. Klose, F. Thoma, L. Kovar, W. Huisinga, R. Michelet, C. Kloft\n\n\nPAGE 2024\n\n\n\n\n\n\n\nThe Impact of UGT2B7 and CYP2D6 Gene-Drug- and CYP-mediated Drug-Drug-Interactions on Oxycodone and Oxymorphone Pharmacokinetics using PBPK Modelling.\n\n\n\n\n\n\nFirst author\n\n\nPBPK\n\n\nOxycodone\n\n\nGDI\n\n\nDDI\n\n\nUGT2B7\n\n\nCYP2D6\n\n\n\n\n\n\nOct 30, 2022\n\n\nKlose M, Schmidt S, Cristofoletti R.\n\n\nACoP13\n\n\n\n\n\n\n\nUsing microdose-based activity measurement to individualise dosing of cytochrome P450 metabolised drugs: a case study with yohimbine and tamoxifen\n\n\n\n\n\n\nMiddle author\n\n\nYohimbine\n\n\nTamoxifen\n\n\nCYP2D6\n\n\nMicrodosing\n\n\n\n\n\n\nSep 2, 2021\n\n\nR. Michelet, F. Weinelt, M. Klose, A. M. Mc Laughlin, F. Kluwe, C. Montefusco-Pereira, M. Van Dyk, M. Vay, W. Huisinga, C. Kloft & G. Mikus\n\n\nPAGE 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/abstracts_docs/10-30-2022_oxycodone_pbpk_ddi_gdi.html",
    "href": "publications/abstracts_docs/10-30-2022_oxycodone_pbpk_ddi_gdi.html",
    "title": "The Impact of UGT2B7 and CYP2D6 Gene-Drug- and CYP-mediated Drug-Drug-Interactions on Oxycodone and Oxymorphone Pharmacokinetics using PBPK Modelling.",
    "section": "",
    "text": "https://www.go-acop.org/?abstract=413"
  },
  {
    "objectID": "publications/abstracts_docs/10-30-2022_oxycodone_pbpk_ddi_gdi.html#url",
    "href": "publications/abstracts_docs/10-30-2022_oxycodone_pbpk_ddi_gdi.html#url",
    "title": "The Impact of UGT2B7 and CYP2D6 Gene-Drug- and CYP-mediated Drug-Drug-Interactions on Oxycodone and Oxymorphone Pharmacokinetics using PBPK Modelling.",
    "section": "",
    "text": "https://www.go-acop.org/?abstract=413"
  },
  {
    "objectID": "publications/publications_docs/04-20-2021_mero_risk_calculator_evaluation.html",
    "href": "publications/publications_docs/04-20-2021_mero_risk_calculator_evaluation.html",
    "title": "Evaluation of the MeroRisk Calculator, A User-Friendly Tool to Predict the Risk of Meropenem Target Non-Attainment in Critically Ill Patients",
    "section": "",
    "text": "Background: The MeroRisk-calculator, an easy-to-use tool to determine the risk of meropenem target non-attainment after standard dosing (1000 mg; q8h), uses a patient’s creatinine clearance and the minimum inhibitory concentration (MIC) of the pathogen. In clinical practice, however, the MIC is rarely available. The objectives were to evaluate the MeroRisk-calculator and to extend risk assessment by including general pathogen sensitivity data.\nMethods: Using a clinical routine dataset (155 patients, 891 samples), a direct data-based evaluation was not feasible. Thus, in step 1, the performance of a pharmacokinetic model was determined for predicting the measured concentrations. In step 2, the PK model was used for a model-based evaluation of the MeroRisk-calculator: risk of target non-attainment was calculated using the PK model and agreement with the MeroRisk-calculator was determined by a visual and statistical (Lin’s concordance correlation coefficient (CCC)) analysis for MIC values 0.125-16 mg/L. The MeroRisk-calculator was extended to include risk assessment based on EUCAST-MIC distributions and cumulative-fraction-of-response analysis.\nResults: Step 1 showed a negligible bias of the PK model to underpredict concentrations (-0.84 mg/L). Step 2 revealed a high level of agreement between risk of target non-attainment predictions for creatinine clearances &gt;50 mL/min (CCC = 0.990), but considerable deviations for patients &lt;50 mL/min. For 27% of EUCAST-listed pathogens the median cumulative-fraction-of-response for the observed patients receiving standard dosing was &lt; 90%.\nConclusions: The MeroRisk-calculator was successfully evaluated: For patients with maintained renal function it allows a reliable and user-friendly risk assessment. The integration of pathogen-based risk assessment substantially increases the applicability of the tool."
  },
  {
    "objectID": "publications/publications_docs/04-20-2021_mero_risk_calculator_evaluation.html#abstract",
    "href": "publications/publications_docs/04-20-2021_mero_risk_calculator_evaluation.html#abstract",
    "title": "Evaluation of the MeroRisk Calculator, A User-Friendly Tool to Predict the Risk of Meropenem Target Non-Attainment in Critically Ill Patients",
    "section": "",
    "text": "Background: The MeroRisk-calculator, an easy-to-use tool to determine the risk of meropenem target non-attainment after standard dosing (1000 mg; q8h), uses a patient’s creatinine clearance and the minimum inhibitory concentration (MIC) of the pathogen. In clinical practice, however, the MIC is rarely available. The objectives were to evaluate the MeroRisk-calculator and to extend risk assessment by including general pathogen sensitivity data.\nMethods: Using a clinical routine dataset (155 patients, 891 samples), a direct data-based evaluation was not feasible. Thus, in step 1, the performance of a pharmacokinetic model was determined for predicting the measured concentrations. In step 2, the PK model was used for a model-based evaluation of the MeroRisk-calculator: risk of target non-attainment was calculated using the PK model and agreement with the MeroRisk-calculator was determined by a visual and statistical (Lin’s concordance correlation coefficient (CCC)) analysis for MIC values 0.125-16 mg/L. The MeroRisk-calculator was extended to include risk assessment based on EUCAST-MIC distributions and cumulative-fraction-of-response analysis.\nResults: Step 1 showed a negligible bias of the PK model to underpredict concentrations (-0.84 mg/L). Step 2 revealed a high level of agreement between risk of target non-attainment predictions for creatinine clearances &gt;50 mL/min (CCC = 0.990), but considerable deviations for patients &lt;50 mL/min. For 27% of EUCAST-listed pathogens the median cumulative-fraction-of-response for the observed patients receiving standard dosing was &lt; 90%.\nConclusions: The MeroRisk-calculator was successfully evaluated: For patients with maintained renal function it allows a reliable and user-friendly risk assessment. The integration of pathogen-based risk assessment substantially increases the applicability of the tool."
  },
  {
    "objectID": "publications/abstracts_docs/09-02-2021_microdosed_yohimbine.html",
    "href": "publications/abstracts_docs/09-02-2021_microdosed_yohimbine.html",
    "title": "Using microdose-based activity measurement to individualise dosing of cytochrome P450 metabolised drugs: a case study with yohimbine and tamoxifen",
    "section": "",
    "text": "Objectives: Understanding pharmacokinetic (PK) interindividual variability (IIV) can enable reaching optimal drug exposure, minimising therapeutic failure. Genotype-derived phenotypes are often applied to derive a patient’s individual clearance (iCL) but do not always translate into the optimal individual dose. Alternatively, the direct measurement of enzyme activity using a microdosed external probe could provide insights into the patient’s iCL and could be used in model-informed precision dosing (MIPD). Here, we propose the use of the CYP2D6 substrate yohimbine (YOH) as a probe to individualise the dosing of the selective oestrogen receptor modulator tamoxifen (TAM). While YOH’s PK was previously characterised using intensive blood sampling over 24 hours [1], deriving YOH iCL (iCLYOH), which varies 1000-fold between normal and poor metabolisers, with less samples would be ideal. This iCL can then be used in a Bayesian framework to predict the optimal dose of other CYP2D6 substrates. After standard dosing of 20 mg TAM once daily, high IIV in TAM concentrations and its ~100-fold more active metabolite endoxifen (ENDX) is observed and attributed to variability in CYP2D6 activity [2]. Therefore, treatment with TAM would benefit from MIPD and thus we present this as a case study of using microdose-based activity measurement to individualise dosing of CYP metabolised drugs.\nMaterial/Methods: A recent study investigating oral YOH as a predictor for CYP2D6 activity was used for PK model development [1], including the CYP2D6 genotype-derived phenotype, using NONMEM v. 7.4. The best-fitting model was then refitted to the data blinded for the attributed CYP2D6 activity score (AS) to mimic the application where this data is not available. To use the final model for Bayesian inference in a clinical setting, optimal sampling time points between 0.25 and 4 h post-dose were determined using optimal design analysis in R/Rstudio (v.3.6.3/1.3.959) applying the popED package (v.0.5.0). This design was evaluated by stochastically simulating YOH concentrations at the optimal timepoints to estimate MAP CL. The agreement between the MAP estimate and the iCL of the simulation was then evaluated: bias and precision were assessed using median estimation errors and median absolute estimation errors. To incorporate the ‘real-world’ small deviations from planned sampling times, simulated sampling times were drawn from a normal distribution (sd=5 min) around the planned sample. For the MIPD application, a published parent-metabolite TAM PK model [2] was selected. Based on the empirical Bayes estimates and the CYP2D6 AS of the patients in the original YOH model development dataset, iCLYOH for the blinded model application dataset were converted to CYP2D6 AS’s, which were implemented as covariates in the TAM model. Then ENDX exposure of 1000 virtual patients with the same AS were stochastically simulated after 20, 40 and 60 mg daily doses. The percentages of virtual patients reaching the target ENDX minimum steady state concentration of 5.97 ng/mL were calculated [3]. For each patient, the lowest dose resulting in &gt;90% of target attainment was selected as optimal dose.\nResults: A two-compartmental model with first-order absorption and linear elimination best described the YOH data. The IIV on YOH CL was largely explained by including CYP2D6 activity as a covariate, leading to a reduction from 1,143 to 43.9 CV%. Based on the optimal experimental design analysis, one early (0.25 h) and one late (4 h) sample were sufficient to reliably estimate iCL. iCLYOH were linked to the following phenotypes: patients with iCLYOH≤6 L/h: poor metabolisers, 6&lt;iCLYOH&lt;180 L/h: intermediate metabolisers and iCLYOH≥180 L/h: normal metabolisers. The PK model and iCL estimation were successfully linked to the TAM model in order to provide dosing recommendations for TAM treatment based on 2 YOH samples.\nConclusion: This study achieved TAM dose individualisation by using YOH derived CYP2D6 activity and MIPD. A clinical study where both TAM and YOH are administered could inform a direct link between iCLYOH and iCLTAM. This framework can be used for dose individualisation of other CYP substrates as long as a PK model and a probe is available; e.g. midazolam for CYP3A4 could be considered to expand this microdose-based activity measurement for individualised dosing and its utility should be investigated in prospective clinical trials. Furthermore, integration of the workflow in an easy-to-use tool would further encourage clinical application."
  },
  {
    "objectID": "publications/abstracts_docs/09-02-2021_microdosed_yohimbine.html#abstract",
    "href": "publications/abstracts_docs/09-02-2021_microdosed_yohimbine.html#abstract",
    "title": "Using microdose-based activity measurement to individualise dosing of cytochrome P450 metabolised drugs: a case study with yohimbine and tamoxifen",
    "section": "",
    "text": "Objectives: Understanding pharmacokinetic (PK) interindividual variability (IIV) can enable reaching optimal drug exposure, minimising therapeutic failure. Genotype-derived phenotypes are often applied to derive a patient’s individual clearance (iCL) but do not always translate into the optimal individual dose. Alternatively, the direct measurement of enzyme activity using a microdosed external probe could provide insights into the patient’s iCL and could be used in model-informed precision dosing (MIPD). Here, we propose the use of the CYP2D6 substrate yohimbine (YOH) as a probe to individualise the dosing of the selective oestrogen receptor modulator tamoxifen (TAM). While YOH’s PK was previously characterised using intensive blood sampling over 24 hours [1], deriving YOH iCL (iCLYOH), which varies 1000-fold between normal and poor metabolisers, with less samples would be ideal. This iCL can then be used in a Bayesian framework to predict the optimal dose of other CYP2D6 substrates. After standard dosing of 20 mg TAM once daily, high IIV in TAM concentrations and its ~100-fold more active metabolite endoxifen (ENDX) is observed and attributed to variability in CYP2D6 activity [2]. Therefore, treatment with TAM would benefit from MIPD and thus we present this as a case study of using microdose-based activity measurement to individualise dosing of CYP metabolised drugs.\nMaterial/Methods: A recent study investigating oral YOH as a predictor for CYP2D6 activity was used for PK model development [1], including the CYP2D6 genotype-derived phenotype, using NONMEM v. 7.4. The best-fitting model was then refitted to the data blinded for the attributed CYP2D6 activity score (AS) to mimic the application where this data is not available. To use the final model for Bayesian inference in a clinical setting, optimal sampling time points between 0.25 and 4 h post-dose were determined using optimal design analysis in R/Rstudio (v.3.6.3/1.3.959) applying the popED package (v.0.5.0). This design was evaluated by stochastically simulating YOH concentrations at the optimal timepoints to estimate MAP CL. The agreement between the MAP estimate and the iCL of the simulation was then evaluated: bias and precision were assessed using median estimation errors and median absolute estimation errors. To incorporate the ‘real-world’ small deviations from planned sampling times, simulated sampling times were drawn from a normal distribution (sd=5 min) around the planned sample. For the MIPD application, a published parent-metabolite TAM PK model [2] was selected. Based on the empirical Bayes estimates and the CYP2D6 AS of the patients in the original YOH model development dataset, iCLYOH for the blinded model application dataset were converted to CYP2D6 AS’s, which were implemented as covariates in the TAM model. Then ENDX exposure of 1000 virtual patients with the same AS were stochastically simulated after 20, 40 and 60 mg daily doses. The percentages of virtual patients reaching the target ENDX minimum steady state concentration of 5.97 ng/mL were calculated [3]. For each patient, the lowest dose resulting in &gt;90% of target attainment was selected as optimal dose.\nResults: A two-compartmental model with first-order absorption and linear elimination best described the YOH data. The IIV on YOH CL was largely explained by including CYP2D6 activity as a covariate, leading to a reduction from 1,143 to 43.9 CV%. Based on the optimal experimental design analysis, one early (0.25 h) and one late (4 h) sample were sufficient to reliably estimate iCL. iCLYOH were linked to the following phenotypes: patients with iCLYOH≤6 L/h: poor metabolisers, 6&lt;iCLYOH&lt;180 L/h: intermediate metabolisers and iCLYOH≥180 L/h: normal metabolisers. The PK model and iCL estimation were successfully linked to the TAM model in order to provide dosing recommendations for TAM treatment based on 2 YOH samples.\nConclusion: This study achieved TAM dose individualisation by using YOH derived CYP2D6 activity and MIPD. A clinical study where both TAM and YOH are administered could inform a direct link between iCLYOH and iCLTAM. This framework can be used for dose individualisation of other CYP substrates as long as a PK model and a probe is available; e.g. midazolam for CYP3A4 could be considered to expand this microdose-based activity measurement for individualised dosing and its utility should be investigated in prospective clinical trials. Furthermore, integration of the workflow in an easy-to-use tool would further encourage clinical application."
  },
  {
    "objectID": "posts/expressing_ruv_as_theta/index.html",
    "href": "posts/expressing_ruv_as_theta/index.html",
    "title": "Expressing RUV as $THETA in NONMEM",
    "section": "",
    "text": "Code\n# load packages\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(kableExtra)\nWhen I started my PhD in pharmacometrics, I wanted to try something fancy1: specifying a combined proportional and additive error model in NONMEM for one of my projects. A colleague kindly sent me a reference model, and to my confusion, the code included a novel way (at least to me) of defining residual unexplained variability (RUV):\nalternative way (combined)\n\n$THETA\n0.15        ; RUV_PROP\n0.23        ; RUV_ADD\n\n$ERROR\nIPRED = F\nSD_PROP = THETA(1)*IPRED\nSD_ADD = THETA(2)\nSD = SQRT(SD_PROP**2 + SD_ADD**2)\nY = IPRED + SD * EPS(1)\n\n$SIGMA\n1 FIX\nIt wasn’t immediately clear why it was set up this way, and I was left with some questions:\nIt seemed a bit odd to me. I was more familiar with defining RUV directly in the $SIGMA block, something like:\nclassical way v1\n\n$ERROR\nIPRED = F\nY = IPRED + IPRED * EPS(1) + EPS(2)\n\n$SIGMA\n0.0225\n0.0529\nor maybe in a slightly more elegant form:\nclassical way v2\n\nY = IPRED * (1 + EPS(1)) + EPS(2)\nSo, why use this “alternative”2 way of defining the error? Before we try to explain this way of writing a combined error model to ourselves, let’s break down the additive and proportional error model separately to understand what’s going on. Please note: most of this content can also be found elsewhere (Proost 2017)."
  },
  {
    "objectID": "posts/expressing_ruv_as_theta/index.html#additive-error-models",
    "href": "posts/expressing_ruv_as_theta/index.html#additive-error-models",
    "title": "Expressing RUV as $THETA in NONMEM",
    "section": "Additive error models",
    "text": "Additive error models\nThe “classical” way (if I can call it that) of specifying an additive error model in NONMEM is as follows:\n\n\n\nclassical way (additive)\n\n$ERROR\nIPRED = F\nY = IPRED + EPS(1)\n\n$SIGMA\n0.0529\n\n\nIn this approach, RUV is defined directly in the $SIGMA block, where EPS(1) is assumed to be normally distributed with a mean of 0 and variance of 0.0529:\n\\[EPS(1) \\sim \\mathcal{N}(0,0.0529)\\]\nIt is quite important to note that we are specifying variances in $SIGMA. Now the alternative way (my colleague called it the Uppsala way3) of coding the additive error model looks like this:\n\n\n\nalternative way (additive)\n\n$THETA\n0.23        ; RUV_ADD\n\n$ERROR\nIPRED = F\nSD_ADD = THETA(1)\nY = IPRED + SD_ADD * EPS(1)\n\n$SIGMA\n1 FIX\n\n\nHere, $SIGMA is fixed so EPS(1) has a variance of 1, effectively making it a standard normal distribution:\n\\[EPS(1) \\sim \\mathcal{N}(0,1)\\]\nBut we then multiply this random variable EPS(1) by a scaling factor SD_ADD (which is being estimated as a THETA parameter) before the product is being added to the individual predicted IPRED value:\n\n\n\nalternative way (additive)\n\nY = IPRED + SD_ADD * EPS(1)\n\n\nI am not super familiar what happens if we multiply a random variable with a scaling factor. So maybe it is a good idea to visualize what happens when we fix $SIGMA to 1 and multiply it by SD = 0.23. Let’s start with plotting a standard normal distribution ($SIGMA 1 FIX):\n\n\nCode\n# sample from standard normal distribution\nx &lt;- rnorm(100000, mean = 0, sd = 1)\nstd_norm &lt;- tibble(x = x, source = \"unscaled\")\n\n# plot\nstd_norm |&gt; \n  ggplot(aes(x = x, fill = source)) +\n  geom_density(alpha=0.2)+\n  labs(title = \"Standard normal distribution\", x = \"\", y = \"Density\")+\n  scale_fill_manual(\n    \"Source\",\n    values = c(\n      \"unscaled\" = \"#003049\"\n    )\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThe resulting standard deviation should be 1, and since \\(1^2 = 1\\), the resulting variance should also be 1. Let’s be sure and check our empirical estimates (it is a simulation, after all) to confirm this:\n\n\nCode\n# summarize data and calculate sd and variance\nstd_norm |&gt; \n  group_by(source) |&gt;   \n  summarize(\n    sd = sd(x) |&gt; signif(digits = 3),\n    var = var(x) |&gt; signif(digits = 3)\n  ) |&gt; \n  rename(\n    \"Source\" = source,\n    \"Standard Deviation\" = sd,\n    \"Variance\" = var\n  ) |&gt;\n  kbl() |&gt; kable_styling()\n\n\n\n\n\nSource\nStandard Deviation\nVariance\n\n\n\n\nunscaled\n0.998\n0.996\n\n\n\n\n\n\n\nGood. But what happens now to this standard normal distribution if we multiply the random variable with some scaling parameter \\(SD = 0.23\\)? Let’s find out:\n\n\nCode\n# set a seed\nset.seed(123)\n\n# multiply with W\nSD &lt;- 0.23\nx_scaled &lt;- x * SD\nstd_norm_scaled &lt;- tibble(x = x_scaled, source = \"scaled\")\n\n# combine both\nstd_norm_combined &lt;- bind_rows(std_norm, std_norm_scaled)\n\n# plot\nstd_norm_combined |&gt; \n  ggplot(aes(x = x, fill = source)) +\n  geom_density(alpha = 0.2)+\n  labs(title = \"Normal distributions: Impact of scaling factor SD\", x = \"\", y = \"Density\")+\n  scale_fill_manual(\n    \"Source\",\n    values = c(\n      \"unscaled\" = \"#003049\",  # Blue color for original\n      \"scaled\" = \"#c1121f\"     # Orange color for scaled\n    )\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nLet’s compare the standard deviation and variance of both distributions:\n\n\nCode\n# summarize data and calculate sd and variance\nstd_norm_combined |&gt; \n  group_by(source) |&gt;   \n  summarize(\n    sd = sd(x) |&gt; signif(digits = 2),\n    var = var(x) |&gt; signif(digits = 2)\n  ) |&gt; \n  rename(\n    \"Source\" = source,\n    \"Standard Deviation\" = sd,\n    \"Variance\" = var\n  ) |&gt; \n  kbl() |&gt; \n  kable_styling()\n\n\n\n\n\nSource\nStandard Deviation\nVariance\n\n\n\n\nscaled\n0.23\n0.053\n\n\nunscaled\n1.00\n1.000\n\n\n\n\n\n\n\nFor the scaled distribution, we can see that the resulting standard deviation \\(\\sigma\\) is approximately equal to our scaling factor SD_ADD (which is 0.23) and the variance is \\(0.23^2 \\approx 0.053\\). This means that in our model code\n\n\n\nalternative way (additive)\n\nSD_ADD * EPS(1)\n\n\nthe SD_ADD parameter (specified via $THETA) is representing a standard deviation. Cool thing! Probably it’s not too surprising given my naming scheme, but anyways.4 Overall, both of these models should be equivalent:\n\n\n\nclassical way (additive)\n\n$SIGMA\n0.0529   ; variance\n\n\nand\n\n\n\nalternative way (additive)\n\n$THETA\n0.23   ; standard deviation\n\n$SIGMA\n1 FIX\n\n\nTo sum it up: We need to be careful with the units. If we use the classical way, we are estimating a variance via $SIGMA, but if we use the alternative way, we are estimating a standard deviation via $THETA and fix the $SIGMA to a standard normal. Typically, we would report the standard deviation (rather than the variance) if we use an additive model, and I think one of the advantages of the alternative way is that we directly read out the standard deviation from the parameter estimates (without the need to transform anything). Some also say that the estimation becomes more stable if we model the stochastic parts via $THETA, but I cannot judge if this is true or not.\n\n\n\n\n\n\nSpecifying additive RUV via $THETA gives us a standard deviation\n\n\n\nWhenever we have an additive error model and we specify the RUV in the $THETA block (the alternative way), the resulting estimate is a standard deviation."
  },
  {
    "objectID": "posts/expressing_ruv_as_theta/index.html#proportional-error-models",
    "href": "posts/expressing_ruv_as_theta/index.html#proportional-error-models",
    "title": "Expressing RUV as $THETA in NONMEM",
    "section": "Proportional error models",
    "text": "Proportional error models\nNow, let’s look at proportional error models. The classical way of specifying the proportional error model looks like this:\n\n\n\nclassical way (proportional)\n\n$ERROR\nIPRED = F\nY = IPRED + IPRED * EPS(1)\n\n$SIGMA\n0.0225\n\n\nAnd the alternative way is:\n\n\n\nalternative way (proportional)\n\n$THETA\n0.15        ; RUV_PROP\n\n$ERROR\nIPRED = F\nSD_PROP = IPRED * THETA(1)\nY = IPRED + SD_PROP * EPS(1)\n\n$SIGMA\n1 FIX\n\n\nThe structure is similar to the additive model we discussed earlier, except that the standard deviation of the random noise around our prediction depends on the prediction itself. This is why we first calculate the standard deviation SD_PROP at the given prediction as:\n\n\n\nalternative way (proportional)\n\nSD_PROP = IPRED * THETA(1)\n\n\nThis already gives us an understanding of the units of THETA(1): it represents the coefficient of variation (CV) of the prediction IPRED. Why? A coefficient of variation represents the ratio of the standard deviation to the mean. This is why we end up with a standard deviation (SD_PROP) if we multiply the prediction (IPRED) with the CV (THETA(1)). So we always have a fraction of the prediction representing our standard deviation at that point.\n\nAn example\nSuppose we have a prediction (IPRED) of 10 mg/L and we want to show the resulting distribution. For the classical approach, we would specify a variance (EPS(1)) of 0.0225, and for the alternative way, we would specify a CV (THETA(1)) of 0.15. What do you think? Will this be equivalent or not? Let’s find out!\n\n\nCode\n# Set seed for reproducibility\nset.seed(123)\n\n# Parameters\nIPRED &lt;- 10         \nCV_percent &lt;- 0.15    \nSD_prop &lt;- CV_percent * IPRED  \nsd_classical &lt;- IPRED * sqrt(0.0225)  \n\n# Number of samples\nn &lt;- 100000\n\n# Classical way: Specify variance directly\neps_classical &lt;- rnorm(n, mean = 10, sd = sd_classical)  \n\n# Alternative way: Specify CV%\neps_alternative &lt;- rnorm(n, mean = 10, sd = 1 * SD_prop) \n\n# Create a tibble combining both distributions\nprop_models &lt;- tibble(\n  value = c(eps_classical, eps_alternative),\n  source = rep(c(\"Classical (Variance = 0.0225)\", \"Alternative (CV = 0.15)\"), each = n)\n)\n\n# Plot the density of both distributions\nprop_models |&gt; \n  ggplot(aes(x = value, fill = source)) +\n  geom_density(alpha = 0.4) +\n  labs(\n    title = \"Classical vs. alternative specification\",\n    x = \"Concentration [mg/L]\",\n    y = \"Density\"\n  ) +\n  scale_fill_manual(\n    \"Model Specification\",\n    values = c(\n      \"Classical (Variance = 0.0225)\" = \"#003049\",  # Blue\n      \"Alternative (CV = 0.15)\" = \"#c1121f\"      # Red\n    )\n  ) +\n  scale_x_continuous(breaks=c(4,6,8,10,12,14,16))+\n  theme_bw()\n\n\n\n\n\n\n\n\n\nBoth models end up with the same distribution. In the classical way, we are given a variance of 0.0225. To get the standard deviation, we take the square root of the variance:\n\\[\n\\sigma_{EPS} = \\sqrt{0.0225} = 0.15\n\\] This means, that our random variable EPS(1) has a standard deviation of 0.15 mg/L in our classical model:\n\n\n\nclassical way (proportional)\n\nY = IPRED + IPRED * EPS(1)\n\n\nBy multiplying this EPS(1) by the prediction (IPRED) of 10 mg/L, we are scaling this random variable to have the (desired) standard deviation of the prediction distribution (PRED):\n\\[\n\\sigma_{Y} = 0.15 \\times 10 = 1.5 \\, \\text{mg/L}\n\\]\nIn the alternative way, we are directly estimating the coefficient of variation (CV) as 0.15.\n\n\n\nalternative way (proportional)\n\nSD_PROP = IPRED * THETA(1)\nY = IPRED + SD_PROP * EPS(1)\n\n\nWe are first calculating the respective standard deviation (SD_PROP) by multiplying CV with IPRED. We then turn this standard deviation into a random variable with this standard deviation by multiplying it with a random variable from a standard normal (EPS(1)). Also here, the respective standard deviation of the prediction distribution (PRED) is 1.5 mg/L:\n\\[\n\\sigma_{Y} = 0.15 \\times 10 = 1.5 \\, \\text{mg/L}\n\\]\nIn both cases, the resulting variability is the same, meaning both approaches lead to the same standard deviation of 1.5 mg/L. Again, it is a bit more convenient to specify the CV directly, as it is more intuitive and easier to interpret. And if the stability argument is true (see above), we would also make our estimation more robust this way.\n\n\n\n\n\n\nSpecifying proportional RUV in $THETA gives us a coefficient of variation\n\n\n\nWhenever we have a proportional error model and we specify the RUV in the $THETA block, the resulting estimate is a coefficient of variation."
  },
  {
    "objectID": "posts/expressing_ruv_as_theta/index.html#combined-proportional-and-additive-error-models",
    "href": "posts/expressing_ruv_as_theta/index.html#combined-proportional-and-additive-error-models",
    "title": "Expressing RUV as $THETA in NONMEM",
    "section": "Combined proportional and additive error models",
    "text": "Combined proportional and additive error models\nFinally, let’s combine our knowledge to understand the alternative way of specifying a combined proportional and additive error model:\n\n\n\nalternative way (combined)\n\n$THETA\n0.15        ; RUV_PROP\n0.23        ; RUV_ADD\n\n$ERROR\nIPRED = F\nSD_PROP = THETA(1)*IPRED\nSD_ADD = THETA(2)\nSD = SQRT(SD_PROP**2 + SD_ADD**2)\nY = IPRED + SD * EPS(1)\n\n$SIGMA\n1 FIX\n\n\nTwo parts should already be familiar:\n\n\n\nalternative way (combined)\n\nSD_PROP = THETA(1)*IPRED\nSD_ADD = THETA(2)\n\n\nIn the first part, we calculate SD_PROP, representing the resulting standard deviation of the proportional part (as THETA(1) is a CV). The second part, SD_ADD, gives us the standard deviation of the additive part. Now we want to find the joint standard deviation SD at the given concentration. But how do we combine these components?\n\n\n\nalternative way (combined)\n\nSD = SQRT(SD_PROP**2 + SD_ADD**2)\n\n\nWe can see that we first square both terms, then add them together, then take the square root. Sounds complicated - why not just add them directly together? This is because variances are additive when combining independent random variables, while standard deviations are not (Soch 2020). Written a bit more formally for two independent random variables (we typically assume the covariance to be 0 when modelling RUV):\n\\[\\mathrm{Var}(X + Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y)\\] In our case, SD_PROP and SD_ADD are standard deviations, so we must first square them to get the variances and then add them. However, we want to go back to a standard deviation before we multiply SD with EPS(1) (being fixed to 1). Therefore, we take the square root in the end.\nThis operation has always confused me a bit, but once I understood that I can sum up variances, but not standard deviations 5 it made more sense to me.\n\n\n\n\n\n\nCombined error models\n\n\n\nWhen specifying a combined error model, the estimates in the $THETA block represent a standard deviation for the additive part and a coefficient of variation for the proportional part."
  },
  {
    "objectID": "posts/expressing_ruv_as_theta/index.html#conclusion",
    "href": "posts/expressing_ruv_as_theta/index.html#conclusion",
    "title": "Expressing RUV as $THETA in NONMEM",
    "section": "Conclusion",
    "text": "Conclusion\nThis is a somewhat lengthy explanation of why and how we code the alternative approach in NONMEM. Personally, I wasn’t very familiar with how distributions behave when its random variable is being multiplying by a factor, and I didn’t realize that while variances are additive when combining two random processes, standard deviations are not. If you have a stronger background in statistics, this might have been obvious, but I hope this explanation was still helpful for some others."
  },
  {
    "objectID": "posts/expressing_ruv_as_theta/index.html#footnotes",
    "href": "posts/expressing_ruv_as_theta/index.html#footnotes",
    "title": "Expressing RUV as $THETA in NONMEM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYeah, I know, not really fancy. But that’s how it feels when you touch a combined error model for the first time.↩︎\nFor many of you, this is likely quite standard. The naming reflects my perspective.↩︎\nI’m not sure if this was initially introduced by one of the Uppsala groups or if this is just some hearsay.↩︎\nSome people also code it with W instead of SD but it’s always a good idea to find descriptive variable names.↩︎\nProbably something you would tackle in the first semester of your statistics studies. But not if you study pharmacy ;)↩︎"
  },
  {
    "objectID": "posts/about_this_blog/index.html",
    "href": "posts/about_this_blog/index.html",
    "title": "About this blog",
    "section": "",
    "text": "As someone involved in pharmacometrics, I’ve often found that many concepts are either poorly documented or explained in such technical language that they become hard to grasp (at least for me). This can be quite frustrating when you’re trying to learn or understand something new for which you don’t have a suitable mathematical and statistical background.\nThere are many nice blogs and articles in the field of pharmacometrics, such as\n\nDanielle Navarro\nPMX Solutions\nTingjie Guo’s NMHelp\nmrgsolve\n\nFor instance, Danielle Navarro’s post on population pharmacokinetic models in stan was incredibly helpful in my understanding of implementing pop-PK models in Stan. She just explained the concepts in a very easy-to-understand way and it is at the same time quite entertaining to read. Please check out her blog, if you haven’t already!\nI often explain new concepts to myself using Quarto or RMarkdown documents and I thought I might as well share them as little blog posts. While I don’t expect to match the quality of the blogs I’ve mentioned, I still hope that some posts can be helpful to others who are tackling similar challenges."
  },
  {
    "objectID": "posts/about_this_blog/index.html#motivation",
    "href": "posts/about_this_blog/index.html#motivation",
    "title": "About this blog",
    "section": "",
    "text": "As someone involved in pharmacometrics, I’ve often found that many concepts are either poorly documented or explained in such technical language that they become hard to grasp (at least for me). This can be quite frustrating when you’re trying to learn or understand something new for which you don’t have a suitable mathematical and statistical background.\nThere are many nice blogs and articles in the field of pharmacometrics, such as\n\nDanielle Navarro\nPMX Solutions\nTingjie Guo’s NMHelp\nmrgsolve\n\nFor instance, Danielle Navarro’s post on population pharmacokinetic models in stan was incredibly helpful in my understanding of implementing pop-PK models in Stan. She just explained the concepts in a very easy-to-understand way and it is at the same time quite entertaining to read. Please check out her blog, if you haven’t already!\nI often explain new concepts to myself using Quarto or RMarkdown documents and I thought I might as well share them as little blog posts. While I don’t expect to match the quality of the blogs I’ve mentioned, I still hope that some posts can be helpful to others who are tackling similar challenges."
  },
  {
    "objectID": "posts/about_this_blog/index.html#disclaimers",
    "href": "posts/about_this_blog/index.html#disclaimers",
    "title": "About this blog",
    "section": "Disclaimers",
    "text": "Disclaimers\nBefore diving in, a few things to note:\n\nI’m not an expert in everything I write about; I’m learning as I go and simply documenting my process. Expect some mistakes along the way!\nEnglish is not my first language, so I rely on tools like ChatGPT and DeepL to help with writing. I also use tools like ChatGPT to generate ideas or to find good ways to visualize concepts.\nI do my best to give credit where it’s due and cite sources when appropriate. Much of what I share builds on others’ work, and I’m not trying to reinvent the wheel."
  },
  {
    "objectID": "posts/nlme_estimation_algorithm/index.html",
    "href": "posts/nlme_estimation_algorithm/index.html",
    "title": "My attempt to understand NLME estimation algorithms in NONMEM",
    "section": "",
    "text": "In our little example we assume to have a (simple) hierarchical nonlinear mixed-effects (NLME) model, for which we want to conduct a parameter estimation step. To my understanding the hierarchical structure is given by having variability defined on a population (=parameter) level and an individual (=observation) level, where the individual level depends on the parameter level.\n\n\nThe population level is represented by an inter-individual variability (IIV) term, which assumes a log-normal distribution around a typical parameter value. In this simplified example we only consider IIV on clearance and do not consider any other random effects. The population (or parameter) level can be defined as follows:\n\\[CL = \\theta_{TVCL} \\cdot e^{\\eta_{i}},~~~~~\\eta_{i} \\sim N(0, \\omega^2)\\]\nHere, the clearance (\\(CL\\)) is modeled as a random variable following a log-normal distribution centered around the typical clearance value (\\(\\theta_{\\text{TVCL}}\\)) with variance \\(\\omega^2\\). The random effect \\(\\eta_i\\) itself follows a normal distribution \\(N(0, \\omega^2)\\).\n\n\n\nThe individual level on the other hand is defined by the observed concentrations for each subject and the predicted concentrations. The predictions are based on the structural model and dependent on the individual parameters. If I am not mistaken, this dependency is a representation of the hierarchical and nested nature of our NLME model. The individual level also incorporates residual unexplained variability (RUV), which is an important piece since it enables us to define the likelihood function in the end. The individual level can be defined by:\n\\[Y_{ij} = f(x_{ij}; CL_i) + \\epsilon_{ij},~~~~~\\epsilon_{ij} \\sim N(0, \\sigma^2)\\] where we can note that:\n\n\\(Y_{ij}\\) is the observed concentration for the \\(i^{th}\\) individual at the \\(j^{th}\\) time point, which would be one row (observation) of our dataset.\n\\(f(x_{ij}; CL_i)\\) is the predicted concentration.\n\nIt contains the function \\(f()\\), which represents e.g., the set of ODEs that describe the PK model.\nThe function \\(f()\\) depends on the individual clearance (\\(CL_i\\), can be seen as a realization of the random variable \\(CL\\)) and the variable \\(x_{ij}\\).\nThis \\(x_{ij}\\) contains all the information about covariates (if we would have any), dosing and sampling events for the \\(i^{th}\\) individual at the \\(j^{th}\\) time point.\n\n\\(\\epsilon_{ij}\\) is the residual unexplained variability for the \\(i^{th}\\) individual at the \\(j^{th}\\) time point. It typically follows a normal distribution \\(N(0, \\sigma^2)\\)\n\nIn our example we have two random variables, \\(Y_{ij}\\) and \\(CL_i\\), with parameters \\(\\beta := (\\theta_{TVCL}, \\omega^2, \\sigma^2)\\). In our example we just want to estimate the typical clearance \\(\\theta_{TVCL}\\) and the IIV on clearance \\(\\omega^2\\). The residual unexplained variability \\(\\sigma^2\\) is assumed to be known and fixed. Why do we do that? Just that we can plot the surface of our objective function value in 3D and better undestand how it looks like.\n\n\n\n\nIn our simple case example (with fixed \\(V_D\\) and fixed residual unexplained variability \\(\\sigma^2\\)), we have only two parameters to estimate: \\(\\theta_{TVCL}\\) and \\(\\omega^2\\). The overall goal? To infer the parameters of interest \\((\\hat{\\theta}_{TVCL}, \\hat{\\omega^2})\\) from our observed data \\(y_{1:n}\\) by maximizing the log-likelihood (\\(ln ~L\\)) function:\n\\[(\\hat{\\theta}_{TVCL}, \\hat{\\omega^2})_{ML} = \\underset{\\theta_{TVCL},~ \\omega^2}{\\mathrm{argmax}}~ln~L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}, CL_{i:n}\\right)\\]\nTo align more with the notation in Wang (2007), we can rather deal with the \\(\\eta_i\\) values instead of the realization in the parameter space. We can write the likelihood as:\n\\[(\\hat{\\theta}_{TVCL}, \\hat{\\omega^2})_{ML} = \\underset{\\theta_{TVCL},~ \\omega^2}{\\mathrm{argmax}}~ln~L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}, \\eta_{i:n}\\right)\\]\n\nHowever, the individual \\(\\eta_i\\) values are so called unobserved latent variables. We can only directly observe the \\(y_i\\) values, not the \\(\\eta_i\\) values. Therefore, we cannot easily compute the joint likelihood.\nAnsatz: We want to get rid of the dependence on \\(\\eta_i\\) by working with the marginal likelihood.\nThe marginal likelihood is calculated by integrating out the individual parameters \\(\\eta_i\\) (“marginalizing out \\(\\eta_i\\)”):\n\n\\[(\\hat{\\theta}_{TVCL}, \\hat{\\omega^2})_{ML} = \\underset{\\theta_{TVCL}, \\omega^2}{\\mathrm{argmax}}~ln~L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right)\\]\nwith\n\\[L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right) = p(y_{1:n}; \\theta_{TVCL}, \\omega^2) = \\prod_{i=1}^n p(y_{i}; \\theta_{TVCL},~ \\omega^2)\\]\n\\[L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right) = \\prod_{i=1}^n \\int p(y_{i}, \\eta_i; \\theta_{TVCL}, \\omega^2) \\cdot d\\eta_i \\]\n\nBy integrating over all possible values of \\(\\eta_i\\) we got rid of the dependence and are now left with teh marginal likelihood.\nWe can further split the marginal likelihood equation by using the chain rule of probability:\n\n\\[L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right) = \\prod_{i=1}^n \\int p(y_{i}| \\eta_i; \\theta_{TVCL}, \\omega^2) \\cdot p(\\eta_i | \\theta_{TVCL}, \\omega^2) \\cdot d\\eta_i\\] As \\(p(\\eta_i | \\theta_{TVCL}, \\omega^2)\\) does actually not depend on \\(\\theta_{TVCL}\\), and \\(p(y_{i}| \\eta_i; \\theta_{TVCL}, \\omega^2)\\) does not actually depend on \\(\\omega^2\\), we can simplify the equation to:\n\\[L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right) = \\prod_{i=1}^n \\int p(y_{i}| \\eta_i; \\theta_{TVCL}) \\cdot p(\\eta_i | \\omega^2) \\cdot d\\eta_i\\]\n\nThe integral now contains two parts: The individual level \\(p(y_{i}| \\eta_i; \\theta_{TVCL})\\) and the population level \\(p(\\eta_i | \\omega^2)\\). The intuition behind this can be seen as follows: For a given \\(\\eta_i\\) within the integral, the population term \\(p(\\eta_i |\\omega^2)\\) tells us how likely it is to observe this \\(\\eta_i\\) value in the population. The individual term \\(p(y_{i}| \\eta_i; \\theta_{TVCL})\\) tells us how likely it is to observe the \\(y_i\\) value given that particular \\(\\eta_i\\) value. The Likelihood will be maximal when the product of both terms is maximal.\nHowever, solving the marginal likelihood is much harder due to the integral. The question to answer is: “How can we maximize the marginal log likelihood function?”"
  },
  {
    "objectID": "posts/nlme_estimation_algorithm/index.html#statistical-model",
    "href": "posts/nlme_estimation_algorithm/index.html#statistical-model",
    "title": "My attempt to understand NLME estimation algorithms in NONMEM",
    "section": "",
    "text": "In our little example we assume to have a (simple) hierarchical nonlinear mixed-effects (NLME) model, for which we want to conduct a parameter estimation step. To my understanding the hierarchical structure is given by having variability defined on a population (=parameter) level and an individual (=observation) level, where the individual level depends on the parameter level.\n\n\nThe population level is represented by an inter-individual variability (IIV) term, which assumes a log-normal distribution around a typical parameter value. In this simplified example we only consider IIV on clearance and do not consider any other random effects. The population (or parameter) level can be defined as follows:\n\\[CL = \\theta_{TVCL} \\cdot e^{\\eta_{i}},~~~~~\\eta_{i} \\sim N(0, \\omega^2)\\]\nHere, the clearance (\\(CL\\)) is modeled as a random variable following a log-normal distribution centered around the typical clearance value (\\(\\theta_{\\text{TVCL}}\\)) with variance \\(\\omega^2\\). The random effect \\(\\eta_i\\) itself follows a normal distribution \\(N(0, \\omega^2)\\).\n\n\n\nThe individual level on the other hand is defined by the observed concentrations for each subject and the predicted concentrations. The predictions are based on the structural model and dependent on the individual parameters. If I am not mistaken, this dependency is a representation of the hierarchical and nested nature of our NLME model. The individual level also incorporates residual unexplained variability (RUV), which is an important piece since it enables us to define the likelihood function in the end. The individual level can be defined by:\n\\[Y_{ij} = f(x_{ij}; CL_i) + \\epsilon_{ij},~~~~~\\epsilon_{ij} \\sim N(0, \\sigma^2)\\] where we can note that:\n\n\\(Y_{ij}\\) is the observed concentration for the \\(i^{th}\\) individual at the \\(j^{th}\\) time point, which would be one row (observation) of our dataset.\n\\(f(x_{ij}; CL_i)\\) is the predicted concentration.\n\nIt contains the function \\(f()\\), which represents e.g., the set of ODEs that describe the PK model.\nThe function \\(f()\\) depends on the individual clearance (\\(CL_i\\), can be seen as a realization of the random variable \\(CL\\)) and the variable \\(x_{ij}\\).\nThis \\(x_{ij}\\) contains all the information about covariates (if we would have any), dosing and sampling events for the \\(i^{th}\\) individual at the \\(j^{th}\\) time point.\n\n\\(\\epsilon_{ij}\\) is the residual unexplained variability for the \\(i^{th}\\) individual at the \\(j^{th}\\) time point. It typically follows a normal distribution \\(N(0, \\sigma^2)\\)\n\nIn our example we have two random variables, \\(Y_{ij}\\) and \\(CL_i\\), with parameters \\(\\beta := (\\theta_{TVCL}, \\omega^2, \\sigma^2)\\). In our example we just want to estimate the typical clearance \\(\\theta_{TVCL}\\) and the IIV on clearance \\(\\omega^2\\). The residual unexplained variability \\(\\sigma^2\\) is assumed to be known and fixed. Why do we do that? Just that we can plot the surface of our objective function value in 3D and better undestand how it looks like."
  },
  {
    "objectID": "posts/nlme_estimation_algorithm/index.html#maximum-likelihood-estimation",
    "href": "posts/nlme_estimation_algorithm/index.html#maximum-likelihood-estimation",
    "title": "My attempt to understand NLME estimation algorithms in NONMEM",
    "section": "",
    "text": "In our simple case example (with fixed \\(V_D\\) and fixed residual unexplained variability \\(\\sigma^2\\)), we have only two parameters to estimate: \\(\\theta_{TVCL}\\) and \\(\\omega^2\\). The overall goal? To infer the parameters of interest \\((\\hat{\\theta}_{TVCL}, \\hat{\\omega^2})\\) from our observed data \\(y_{1:n}\\) by maximizing the log-likelihood (\\(ln ~L\\)) function:\n\\[(\\hat{\\theta}_{TVCL}, \\hat{\\omega^2})_{ML} = \\underset{\\theta_{TVCL},~ \\omega^2}{\\mathrm{argmax}}~ln~L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}, CL_{i:n}\\right)\\]\nTo align more with the notation in Wang (2007), we can rather deal with the \\(\\eta_i\\) values instead of the realization in the parameter space. We can write the likelihood as:\n\\[(\\hat{\\theta}_{TVCL}, \\hat{\\omega^2})_{ML} = \\underset{\\theta_{TVCL},~ \\omega^2}{\\mathrm{argmax}}~ln~L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}, \\eta_{i:n}\\right)\\]\n\nHowever, the individual \\(\\eta_i\\) values are so called unobserved latent variables. We can only directly observe the \\(y_i\\) values, not the \\(\\eta_i\\) values. Therefore, we cannot easily compute the joint likelihood.\nAnsatz: We want to get rid of the dependence on \\(\\eta_i\\) by working with the marginal likelihood.\nThe marginal likelihood is calculated by integrating out the individual parameters \\(\\eta_i\\) (“marginalizing out \\(\\eta_i\\)”):\n\n\\[(\\hat{\\theta}_{TVCL}, \\hat{\\omega^2})_{ML} = \\underset{\\theta_{TVCL}, \\omega^2}{\\mathrm{argmax}}~ln~L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right)\\]\nwith\n\\[L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right) = p(y_{1:n}; \\theta_{TVCL}, \\omega^2) = \\prod_{i=1}^n p(y_{i}; \\theta_{TVCL},~ \\omega^2)\\]\n\\[L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right) = \\prod_{i=1}^n \\int p(y_{i}, \\eta_i; \\theta_{TVCL}, \\omega^2) \\cdot d\\eta_i \\]\n\nBy integrating over all possible values of \\(\\eta_i\\) we got rid of the dependence and are now left with teh marginal likelihood.\nWe can further split the marginal likelihood equation by using the chain rule of probability:\n\n\\[L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right) = \\prod_{i=1}^n \\int p(y_{i}| \\eta_i; \\theta_{TVCL}, \\omega^2) \\cdot p(\\eta_i | \\theta_{TVCL}, \\omega^2) \\cdot d\\eta_i\\] As \\(p(\\eta_i | \\theta_{TVCL}, \\omega^2)\\) does actually not depend on \\(\\theta_{TVCL}\\), and \\(p(y_{i}| \\eta_i; \\theta_{TVCL}, \\omega^2)\\) does not actually depend on \\(\\omega^2\\), we can simplify the equation to:\n\\[L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right) = \\prod_{i=1}^n \\int p(y_{i}| \\eta_i; \\theta_{TVCL}) \\cdot p(\\eta_i | \\omega^2) \\cdot d\\eta_i\\]\n\nThe integral now contains two parts: The individual level \\(p(y_{i}| \\eta_i; \\theta_{TVCL})\\) and the population level \\(p(\\eta_i | \\omega^2)\\). The intuition behind this can be seen as follows: For a given \\(\\eta_i\\) within the integral, the population term \\(p(\\eta_i |\\omega^2)\\) tells us how likely it is to observe this \\(\\eta_i\\) value in the population. The individual term \\(p(y_{i}| \\eta_i; \\theta_{TVCL})\\) tells us how likely it is to observe the \\(y_i\\) value given that particular \\(\\eta_i\\) value. The Likelihood will be maximal when the product of both terms is maximal.\nHowever, solving the marginal likelihood is much harder due to the integral. The question to answer is: “How can we maximize the marginal log likelihood function?”"
  },
  {
    "objectID": "cv/cv.html",
    "href": "cv/cv.html",
    "title": "Education",
    "section": "",
    "text": "Education\n\n2022: License to Practise as a Pharmacist - LaGeSo Berlin\n2017 - 2021: Pharmacy Studies - Freie Universität Berlin (8 semesters | GPA 3.7)\n2015 - 2017: Chemistry and Biochemistry Studies - FU und HU Berlin (3 semesters | GPA 2.7)\n2006 - 2014: Secondary School - St. Ursula Gymnasium Attendorn (GPA 3.1)\n\n\n\nWork Experience\n\nOct 2022 - Dec 2022: Intern - Boehringer Ingelheim TMCP Data Science Building up a code library for various modeling approaches and adjusting reporting tools.\nNov 2021 - Apr 2022: Short-term Scholar - Center for Pharmacometrics and Systems Pharmacology, University of Florida PBPK Modeling & Simulation for drug-drug- and gene-drug-interactions of oxycodone.\nMay 2021 - Oct 2021: Pre-registration Pharmacist - Medios Apotheke an der Charite Internship in a community pharmacy.\nJul 2020 - Mar 2020: Teaching Assistant - Charité University Hospital Berlin Support for lab exercises and oral exams in ‘Medical Microbiology, Virology and Infection Prevention’.\nDec 2017 - Dec 2019: Student Assistant - PharMetrX Program Supported administrative tasks and planning of PharMetrX anniversary.\nAug 2019 - Sep 2019: University Internship - PharMetrX Program Modeling and simulation of meropenem concentration-time profiles in critically ill patients.\nSep 2018 - Oct 2018: Industry Internship - Sanofi Contributed to a white paper on modeling and simulation in pediatric type 2 diabetes patients.\n\n\n\nMiscellaneous\n\n2019 - 2021: Student Member - Institute Council, Freie Universität Berlin Represented student interests.\n2017 - 2021: Member - Students Council, Freie Universität Berlin Supported various student projects.\n2014 - 2015: Work and Travel - Australia, New Zealand, Asia 12-month duration.\n\n\n\nLanguage Skills\n\nGerman: Native\nEnglish: Fluent\nLatin: Basic\n\n\n\nComputer Skills\n\nMicrosoft Office: Intermediate\nLaTeX: Intermediate\nPharmacometrics: GastroPlus (Intermediate), Monolix (Beginner), MatLab/SimBiology (Beginner)\nProgramming: R (Intermediate), Python (Beginner), HTML/CSS/JavaScript (Beginner)\n\n\n\nCourses and Training\n\n2022: Pharmacometrics Spring School, Lixoft (MonolixSuite)\n2022: MATLAB/SimBiology PBPK Modeling Workshop\n2021: Python: Data Science, Machine Learning & Neural Networks (Udemy, 29.5h)"
  }
]