[
  {
    "objectID": "publications/template/xx-xx-xxxx_short_title.html",
    "href": "publications/template/xx-xx-xxxx_short_title.html",
    "title": "TITLE",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet."
  },
  {
    "objectID": "publications/template/xx-xx-xxxx_short_title.html#abstract",
    "href": "publications/template/xx-xx-xxxx_short_title.html#abstract",
    "title": "TITLE",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet."
  },
  {
    "objectID": "publications/publications_docs/07-03-2024_t1d_cts.html",
    "href": "publications/publications_docs/07-03-2024_t1d_cts.html",
    "title": "Type 1 diabetes prevention clinical trial simulator: Case reports of model-informed drug development tool",
    "section": "",
    "text": "Clinical trials seeking to delay or prevent the onset of type 1 diabetes (T1D) face a series of pragmatic challenges. Despite more than 100 years since the discovery of insulin, teplizumab remains the only FDA-approved therapy to delay progression from Stage 2 to Stage 3 T1D. To increase the efficiency of clinical trials seeking this goal, our project sought to inform T1D clinical trial designs by developing a disease progression model-based clinical trial simulation tool. Using individual-level data collected from the TrialNet Pathway to Prevention and The Environmental Determinants of Diabetes in the Young natural history studies, we previously developed a quantitative joint model to predict the time to T1D onset. We then applied trial-specific inclusion/exclusion criteria, sample sizes in treatment and placebo arms, trial duration, assessment interval, and dropout rate. We implemented a function for presumed drug effects. To increase the size of the population pool, we generated virtual populations using multivariate normal distribution and ctree machine learning algorithms. As an output, power was calculated, which summarizes the probability of success, showing a statistically significant difference in the time distribution until the T1D diagnosis between the two arms. Using this tool, power curves can also be generated through iterations. The web-based tool is publicly available: https://app.cop.ufl.edu/t1d/. Herein, we briefly describe the tool and provide instructions for simulating a planned clinical trial with two case studies. This tool will allow for improved clinical trial designs and accelerate efforts seeking to prevent or delay the onset of T1D."
  },
  {
    "objectID": "publications/publications_docs/07-03-2024_t1d_cts.html#abstract",
    "href": "publications/publications_docs/07-03-2024_t1d_cts.html#abstract",
    "title": "Type 1 diabetes prevention clinical trial simulator: Case reports of model-informed drug development tool",
    "section": "",
    "text": "Clinical trials seeking to delay or prevent the onset of type 1 diabetes (T1D) face a series of pragmatic challenges. Despite more than 100 years since the discovery of insulin, teplizumab remains the only FDA-approved therapy to delay progression from Stage 2 to Stage 3 T1D. To increase the efficiency of clinical trials seeking this goal, our project sought to inform T1D clinical trial designs by developing a disease progression model-based clinical trial simulation tool. Using individual-level data collected from the TrialNet Pathway to Prevention and The Environmental Determinants of Diabetes in the Young natural history studies, we previously developed a quantitative joint model to predict the time to T1D onset. We then applied trial-specific inclusion/exclusion criteria, sample sizes in treatment and placebo arms, trial duration, assessment interval, and dropout rate. We implemented a function for presumed drug effects. To increase the size of the population pool, we generated virtual populations using multivariate normal distribution and ctree machine learning algorithms. As an output, power was calculated, which summarizes the probability of success, showing a statistically significant difference in the time distribution until the T1D diagnosis between the two arms. Using this tool, power curves can also be generated through iterations. The web-based tool is publicly available: https://app.cop.ufl.edu/t1d/. Herein, we briefly describe the tool and provide instructions for simulating a planned clinical trial with two case studies. This tool will allow for improved clinical trial designs and accelerate efforts seeking to prevent or delay the onset of T1D."
  },
  {
    "objectID": "publications/publications_docs/04-20-2021_mero_risk_calculator_evaluation.html",
    "href": "publications/publications_docs/04-20-2021_mero_risk_calculator_evaluation.html",
    "title": "Evaluation of the MeroRisk Calculator, A User-Friendly Tool to Predict the Risk of Meropenem Target Non-Attainment in Critically Ill Patients",
    "section": "",
    "text": "The MeroRisk-calculator, an easy-to-use tool to determine the risk of meropenem target non-attainment after standard dosing (1000 mg; q8h), uses a patient’s creatinine clearance and the minimum inhibitory concentration (MIC) of the pathogen. In clinical practice, however, the MIC is rarely available. The objectives were to evaluate the MeroRisk-calculator and to extend risk assessment by including general pathogen sensitivity data.\n\n\n\nUsing a clinical routine dataset (155 patients, 891 samples), a direct data-based evaluation was not feasible. Thus, in step 1, the performance of a pharmacokinetic model was determined for predicting the measured concentrations. In step 2, the PK model was used for a model-based evaluation of the MeroRisk-calculator: risk of target non-attainment was calculated using the PK model and agreement with the MeroRisk-calculator was determined by a visual and statistical (Lin’s concordance correlation coefficient (CCC)) analysis for MIC values 0.125-16 mg/L. The MeroRisk-calculator was extended to include risk assessment based on EUCAST-MIC distributions and cumulative-fraction-of-response analysis.\n\n\n\nStep 1 showed a negligible bias of the PK model to underpredict concentrations (-0.84 mg/L). Step 2 revealed a high level of agreement between risk of target non-attainment predictions for creatinine clearances &gt;50 mL/min (CCC = 0.990), but considerable deviations for patients &lt;50 mL/min. For 27% of EUCAST-listed pathogens the median cumulative-fraction-of-response for the observed patients receiving standard dosing was &lt; 90%.\n\n\n\nThe MeroRisk-calculator was successfully evaluated: For patients with maintained renal function it allows a reliable and user-friendly risk assessment. The integration of pathogen-based risk assessment substantially increases the applicability of the tool."
  },
  {
    "objectID": "publications/publications_docs/04-20-2021_mero_risk_calculator_evaluation.html#abstract",
    "href": "publications/publications_docs/04-20-2021_mero_risk_calculator_evaluation.html#abstract",
    "title": "Evaluation of the MeroRisk Calculator, A User-Friendly Tool to Predict the Risk of Meropenem Target Non-Attainment in Critically Ill Patients",
    "section": "",
    "text": "The MeroRisk-calculator, an easy-to-use tool to determine the risk of meropenem target non-attainment after standard dosing (1000 mg; q8h), uses a patient’s creatinine clearance and the minimum inhibitory concentration (MIC) of the pathogen. In clinical practice, however, the MIC is rarely available. The objectives were to evaluate the MeroRisk-calculator and to extend risk assessment by including general pathogen sensitivity data.\n\n\n\nUsing a clinical routine dataset (155 patients, 891 samples), a direct data-based evaluation was not feasible. Thus, in step 1, the performance of a pharmacokinetic model was determined for predicting the measured concentrations. In step 2, the PK model was used for a model-based evaluation of the MeroRisk-calculator: risk of target non-attainment was calculated using the PK model and agreement with the MeroRisk-calculator was determined by a visual and statistical (Lin’s concordance correlation coefficient (CCC)) analysis for MIC values 0.125-16 mg/L. The MeroRisk-calculator was extended to include risk assessment based on EUCAST-MIC distributions and cumulative-fraction-of-response analysis.\n\n\n\nStep 1 showed a negligible bias of the PK model to underpredict concentrations (-0.84 mg/L). Step 2 revealed a high level of agreement between risk of target non-attainment predictions for creatinine clearances &gt;50 mL/min (CCC = 0.990), but considerable deviations for patients &lt;50 mL/min. For 27% of EUCAST-listed pathogens the median cumulative-fraction-of-response for the observed patients receiving standard dosing was &lt; 90%.\n\n\n\nThe MeroRisk-calculator was successfully evaluated: For patients with maintained renal function it allows a reliable and user-friendly risk assessment. The integration of pathogen-based risk assessment substantially increases the applicability of the tool."
  },
  {
    "objectID": "publications/publications_docs/02-02-2024_pk_safety_efficacy_tld1.html",
    "href": "publications/publications_docs/02-02-2024_pk_safety_efficacy_tld1.html",
    "title": "TLD-1, a novel liposomal doxorubicin, in patients with advanced solid tumors: Dose escalation and expansion part of a multicenter open-label phase I trial (SAKK 65/16)",
    "section": "",
    "text": "TLD-1 is a novel liposomal doxorubicin that compared favorably to conventional doxorubicin liposomal formulations in preclinical models. This phase I first-in-human study aimed to define the maximum tolerated dose (MTD), recommended phase 2 dose (RP2D), safety and preliminary activity of TLD-1 in patients with advanced solid tumors.\n\n\n\nWe recruited patients with advanced solid tumors who failed standard therapy and received up to 3 prior lines of palliative systemic chemotherapy. TLD-1 was administered intravenously every 3 weeks up to a maximum of 9 cycles (6 for patients with prior anthracyclines) from a starting dose of 10 mg/m2, according to an accelerated titration design followed by a modified continual reassessment method.\n\n\n\n30 patients were enrolled between November 2018 and May 2021. No dose-limiting toxicities (DLT) were observed. Maximum administered dose of TLD-1 was 45 mg/m2, RP2D was defined at 40 mg/m2. Most frequent treatment-related adverse events (TRAE) of any grade included palmar-plantar erythrodysesthesia (PPE) (50% of patients), oral mucositis (50%), fatigue (30%) and skin rash (26.7%). Most common G3 TRAE included PPE in 4 patients (13.3%) and oral mucositis in 2 (6.7%). Overall objective response rate was 10% in the whole population and 23.1% among 13 patients with breast cancer; median time-to-treatment failure was 2.7 months. TLD-1 exhibit linear pharmacokinetics, with a median terminal half-life of 95 h.\n\n\n\nThe new liposomal doxorubicin formulation TLD-1 showed a favourable safety profile and antitumor activity, particularly in breast cancer. RP2D was defined at 40 mg/m2 administered every 3 weeks. (NCT03387917)"
  },
  {
    "objectID": "publications/publications_docs/02-02-2024_pk_safety_efficacy_tld1.html#abstract",
    "href": "publications/publications_docs/02-02-2024_pk_safety_efficacy_tld1.html#abstract",
    "title": "TLD-1, a novel liposomal doxorubicin, in patients with advanced solid tumors: Dose escalation and expansion part of a multicenter open-label phase I trial (SAKK 65/16)",
    "section": "",
    "text": "TLD-1 is a novel liposomal doxorubicin that compared favorably to conventional doxorubicin liposomal formulations in preclinical models. This phase I first-in-human study aimed to define the maximum tolerated dose (MTD), recommended phase 2 dose (RP2D), safety and preliminary activity of TLD-1 in patients with advanced solid tumors.\n\n\n\nWe recruited patients with advanced solid tumors who failed standard therapy and received up to 3 prior lines of palliative systemic chemotherapy. TLD-1 was administered intravenously every 3 weeks up to a maximum of 9 cycles (6 for patients with prior anthracyclines) from a starting dose of 10 mg/m2, according to an accelerated titration design followed by a modified continual reassessment method.\n\n\n\n30 patients were enrolled between November 2018 and May 2021. No dose-limiting toxicities (DLT) were observed. Maximum administered dose of TLD-1 was 45 mg/m2, RP2D was defined at 40 mg/m2. Most frequent treatment-related adverse events (TRAE) of any grade included palmar-plantar erythrodysesthesia (PPE) (50% of patients), oral mucositis (50%), fatigue (30%) and skin rash (26.7%). Most common G3 TRAE included PPE in 4 patients (13.3%) and oral mucositis in 2 (6.7%). Overall objective response rate was 10% in the whole population and 23.1% among 13 patients with breast cancer; median time-to-treatment failure was 2.7 months. TLD-1 exhibit linear pharmacokinetics, with a median terminal half-life of 95 h.\n\n\n\nThe new liposomal doxorubicin formulation TLD-1 showed a favourable safety profile and antitumor activity, particularly in breast cancer. RP2D was defined at 40 mg/m2 administered every 3 weeks. (NCT03387917)"
  },
  {
    "objectID": "publications/presentations_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html",
    "href": "publications/presentations_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html",
    "title": "A Bayesian-NLME approach identifies patients at risk of delayed MTX elimination if informative TDM data is provided",
    "section": "",
    "text": "https://www.pkpd-expertentreffen.de/"
  },
  {
    "objectID": "publications/presentations_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html#url",
    "href": "publications/presentations_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html#url",
    "title": "A Bayesian-NLME approach identifies patients at risk of delayed MTX elimination if informative TDM data is provided",
    "section": "",
    "text": "https://www.pkpd-expertentreffen.de/"
  },
  {
    "objectID": "publications/presentations_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html#abstract",
    "href": "publications/presentations_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html#abstract",
    "title": "A Bayesian-NLME approach identifies patients at risk of delayed MTX elimination if informative TDM data is provided",
    "section": "Abstract",
    "text": "Abstract"
  },
  {
    "objectID": "publications/presentations_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html#introduction",
    "href": "publications/presentations_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html#introduction",
    "title": "A Bayesian-NLME approach identifies patients at risk of delayed MTX elimination if informative TDM data is provided",
    "section": "Introduction",
    "text": "Introduction\nPhysicians must constantly assess treatment-related risks based on limited data. This is particularly difficult in oncology, as compounds often have a narrow therapeutic window. For example, patients receiving high-dose methotrexate (HD-MTX) are at increased risk of toxicity if MTX elimination is delayed (MTX &gt; threshold of 0.2 µM for &gt;72 h post-dose). To timely identify these patients (‘delayed eliminators’), TDM is being performed until MTX concentrations drop below the threshold. However, early translation of MTX concentrations and clinical biomarkers into a risk score at the bedside can be challenging. A Bayesian-NLME framework allows to combine prior knowledge about the population with individual measurements. Evaluating its predictive performance in clinically relevant scenarios with unseen patients is crucial to determine the clinical utility of the obtained predictions. However, most published HD-MTX models lack such validation, leaving their clinical applicability uncertain."
  },
  {
    "objectID": "publications/presentations_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html#objectives",
    "href": "publications/presentations_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html#objectives",
    "title": "A Bayesian-NLME approach identifies patients at risk of delayed MTX elimination if informative TDM data is provided",
    "section": "Objectives",
    "text": "Objectives\nTherefore, the objectives of our work were:\n\nto develop an MTX NLME model for an adult central nervous system (CNS) lymphoma population based on a training cohort, and\nto assess how well the model identifies delayed eliminators in a test cohort at different time points with varying amounts of data available for Bayesian forecasting."
  },
  {
    "objectID": "publications/presentations_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html#methods",
    "href": "publications/presentations_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html#methods",
    "title": "A Bayesian-NLME approach identifies patients at risk of delayed MTX elimination if informative TDM data is provided",
    "section": "Methods",
    "text": "Methods\nClinical routine data of adult CNS-lymphoma patients receiving HD-MTX i.v. infusion (median duration 3.25 h) was retrospectively collected and split by ID into a training (75%) and a test (25%) cohort. Cycles exceeding 72 h to MTX&lt;0.2 µM were classified as delayed. To avoid biased parameter estimates due to a longer follow-up in delayed eliminators, censored observations (&lt;0.2 µM) were added every 24 h up to 22 days since the last observation &lt;0.2 µM per patient and cycle. Parameters were estimated using SAEM and M3 method in NONMEM 7.5.1. Pre-specified covariate relations were tested on parameters with variability using SCM. Per cycle, time-to-threshold predictions were evaluated at pre-dose, 10 h, 30 h, and 50 h after start of infusion using data available up to each time point. Bayesian forecasting using mapbayr was conducted once MTX concentrations became available, including data from previous cycles. The predicted and observed times to threshold were compared for each time point within a cycle by calculating the mean absolute error (MAE) and the fraction of correctly identified delayed patients (true positive rate, TPR)."
  },
  {
    "objectID": "publications/presentations_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html#results",
    "href": "publications/presentations_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html#results",
    "title": "A Bayesian-NLME approach identifies patients at risk of delayed MTX elimination if informative TDM data is provided",
    "section": "Results",
    "text": "Results\nThe training data (nID: 132, nCycl: 410, nMTX: 2906) was best described by a 3 cmt model with first-order disposition processes and variability on CL (IIV: 19.0%, IOV: 14.4%) and Q3 (IIV: 29.6%, IOV: 27.3%). CL was positively associated with estimated glomerular filtration rate and serum albumin concentrations and negatively associated with C-reactive protein concentrations. Parameters were precisely estimated (SIR: 1.0%-26% RSE), with GOF and VPC plots demonstrating good agreement with the data.\nPredictivity results for the time to threshold in all cycles of the test cohort (nID: 44, nCycl: 137, nMTX: 985) were:\n\n\n\nTime Point\nMAE (h)\nTPR (%)\n\n\n\n\nPre-dose\n22.8\n30.6\n\n\n≤ 10 h\n22.2\n38.9\n\n\n≤ 30 h\n15.7\n61.1\n\n\n≤ 50 h\n11.9\n77.8\n\n\n\nAt pre-dose, the model showed poor identification of delayed eliminators (TPR) and low predictive accuracy (MAE). Including MTX concentrations at the end of infusion only slightly improved predictive performance, with a minor reduction in MAE (-0.6 h) and a small increase in TPR (+8.3% points). In contrast, incorporating additional data between 10 h and 30 h substantially improved predictive performance, increasing the TPR by 22.2% points and reducing MAE by 6.5 h compared to the 10 h time point. The 50 h time point showed the best predictive performance. Adding data from previous cycles improved TPR for early time points (pre-dose: 38% vs 10%; 10 h: 46% vs 20%), but worsened TPR for later time points (30 h: 58% vs 70%; 50 h: 77% vs 80%). A similar pattern was observed for MAE."
  },
  {
    "objectID": "publications/presentations_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html#conclusion",
    "href": "publications/presentations_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html#conclusion",
    "title": "A Bayesian-NLME approach identifies patients at risk of delayed MTX elimination if informative TDM data is provided",
    "section": "Conclusion",
    "text": "Conclusion\nDepending on the amount of data available for a given patient, the predictive performance of the NLME model ranged from poor to good. At early time points, the model showed substantial inaccuracy and insufficient identification of delayed eliminators. We conclude that TDM data beyond 10 h is essential for identifying at-risk patients. Next, we will extend the framework to a full Bayesian approach using Stan and Torsten and integrate it into an RShiny app."
  },
  {
    "objectID": "publications/abstracts_docs/09-02-2021_microdosed_yohimbine.html",
    "href": "publications/abstracts_docs/09-02-2021_microdosed_yohimbine.html",
    "title": "Using microdose-based activity measurement to individualise dosing of cytochrome P450 metabolised drugs: a case study with yohimbine and tamoxifen",
    "section": "",
    "text": "https://www.page-meeting.org/default.asp?abstract=9807"
  },
  {
    "objectID": "publications/abstracts_docs/09-02-2021_microdosed_yohimbine.html#url",
    "href": "publications/abstracts_docs/09-02-2021_microdosed_yohimbine.html#url",
    "title": "Using microdose-based activity measurement to individualise dosing of cytochrome P450 metabolised drugs: a case study with yohimbine and tamoxifen",
    "section": "",
    "text": "https://www.page-meeting.org/default.asp?abstract=9807"
  },
  {
    "objectID": "publications/abstracts_docs/09-02-2021_microdosed_yohimbine.html#abstract",
    "href": "publications/abstracts_docs/09-02-2021_microdosed_yohimbine.html#abstract",
    "title": "Using microdose-based activity measurement to individualise dosing of cytochrome P450 metabolised drugs: a case study with yohimbine and tamoxifen",
    "section": "Abstract",
    "text": "Abstract\n\nObjectives\nUnderstanding pharmacokinetic (PK) interindividual variability (IIV) can enable reaching optimal drug exposure, minimising therapeutic failure. Genotype-derived phenotypes are often applied to derive a patient’s individual clearance (iCL) but do not always translate into the optimal individual dose. Alternatively, the direct measurement of enzyme activity using a microdosed external probe could provide insights into the patient’s iCL and could be used in model-informed precision dosing (MIPD). Here, we propose the use of the CYP2D6 substrate yohimbine (YOH) as a probe to individualise the dosing of the selective oestrogen receptor modulator tamoxifen (TAM). While YOH’s PK was previously characterised using intensive blood sampling over 24 hours [1], deriving YOH iCL (iCLYOH), which varies 1000-fold between normal and poor metabolisers, with less samples would be ideal. This iCL can then be used in a Bayesian framework to predict the optimal dose of other CYP2D6 substrates. After standard dosing of 20 mg TAM once daily, high IIV in TAM concentrations and its ~100-fold more active metabolite endoxifen (ENDX) is observed and attributed to variability in CYP2D6 activity [2]. Therefore, treatment with TAM would benefit from MIPD and thus we present this as a case study of using microdose-based activity measurement to individualise dosing of CYP metabolised drugs.\n\n\nMaterial/Methods\nA recent study investigating oral YOH as a predictor for CYP2D6 activity was used for PK model development [1], including the CYP2D6 genotype-derived phenotype, using NONMEM v. 7.4. The best-fitting model was then refitted to the data blinded for the attributed CYP2D6 activity score (AS) to mimic the application where this data is not available. To use the final model for Bayesian inference in a clinical setting, optimal sampling time points between 0.25 and 4 h post-dose were determined using optimal design analysis in R/Rstudio (v.3.6.3/1.3.959) applying the popED package (v.0.5.0). This design was evaluated by stochastically simulating YOH concentrations at the optimal timepoints to estimate MAP CL. The agreement between the MAP estimate and the iCL of the simulation was then evaluated: bias and precision were assessed using median estimation errors and median absolute estimation errors. To incorporate the ‘real-world’ small deviations from planned sampling times, simulated sampling times were drawn from a normal distribution (sd=5 min) around the planned sample. For the MIPD application, a published parent-metabolite TAM PK model [2] was selected. Based on the empirical Bayes estimates and the CYP2D6 AS of the patients in the original YOH model development dataset, iCLYOH for the blinded model application dataset were converted to CYP2D6 AS’s, which were implemented as covariates in the TAM model. Then ENDX exposure of 1000 virtual patients with the same AS were stochastically simulated after 20, 40 and 60 mg daily doses. The percentages of virtual patients reaching the target ENDX minimum steady state concentration of 5.97 ng/mL were calculated [3]. For each patient, the lowest dose resulting in &gt;90% of target attainment was selected as optimal dose.\n\n\nResults\nA two-compartmental model with first-order absorption and linear elimination best described the YOH data. The IIV on YOH CL was largely explained by including CYP2D6 activity as a covariate, leading to a reduction from 1,143 to 43.9 CV%. Based on the optimal experimental design analysis, one early (0.25 h) and one late (4 h) sample were sufficient to reliably estimate iCL. iCLYOH were linked to the following phenotypes: patients with iCLYOH≤6 L/h: poor metabolisers, 6&lt;iCLYOH&lt;180 L/h: intermediate metabolisers and iCLYOH≥180 L/h: normal metabolisers. The PK model and iCL estimation were successfully linked to the TAM model in order to provide dosing recommendations for TAM treatment based on 2 YOH samples.\n\n\nConclusion\nThis study achieved TAM dose individualisation by using YOH derived CYP2D6 activity and MIPD. A clinical study where both TAM and YOH are administered could inform a direct link between iCLYOH and iCLTAM. This framework can be used for dose individualisation of other CYP substrates as long as a PK model and a probe is available; e.g. midazolam for CYP3A4 could be considered to expand this microdose-based activity measurement for individualised dosing and its utility should be investigated in prospective clinical trials. Furthermore, integration of the workflow in an easy-to-use tool would further encourage clinical application.\n\n\nReferences\n[1] M. Vay, M.J. Meyer, A. Blank, G. Skopp, P. Rose, M.V. Tzvetkov, G. Mikus. Oral Yohimbine as a New Probe Drug to Predict CYP2D6 Activity: Results of a Fixed-Sequence Phase I Trial. Clin. Pharmacokinet. 59: 927–939 (2020).\n[2] A. Mueller-Schoell, L. Klopp-Schulze, W. Schroth, T. Mürdter, R. Michelet, H. Brauch, W. Huisinga, M. Joerger, P. Neven, S.L.W. Koolen, R.H.J. Mathijssen, E. Copson, D. Eccles, S. Chen, B. Chowbay, A. Tfayli, N.K. Zgheib, M. Schwab, C. Kloft. Obesity Alters Endoxifen Plasma Levels in Young Breast Cancer Patients: A Pharmacometric Simulation Approach. Clin. Pharmacol. Ther. 108: 661–670 (2020).\n[3] L Madlensky, L Natarajan, S Tchu, M Pu, J Mortimer, S W Flatt, D M Nikoloff, G Hillman, M R Fontecha, H J Lawrence, B A Parker, A H B Wu, J P Pierce Tamoxifen metabolite concentrations, CYP2D6 genotype, and breast cancer outcomes. Clin Pharmacol Ther. 89: 718-25 (2011)"
  },
  {
    "objectID": "publications/abstracts_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html",
    "href": "publications/abstracts_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html",
    "title": "A Bayesian-NLME approach identifies patients at risk of delayed MTX elimination if informative TDM data is provided",
    "section": "",
    "text": "https://www.page-meeting.org/default.asp?abstract=11507"
  },
  {
    "objectID": "publications/abstracts_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html#url",
    "href": "publications/abstracts_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html#url",
    "title": "A Bayesian-NLME approach identifies patients at risk of delayed MTX elimination if informative TDM data is provided",
    "section": "",
    "text": "https://www.page-meeting.org/default.asp?abstract=11507"
  },
  {
    "objectID": "publications/abstracts_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html#abstract",
    "href": "publications/abstracts_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html#abstract",
    "title": "A Bayesian-NLME approach identifies patients at risk of delayed MTX elimination if informative TDM data is provided",
    "section": "Abstract",
    "text": "Abstract"
  },
  {
    "objectID": "publications/abstracts_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html#introduction",
    "href": "publications/abstracts_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html#introduction",
    "title": "A Bayesian-NLME approach identifies patients at risk of delayed MTX elimination if informative TDM data is provided",
    "section": "Introduction",
    "text": "Introduction\nPhysicians must constantly assess treatment-related risks based on limited data. This is particularly difficult in oncology, as compounds often have a narrow therapeutic window. For example, patients receiving high-dose methotrexate (HD-MTX) are at increased risk of toxicity if MTX elimination is delayed (MTX &gt; threshold of 0.2 µM for &gt;72 h post-dose). To timely identify these patients (‘delayed eliminators’), TDM is being performed until MTX concentrations drop below the threshold. However, early translation of MTX concentrations and clinical biomarkers into a risk score at the bedside can be challenging. A Bayesian-NLME framework allows to combine prior knowledge about the population with individual measurements. Evaluating its predictive performance in clinically relevant scenarios with unseen patients is crucial to determine the clinical utility of the obtained predictions. However, most published HD-MTX models lack such validation, leaving their clinical applicability uncertain."
  },
  {
    "objectID": "publications/abstracts_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html#objectives",
    "href": "publications/abstracts_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html#objectives",
    "title": "A Bayesian-NLME approach identifies patients at risk of delayed MTX elimination if informative TDM data is provided",
    "section": "Objectives",
    "text": "Objectives\nTherefore, the objectives of our work were:\n\nto develop an MTX NLME model for an adult central nervous system (CNS) lymphoma population based on a training cohort, and\nto assess how well the model identifies delayed eliminators in a test cohort at different time points with varying amounts of data available for Bayesian forecasting."
  },
  {
    "objectID": "publications/abstracts_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html#methods",
    "href": "publications/abstracts_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html#methods",
    "title": "A Bayesian-NLME approach identifies patients at risk of delayed MTX elimination if informative TDM data is provided",
    "section": "Methods",
    "text": "Methods\nClinical routine data of adult CNS-lymphoma patients receiving HD-MTX i.v. infusion (median duration 3.25 h) was retrospectively collected and split by ID into a training (75%) and a test (25%) cohort. Cycles exceeding 72 h to MTX&lt;0.2 µM were classified as delayed. To avoid biased parameter estimates due to a longer follow-up in delayed eliminators, censored observations (&lt;0.2 µM) were added every 24 h up to 22 days since the last observation &lt;0.2 µM per patient and cycle. Parameters were estimated using SAEM and M3 method in NONMEM 7.5.1. Pre-specified covariate relations were tested on parameters with variability using SCM. Per cycle, time-to-threshold predictions were evaluated at pre-dose, 10 h, 30 h, and 50 h after start of infusion using data available up to each time point. Bayesian forecasting using mapbayr was conducted once MTX concentrations became available, including data from previous cycles. The predicted and observed times to threshold were compared for each time point within a cycle by calculating the mean absolute error (MAE) and the fraction of correctly identified delayed patients (true positive rate, TPR)."
  },
  {
    "objectID": "publications/abstracts_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html#results",
    "href": "publications/abstracts_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html#results",
    "title": "A Bayesian-NLME approach identifies patients at risk of delayed MTX elimination if informative TDM data is provided",
    "section": "Results",
    "text": "Results\nThe training data (nID: 132, nCycl: 410, nMTX: 2906) was best described by a 3 cmt model with first-order disposition processes and variability on CL (IIV: 19.0%, IOV: 14.4%) and Q3 (IIV: 29.6%, IOV: 27.3%). CL was positively associated with estimated glomerular filtration rate and serum albumin concentrations and negatively associated with C-reactive protein concentrations. Parameters were precisely estimated (SIR: 1.0%-26% RSE), with GOF and VPC plots demonstrating good agreement with the data.\nPredictivity results for the time to threshold in all cycles of the test cohort (nID: 44, nCycl: 137, nMTX: 985) were:\n\n\n\nTime Point\nMAE (h)\nTPR (%)\n\n\n\n\nPre-dose\n22.8\n30.6\n\n\n≤ 10 h\n22.2\n38.9\n\n\n≤ 30 h\n15.7\n61.1\n\n\n≤ 50 h\n11.9\n77.8\n\n\n\nAt pre-dose, the model showed poor identification of delayed eliminators (TPR) and low predictive accuracy (MAE). Including MTX concentrations at the end of infusion only slightly improved predictive performance, with a minor reduction in MAE (-0.6 h) and a small increase in TPR (+8.3% points). In contrast, incorporating additional data between 10 h and 30 h substantially improved predictive performance, increasing the TPR by 22.2% points and reducing MAE by 6.5 h compared to the 10 h time point. The 50 h time point showed the best predictive performance. Adding data from previous cycles improved TPR for early time points (pre-dose: 38% vs 10%; 10 h: 46% vs 20%), but worsened TPR for later time points (30 h: 58% vs 70%; 50 h: 77% vs 80%). A similar pattern was observed for MAE."
  },
  {
    "objectID": "publications/abstracts_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html#conclusion",
    "href": "publications/abstracts_docs/05-16-2025_full_bayes_mtx_delayed_elimination.html#conclusion",
    "title": "A Bayesian-NLME approach identifies patients at risk of delayed MTX elimination if informative TDM data is provided",
    "section": "Conclusion",
    "text": "Conclusion\nDepending on the amount of data available for a given patient, the predictive performance of the NLME model ranged from poor to good. At early time points, the model showed substantial inaccuracy and insufficient identification of delayed eliminators. We conclude that TDM data beyond 10 h is essential for identifying at-risk patients. Next, we will extend the framework to a full Bayesian approach using Stan and Torsten and integrate it into an RShiny app."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html",
    "href": "posts/understanding_nlme_estimation/index.html",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "",
    "text": "Code\n# get rid of workspace\nrm(list = ls())\n\n# load packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(kableExtra)\nlibrary(xpose4)\nlibrary(tidyr)\nlibrary(plotly)\n\n# define base path\nbase_path &lt;- paste0(here::here(), \"/posts/understanding_nlme_estimation/\")\n\n# define function for html table output\nmytbl &lt;- function(tbl){\n  tbl |&gt; \n    kable() |&gt; \n    kable_styling()\n}"
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-prol-mot",
    "href": "posts/understanding_nlme_estimation/index.html#sec-prol-mot",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "1.1 Motivation",
    "text": "1.1 Motivation\nIn this (somewhat lengthy) document I want to share my attempt at understanding and reproducing NONMEM’s objective function in R. Of course you can use NONMEM effectively without knowing the exact calculations behind the objective function and I did so myself for quite a while. But I believe that it’s helpful to have some understanding of what’s happening under the hood, even if it’s just to some extent. Calculating the objective function manually and understanding the math behind the estimation has always been on my bucket list, but I never really knew where to start. After getting some very helpful input during the PharMetrX (2024) A5 module, I felt ready to give it at least a try. So, here we are!"
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-prol-struc",
    "href": "posts/understanding_nlme_estimation/index.html#sec-prol-struc",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "1.2 Structure",
    "text": "1.2 Structure\nLet me briefly outline how I structured this document. The main goal is to manually calculate the objective function in R using straight line equations. Furthermore, I would like to visualize its 3D surface to see the path the estimation algorithm is taking. I also want to reproduce two key steps associated with the estimation: The COVARIANCE step to assess the parameter precision and the POSTHOC step to get individual parameter estimates. As we try to work through these calculations, I also aim to explore and explain to myself some of the theory and intuition behind concepts and calculations along the way.\nTo achieve this, we will first define a simple one-compartment pharmacokinetic model with intravenous bolus administration and first-order elimination (Section 2). Afterwards we will use this model to simulate some virtual concentration-time data (Section 3), which we will then fit using the Laplacian estimation (Section 4) to obtain a reference solution. I chose the Laplacian algorithm because it makes fewer assumptions and simplifications compared to FOCE or FO. It should be easier to go from Laplacian to FOCE-I than vice versa.\nThen, in the biggest part of this document, we will try to construct the equations needed to calculate the objective function value for a given set of parameters and understand why we are taking each step (Section 5). After this is done, we will implement the functions in R (Section 6), reproduce each iteration of the NONMEM run and compare the results to the reference solution.\nFinally, the reward for all the hard work: We will visualize the objective function surface in a 3D plot (Section 7). This should give us a better understanding of the search algorithm and the behavior of the objective function in dependence of the parameter estimates.\nAfter that, we will attempt a complete estimation directly in R using the optim() function (Section 8) instead of only reproducing the objective function based on the iterations NONMEM took. We will compare our R-based parameter estimates against those obtained by NONMEM and discuss any differences.\nFollowing this, we will mimick the COVARIANCE step by retrieving and inverting the Hessian matrix (Section 9) obtained during the R-based optimization to assess parameter precision (relative standard errors). In a last step, we will then calculate the individual ETAs by reproducing the POSTHOC step (Section 10) and comparing the results against NONMEM’s outputs."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-prol-acknow",
    "href": "posts/understanding_nlme_estimation/index.html#sec-prol-acknow",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "1.3 Acknowledgments",
    "text": "1.3 Acknowledgments\nA special thanks to Dr. Niklas Hartung for his input during the PharMetrX (2024) A5 module and his assistance while writing this blog post, Prof. Wilhelm Huisinga for his contributions during the module, and Dr. Christin Nyhoegen for the helpful discussions."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-prol-discl",
    "href": "posts/understanding_nlme_estimation/index.html#sec-prol-discl",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "1.4 Disclaimer",
    "text": "1.4 Disclaimer\nBefore we get started, I want to note a few disclaimers to ward off any imposter syndrome that might kick in. I’m just a PhD student, not formally trained in mathematics or statistics, and I’m learning as I go. If you’re expecting an entirely error-free derivation with coherent statistical notation, this is not be the best resource. You better go with Wang (2007) in that case. My focus here is more about the intuition and maybe about developing a general understanding of the underlying processes.\nMuch of the content in this document is based on the work of others, and there’s not a lot of original thought here. I’ve relied heavily on several key publications (e.g., Wang (2007)), gained a lot of intuition from the PharMetrX (2024) A5 module, had important input from colleagues (see above) and used tools like Wolfram/Mathematica for some calculations and ChatGPT for parts of the writing. With that being said, let’s start!"
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-moddef-code",
    "href": "posts/understanding_nlme_estimation/index.html#sec-moddef-code",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "2.1 Model code",
    "text": "2.1 Model code\nThe NONMEM model code for our simple one-compartment PK model is shown below. Please note that we are using the ADVAN1 routine, which relies on the analytical expression for a one-compartment model, rather than an ODE solver.\n\n\n1cmt_iv_sim.mod\n\n# read_file(paste0(base_path, \"/models/simulation/1cmt_iv_sim.mod\"))\n\n$PROBLEM 1cmt_iv_sim\n\n$INPUT ID TIME EVID AMT RATE DV MDV\n\n$DATA C:\\Users\\mklose\\Desktop\\G\\Mitarbeiter\\Klose\\Miscellaneous\\NLME_reproduction_R\\data\\input_for_sim\\input_data.csv IGNORE=@\n\n$SUBROUTINES ADVAN1 TRANS2\n\n$PK\n; define fixed effects parameters\nCL = THETA(1) * EXP(ETA(1))\nV = THETA(2)\n\n; scaling\nS1=V\n\n$THETA\n(0, 0.2, 1)             ; 1 TVCL\n3.15 FIX                  ; 2 TVV\n\n$OMEGA \n0.2                     ; 1 OM_CL\n\n$SIGMA\n0.1 FIX                 ; 1 SIG_ADD\n\n$ERROR \n; add additive error\nY = F + EPS(1)\n\n; store error for table output\nERR1 = EPS(1)\n\n; $ESTIMATION METHOD=COND LAPLACIAN MAXEVAL=9999 SIGDIG=3 PRINT=1 NOABORT POSTHOC\n$SIMULATION (12345678) ONLYSIM\n\n$TABLE ID TIME EVID AMT RATE DV MDV ETA1 ERR1 NOAPPEND ONEHEADER NOPRINT FILE=sim_out\n\nDepending on the task (simulation vs. estimation), we’ll adjust the model code slightly. For simulation, we use $SIMULATION (12345678) ONLYSIM and for estimation, we are going to use $ESTIMATION METHOD=COND LAPLACIAN MAXEVAL=9999 SIGDIG=3 PRINT=1 NOABORT POSTHOC. Additionally, the $DATA block will vary depending on the task. For simulation, we will just pass a dosing and sampling scheme. For estimation, we’ll then use the simulated data from the first step and use the simulated concentration values for parameter estimation."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-sim-input",
    "href": "posts/understanding_nlme_estimation/index.html#sec-sim-input",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "3.1 Input dataset generation",
    "text": "3.1 Input dataset generation\nTo simulate data, we consider a scenario with 10 individuals, each having five observations at different time points within 24 hours. The selected time points are 0.01, 3, 6, 12, and 24 hours. To my understanding, it is crucial to have at least two observations per individual to reliably estimate inter-individual variability, as having only one observation per individual would make it impossible to distinguish between inter-individual variability and residual variability.\nThe dataset includes dosing records (EVID = 1) and observation records (EVID = 0). For now the dependent variable (DV) will be flagged to -99 since we will simulate these values in the following steps. Here is how our input dataset looks like:\n\n\nCode\n# generate NONMEM input dataset for n individuals with 3 observations at different timepoints\nn_ind &lt;- 10\nn_obs &lt;- 5\ntimepoints &lt;- c(0.01, 3, 6, 12, 24)\n\n# observation events (EVID == 0)\nobs_input &lt;- tibble(\n  ID = rep(1:n_ind, each = n_obs),\n  TIME = rep(timepoints, n_ind),\n  EVID = 0,\n  AMT = 0,\n  RATE = 0,\n  DV = -99, # DV is what we want to simulate\n  MDV = 0\n)\n\n# dosing events (EVID == 1)\ndosing_input &lt;- tibble(\n  ID = rep(1:n_ind, each = 1),\n  TIME = 0,\n  EVID = 1,\n  AMT = 100,\n  RATE = 0,\n  DV = 0,\n  MDV = 1\n)\n\n# bind together and sort by ID and TIME\ninput_data &lt;- bind_rows(obs_input, dosing_input) |&gt; \n  arrange(ID, TIME)\n\n# show input data\ninput_data |&gt; \n  head(n=10) |&gt; \n  mytbl()\n\n\n\n\n\nID\nTIME\nEVID\nAMT\nRATE\nDV\nMDV\n\n\n\n\n1\n0.00\n1\n100\n0\n0\n1\n\n\n1\n0.01\n0\n0\n0\n-99\n0\n\n\n1\n3.00\n0\n0\n0\n-99\n0\n\n\n1\n6.00\n0\n0\n0\n-99\n0\n\n\n1\n12.00\n0\n0\n0\n-99\n0\n\n\n1\n24.00\n0\n0\n0\n-99\n0\n\n\n2\n0.00\n1\n100\n0\n0\n1\n\n\n2\n0.01\n0\n0\n0\n-99\n0\n\n\n2\n3.00\n0\n0\n0\n-99\n0\n\n\n2\n6.00\n0\n0\n0\n-99\n0\n\n\n\n\n\nCode\n# save data to file\nwrite_csv(input_data, paste0(base_path, \"data/input_for_sim/input_data.csv\"))\n# write_csv(input_data, \"C:\\\\Users\\\\mklose\\\\Desktop\\\\G\\\\Mitarbeiter\\\\Klose\\\\Miscellaneous\\\\NLME_reproduction_R\\\\data\\\\input_for_sim\\\\input_data.csv\")\n\n\nWe arbitrarily decided that each individual receives the same dose of 100 mg at time 0. Same dose fits all, right? We can see that at 0.01, 3, 6, 12, and 24 hours after dosing we encoded sampling events (EVID = 0). We can now plug this dataset into NONMEM and simulate these concentrations!"
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-sim-sim",
    "href": "posts/understanding_nlme_estimation/index.html#sec-sim-sim",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "3.2 Simulation",
    "text": "3.2 Simulation\nWith the input dataset generated, the next step is to use it to simulate virtual concentration-time data. This simulation is performed using NONMEM, using the dosing and sampling dataset defined above. This step happens in NONMEM itself and will be be executed separately from this R session.\n\n3.2.1 Read in simulated data\nAfter running the simulation in NONMEM, the generated output (in our case a file called sim_out) contains the simulated concentration values within the DV column. The simulated dataset also includes additional columns such as ETA1, which represents the realization of inter-individual variability, and ERR1, which represents the realization of residual unexplained variability (RUV). So for each individual we have drawn one ETA1 and for each observation we have one ERR1.\n\n\nCode\n# load simulated data\nsim_data &lt;- read_nm_table(paste0(base_path, \"models/simulation/sim_out\"))\n\n# show simulated data\nsim_data |&gt; \n  head(n=10) |&gt; \n  mytbl()\n\n\n\n\n\nID\nTIME\nEVID\nAMT\nRATE\nDV\nMDV\nETA1\nERR1\n\n\n\n\n1\n0.00\n1\n100\n0\n0.0000\n1\n0.39117\n0.136050\n\n\n1\n0.01\n0\n0\n0\n30.8160\n0\n0.39117\n-0.900050\n\n\n1\n3.00\n0\n0\n0\n24.5520\n0\n0.39117\n0.598410\n\n\n1\n6.00\n0\n0\n0\n17.9250\n0\n0.39117\n-0.148090\n\n\n1\n12.00\n0\n0\n0\n10.1100\n0\n0.39117\n-0.180050\n\n\n1\n24.00\n0\n0\n0\n3.5975\n0\n0.39117\n0.262420\n\n\n2\n0.00\n1\n100\n0\n0.0000\n1\n0.12054\n-0.037607\n\n\n2\n0.01\n0\n0\n0\n31.4550\n0\n0.12054\n-0.268190\n\n\n2\n3.00\n0\n0\n0\n26.2030\n0\n0.12054\n0.595370\n\n\n2\n6.00\n0\n0\n0\n20.5520\n0\n0.12054\n-0.103830\n\n\n\n\n\nWe can see that we obtained a dataset with the simulated concentration values in the DV column. The simulated data now needs to be saved as .csv in order to use it in the subsequent steps within NONMEM.\n\n\nCode\n# subset dataframe\nsim_data_reduced &lt;- sim_data |&gt; \n  select(ID, TIME, EVID, AMT, RATE, DV, MDV)\n\n# save dataframe\nwrite_csv(sim_data_reduced, paste0(base_path, \"data/output_from_sim/sim_data.csv\"))\n\n# filter to have EVID == 0 only\nsim_data_reduced &lt;- sim_data_reduced |&gt; \n  filter(EVID == 0) \n\n\nGreat! The sim_data.csv dataframe was successfully saved and we can later use it for the generation of the reference solution in NONMEM."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-sim-vis",
    "href": "posts/understanding_nlme_estimation/index.html#sec-sim-vis",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "3.3 Concentration-time profiles",
    "text": "3.3 Concentration-time profiles\nNow we can visualize the simulated data stratified by individual to get a feeling for our virtual clinical data.\n\n\nCode\n# show individual profiles\nsim_data |&gt; \n  filter(EVID == 0) |&gt; \n  ggplot(aes(x=TIME, y=DV, group=ID, color=as.factor(ID))) +\n  geom_point()+\n  geom_line()+\n  theme_bw()+\n  scale_y_continuous(limits=c(0,NA))+\n  labs(x=\"Time after last dose [h]\", y=\"Concentration [mg/L]\")+\n  ggtitle(\"Simulated data\")+\n  scale_color_discrete(\"Individual\")\n\n\n\n\n\n\n\n\nFigure 2: Simulated concentration-time profiles for 10 individuals.\n\n\n\n\n\nAs each of the 10 individuals received the same dose, we can clearly see the impact of variability on clearance on concentration-time profiles. Additionally, the single data points are influenced by the residual unexplained variability, which adds noise to the readouts.\nWe are making progress! In a next step we now want to generate a reference solution for the estimation within NONMEM. By doing so we can (in the end) compare our own implementation of the objective function to the one NONMEM uses."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-nm-est-est",
    "href": "posts/understanding_nlme_estimation/index.html#sec-nm-est-est",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "4.1 Estimation",
    "text": "4.1 Estimation\nAs mentioned before, the actual estimation will again happen in NONMEM and therefore needs to be executed separately from this R session. In this step, we are just going to read in the NONMEM output files and visualize them appropriately. This is going to happen as a next step."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-nm-est-read",
    "href": "posts/understanding_nlme_estimation/index.html#sec-nm-est-read",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "4.2 NONMEM output files",
    "text": "4.2 NONMEM output files\n\n4.2.1 .lst file\nFirst of all, we can read in the .lst file. It contains many information about the parameter estimation process and is a nice way to get a quick overview of the model run.\n\n\n\n\n\n\nTip.lst file\n\n\n\n\n\n\n\nCode\nread_file(paste0(base_path, \"models/estimation/1cmt_iv_est.lst\"))\n\n\n\n\n1cmt_iv_sim.lst\n\nMon 12/30/2024 \n10:54 PM\n$PROBLEM    1cmt_iv_est\n$INPUT      ID TIME EVID AMT RATE DV MDV\n$DATA      sim_data.csv IGNORE=@\n$SUBROUTINE ADVAN1 TRANS2\n$PK\n; define fixed effects parameters\nCL = THETA(1) * EXP(ETA(1))\nV = THETA(2)\n\n; scaling\nS1=V\n\n$THETA  (0,0.1,1) ; 1 TVCL\n 3.15 FIX ; 2 TVV\n$OMEGA  0.15  ;    1 OM_CL\n$SIGMA  0.1  FIX  ;  1 SIG_ADD\n$ERROR \n; add additive error\nY = F + EPS(1)\n\n; store error for table output\nERR1 = EPS(1)\n\n$ESTIMATION METHOD=COND LAPLACIAN MAXEVAL=9999 SIGDIG=3 PRINT=1\n            NOABORT POSTHOC\n$COVARIANCE PRINT=E\n$TABLE      ID TIME EVID AMT RATE DV MDV ETA1 ERR1 CL NOAPPEND\n            ONEHEADER NOPRINT FILE=estim_out\n\n  \nNM-TRAN MESSAGES \n  \n WARNINGS AND ERRORS (IF ANY) FOR PROBLEM    1\n             \n (WARNING  2) NM-TRAN INFERS THAT THE DATA ARE POPULATION.\n  \nLicense Registered to: Freie Universitaet Berlin Department of Clinical Pharmacy Biochemistry\nExpiration Date:    14 JAN 2025\nCurrent Date:       30 DEC 2024\n  **** WARNING!!! Days until program expires :  19 ****\n  **** CONTACT idssoftware@iconplc.com FOR RENEWAL ****\n1NONLINEAR MIXED EFFECTS MODEL PROGRAM (NONMEM) VERSION 7.5.1\n ORIGINALLY DEVELOPED BY STUART BEAL, LEWIS SHEINER, AND ALISON BOECKMANN\n CURRENT DEVELOPERS ARE ROBERT BAUER, ICON DEVELOPMENT SOLUTIONS,\n AND ALISON BOECKMANN. IMPLEMENTATION, EFFICIENCY, AND STANDARDIZATION\n PERFORMED BY NOUS INFOSYSTEMS.\n\n PROBLEM NO.:         1\n 1cmt_iv_est\n0DATA CHECKOUT RUN:              NO\n DATA SET LOCATED ON UNIT NO.:    2\n THIS UNIT TO BE REWOUND:        NO\n NO. OF DATA RECS IN DATA SET:       60\n NO. OF DATA ITEMS IN DATA SET:   7\n ID DATA ITEM IS DATA ITEM NO.:   1\n DEP VARIABLE IS DATA ITEM NO.:   6\n MDV DATA ITEM IS DATA ITEM NO.:  7\n0INDICES PASSED TO SUBROUTINE PRED:\n   3   2   4   5   0   0   0   0   0   0   0\n0LABELS FOR DATA ITEMS:\n ID TIME EVID AMT RATE DV MDV\n0(NONBLANK) LABELS FOR PRED-DEFINED ITEMS:\n CL ERR1\n0FORMAT FOR DATA:\n (7E8.0)\n\n TOT. NO. OF OBS RECS:       50\n TOT. NO. OF INDIVIDUALS:       10\n0LENGTH OF THETA:   2\n0DEFAULT THETA BOUNDARY TEST OMITTED:    NO\n0OMEGA HAS SIMPLE DIAGONAL FORM WITH DIMENSION:   1\n0DEFAULT OMEGA BOUNDARY TEST OMITTED:    NO\n0SIGMA HAS SIMPLE DIAGONAL FORM WITH DIMENSION:   1\n0DEFAULT SIGMA BOUNDARY TEST OMITTED:    NO\n0INITIAL ESTIMATE OF THETA:\n LOWER BOUND    INITIAL EST    UPPER BOUND\n  0.0000E+00     0.1000E+00     0.1000E+01\n  0.3150E+01     0.3150E+01     0.3150E+01\n0INITIAL ESTIMATE OF OMEGA:\n 0.1500E+00\n0INITIAL ESTIMATE OF SIGMA:\n 0.1000E+00\n0SIGMA CONSTRAINED TO BE THIS INITIAL ESTIMATE\n0COVARIANCE STEP OMITTED:        NO\n EIGENVLS. PRINTED:             YES\n SPECIAL COMPUTATION:            NO\n COMPRESSED FORMAT:              NO\n GRADIENT METHOD USED:     NOSLOW\n SIGDIGITS ETAHAT (SIGLO):                  -1\n SIGDIGITS GRADIENTS (SIGL):                -1\n EXCLUDE COV FOR FOCE (NOFCOV):              NO\n Cholesky Transposition of R Matrix (CHOLROFF):0\n KNUTHSUMOFF:                                -1\n RESUME COV ANALYSIS (RESUME):               NO\n SIR SAMPLE SIZE (SIRSAMPLE):\n NON-LINEARLY TRANSFORM THETAS DURING COV (THBND): 1\n PRECONDTIONING CYCLES (PRECOND):        0\n PRECONDTIONING TYPES (PRECONDS):        TOS\n FORCED PRECONDTIONING CYCLES (PFCOND):0\n PRECONDTIONING TYPE (PRETYPE):        0\n FORCED POS. DEFINITE SETTING DURING PRECONDITIONING: (FPOSDEF):0\n SIMPLE POS. DEFINITE SETTING: (POSDEF):-1\n0TABLES STEP OMITTED:    NO\n NO. OF TABLES:           1\n SEED NUMBER (SEED):    11456\n NPDTYPE:    0\n INTERPTYPE:    0\n RANMETHOD:             3U\n MC SAMPLES (ESAMPLE):    300\n WRES SQUARE ROOT TYPE (WRESCHOL): EIGENVALUE\n0-- TABLE   1 --\n0RECORDS ONLY:    ALL\n04 COLUMNS APPENDED:    NO\n PRINTED:                NO\n HEADER:                YES\n FILE TO BE FORWARDED:   NO\n FORMAT:                S1PE11.4\n IDFORMAT:\n LFORMAT:\n RFORMAT:\n FIXED_EFFECT_ETAS:\n0USER-CHOSEN ITEMS:\n ID TIME EVID AMT RATE DV MDV ETA1 ERR1 CL\n1DOUBLE PRECISION PREDPP VERSION 7.5.1\n\n ONE COMPARTMENT MODEL (ADVAN1)\n0MAXIMUM NO. OF BASIC PK PARAMETERS:   2\n0BASIC PK PARAMETERS (AFTER TRANSLATION):\n   ELIMINATION RATE (K) IS BASIC PK PARAMETER NO.:  1\n\n TRANSLATOR WILL CONVERT PARAMETERS\n CLEARANCE (CL) AND VOLUME (V) TO K (TRANS2)\n0COMPARTMENT ATTRIBUTES\n COMPT. NO.   FUNCTION   INITIAL    ON/OFF      DOSE      DEFAULT    DEFAULT\n                         STATUS     ALLOWED    ALLOWED    FOR DOSE   FOR OBS.\n    1         CENTRAL      ON         NO         YES        YES        YES\n    2         OUTPUT       OFF        YES        NO         NO         NO\n1\n ADDITIONAL PK PARAMETERS - ASSIGNMENT OF ROWS IN GG\n COMPT. NO.                             INDICES\n              SCALE      BIOAVAIL.   ZERO-ORDER  ZERO-ORDER  ABSORB\n                         FRACTION    RATE        DURATION    LAG\n    1            3           *           *           *           *\n    2            *           -           -           -           -\n             - PARAMETER IS NOT ALLOWED FOR THIS MODEL\n             * PARAMETER IS NOT SUPPLIED BY PK SUBROUTINE;\n               WILL DEFAULT TO ONE IF APPLICABLE\n0DATA ITEM INDICES USED BY PRED ARE:\n   EVENT ID DATA ITEM IS DATA ITEM NO.:      3\n   TIME DATA ITEM IS DATA ITEM NO.:          2\n   DOSE AMOUNT DATA ITEM IS DATA ITEM NO.:   4\n   DOSE RATE DATA ITEM IS DATA ITEM NO.:     5\n\n0PK SUBROUTINE CALLED WITH EVERY EVENT RECORD.\n PK SUBROUTINE NOT CALLED AT NONEVENT (ADDITIONAL OR LAGGED) DOSE TIMES.\n0ERROR SUBROUTINE CALLED WITH EVERY EVENT RECORD.\n1\n\n\n #TBLN:      1\n #METH: Laplacian Conditional Estimation\n\n ESTIMATION STEP OMITTED:                 NO\n ANALYSIS TYPE:                           POPULATION\n NUMBER OF SADDLE POINT RESET ITERATIONS:      0\n GRADIENT METHOD USED:               NOSLOW\n CONDITIONAL ESTIMATES USED:              YES\n CENTERED ETA:                            NO\n EPS-ETA INTERACTION:                     NO\n LAPLACIAN OBJ. FUNC.:                    YES\n NUMERICAL 2ND DERIVATIVES:               NO\n NO. OF FUNCT. EVALS. ALLOWED:            9999\n NO. OF SIG. FIGURES REQUIRED:            3\n INTERMEDIATE PRINTOUT:                   YES\n ESTIMATE OUTPUT TO MSF:                  NO\n ABORT WITH PRED EXIT CODE 1:             NO\n IND. OBJ. FUNC. VALUES SORTED:           NO\n NUMERICAL DERIVATIVE\n       FILE REQUEST (NUMDER):               NONE\n MAP (ETAHAT) ESTIMATION METHOD (OPTMAP):   0\n ETA HESSIAN EVALUATION METHOD (ETADER):    0\n INITIAL ETA FOR MAP ESTIMATION (MCETA):    0\n SIGDIGITS FOR MAP ESTIMATION (SIGLO):      100\n GRADIENT SIGDIGITS OF\n       FIXED EFFECTS PARAMETERS (SIGL):     100\n NOPRIOR SETTING (NOPRIOR):                 0\n NOCOV SETTING (NOCOV):                     OFF\n DERCONT SETTING (DERCONT):                 OFF\n FINAL ETA RE-EVALUATION (FNLETA):          1\n EXCLUDE NON-INFLUENTIAL (NON-INFL.) ETAS\n       IN SHRINKAGE (ETASTYPE):             NO\n NON-INFL. ETA CORRECTION (NONINFETA):      0\n RAW OUTPUT FILE (FILE): psn.ext\n EXCLUDE TITLE (NOTITLE):                   NO\n EXCLUDE COLUMN LABELS (NOLABEL):           NO\n FORMAT FOR ADDITIONAL FILES (FORMAT):      S1PE12.5\n PARAMETER ORDER FOR OUTPUTS (ORDER):       TSOL\n KNUTHSUMOFF:                               0\n INCLUDE LNTWOPI:                           NO\n INCLUDE CONSTANT TERM TO PRIOR (PRIORC):   NO\n INCLUDE CONSTANT TERM TO OMEGA (ETA) (OLNTWOPI):NO\n ADDITIONAL CONVERGENCE TEST (CTYPE=4)?:    NO\n EM OR BAYESIAN METHOD USED:                 NONE\n\n\n THE FOLLOWING LABELS ARE EQUIVALENT\n PRED=NPRED\n RES=NRES\n WRES=NWRES\n IWRS=NIWRES\n IPRD=NIPRED\n IRS=NIRES\n\n MONITORING OF SEARCH:\n\n\n0ITERATION NO.:    0    OBJECTIVE VALUE:   41.8454368139310        NO. OF FUNC. EVALS.:   4\n CUMULATIVE NO. OF FUNC. EVALS.:        4\n NPARAMETR:  1.0000E-01  1.5000E-01\n PARAMETER:  1.0000E-01  1.0000E-01\n GRADIENT:  -1.0545E+02 -1.0294E+02\n\n0ITERATION NO.:    1    OBJECTIVE VALUE:  -2.74307593433339        NO. OF FUNC. EVALS.:   5\n CUMULATIVE NO. OF FUNC. EVALS.:        9\n NPARAMETR:  1.6837E-01  4.8397E-01\n PARAMETER:  7.0000E-01  6.8569E-01\n GRADIENT:   2.8868E+00  9.4212E+00\n\n0ITERATION NO.:    2    OBJECTIVE VALUE:  -3.06819849357734        NO. OF FUNC. EVALS.:   8\n CUMULATIVE NO. OF FUNC. EVALS.:       17\n NPARAMETR:  1.6369E-01  3.8814E-01\n PARAMETER:  6.6620E-01  5.7537E-01\n GRADIENT:  -2.2554E+00  5.6641E+00\n\n0ITERATION NO.:    3    OBJECTIVE VALUE:  -11.2971148236422        NO. OF FUNC. EVALS.:   5\n CUMULATIVE NO. OF FUNC. EVALS.:       22\n NPARAMETR:  2.1655E-01  1.4655E-01\n PARAMETER:  1.0114E+00  8.8367E-02\n GRADIENT:   6.6259E+00  2.6609E+00\n\n0ITERATION NO.:    4    OBJECTIVE VALUE:  -11.2971148236422        NO. OF FUNC. EVALS.:  10\n CUMULATIVE NO. OF FUNC. EVALS.:       32\n NPARAMETR:  2.1655E-01  1.4655E-01\n PARAMETER:  1.0114E+00  8.8367E-02\n GRADIENT:  -1.3899E+01  2.6592E+00\n\n0ITERATION NO.:    5    OBJECTIVE VALUE:  -12.4187275503686        NO. OF FUNC. EVALS.:   7\n CUMULATIVE NO. OF FUNC. EVALS.:       39\n NPARAMETR:  2.5349E-01  8.6243E-02\n PARAMETER:  1.2172E+00 -1.7673E-01\n GRADIENT:   4.7000E+00 -5.6454E+00\n\n0ITERATION NO.:    6    OBJECTIVE VALUE:  -12.7891181563899        NO. OF FUNC. EVALS.:   7\n CUMULATIVE NO. OF FUNC. EVALS.:       46\n NPARAMETR:  2.4329E-01  1.1715E-01\n PARAMETER:  1.1625E+00 -2.3573E-02\n GRADIENT:  -1.7854E+00  1.1790E+00\n\n0ITERATION NO.:    7    OBJECTIVE VALUE:  -12.8236151522707        NO. OF FUNC. EVALS.:   7\n CUMULATIVE NO. OF FUNC. EVALS.:       53\n NPARAMETR:  2.4614E-01  1.1120E-01\n PARAMETER:  1.1779E+00 -4.9675E-02\n GRADIENT:  -2.9776E-01  2.0940E-01\n\n0ITERATION NO.:    8    OBJECTIVE VALUE:  -12.8245888793110        NO. OF FUNC. EVALS.:   7\n CUMULATIVE NO. OF FUNC. EVALS.:       60\n NPARAMETR:  2.4672E-01  1.0995E-01\n PARAMETER:  1.1810E+00 -5.5291E-02\n GRADIENT:   1.8561E-02 -1.2024E-02\n\n0ITERATION NO.:    9    OBJECTIVE VALUE:  -12.8245933812655        NO. OF FUNC. EVALS.:   7\n CUMULATIVE NO. OF FUNC. EVALS.:       67\n NPARAMETR:  2.4668E-01  1.1002E-01\n PARAMETER:  1.1808E+00 -5.4985E-02\n GRADIENT:  -1.3169E-04  1.3509E-04\n\n0ITERATION NO.:   10    OBJECTIVE VALUE:  -12.8245933812655        NO. OF FUNC. EVALS.:   4\n CUMULATIVE NO. OF FUNC. EVALS.:       71\n NPARAMETR:  2.4668E-01  1.1002E-01\n PARAMETER:  1.1808E+00 -5.4985E-02\n GRADIENT:  -1.3169E-04  1.3509E-04\n\n #TERM:\n0MINIMIZATION SUCCESSFUL\n NO. OF FUNCTION EVALUATIONS USED:       71\n NO. OF SIG. DIGITS IN FINAL EST.:  4.5\n\n ETABAR IS THE ARITHMETIC MEAN OF THE ETA-ESTIMATES,\n AND THE P-VALUE IS GIVEN FOR THE NULL HYPOTHESIS THAT THE TRUE MEAN IS 0.\n\n ETABAR:        -1.8954E-05\n SE:             1.0473E-01\n N:                      10\n\n P VAL.:         9.9986E-01\n\n ETASHRINKSD(%)  1.5107E-01\n ETASHRINKVR(%)  3.0192E-01\n EBVSHRINKSD(%)  1.3728E-01\n EBVSHRINKVR(%)  2.7437E-01\n RELATIVEINF(%)  9.9726E+01\n EPSSHRINKSD(%)  1.8336E+01\n EPSSHRINKVR(%)  3.3311E+01\n\n  \n TOTAL DATA POINTS NORMALLY DISTRIBUTED (N):           50\n N*LOG(2PI) CONSTANT TO OBJECTIVE FUNCTION:    91.893853320467272     \n OBJECTIVE FUNCTION VALUE WITHOUT CONSTANT:   -12.824593381265490     \n OBJECTIVE FUNCTION VALUE WITH CONSTANT:       79.069259939201785     \n REPORTED OBJECTIVE FUNCTION DOES NOT CONTAIN CONSTANT\n  \n TOTAL EFFECTIVE ETAS (NIND*NETA):                            10\n  \n #TERE:\n Elapsed estimation  time in seconds:     0.02\n Elapsed covariance  time in seconds:     0.00\n Elapsed postprocess time in seconds:     0.00\n1\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n ************************************************************************************************************************\n ********************                                                                                ********************\n ********************                         LAPLACIAN CONDITIONAL ESTIMATION                       ********************\n #OBJT:**************                       MINIMUM VALUE OF OBJECTIVE FUNCTION                      ********************\n ********************                                                                                ********************\n ************************************************************************************************************************\n \n\n\n\n\n\n #OBJV:********************************************      -12.825       **************************************************\n1\n ************************************************************************************************************************\n ********************                                                                                ********************\n ********************                         LAPLACIAN CONDITIONAL ESTIMATION                       ********************\n ********************                             FINAL PARAMETER ESTIMATE                           ********************\n ********************                                                                                ********************\n ************************************************************************************************************************\n \n\n\n THETA - VECTOR OF FIXED EFFECTS PARAMETERS   *********\n\n\n         TH 1      TH 2     \n \n         2.47E-01  3.15E+00\n \n\n\n OMEGA - COV MATRIX FOR RANDOM EFFECTS - ETAS  ********\n\n\n         ETA1     \n \n ETA1\n+        1.10E-01\n \n\n\n SIGMA - COV MATRIX FOR RANDOM EFFECTS - EPSILONS  ****\n\n\n         EPS1     \n \n EPS1\n+        1.00E-01\n \n1\n\n\n OMEGA - CORR MATRIX FOR RANDOM EFFECTS - ETAS  *******\n\n\n         ETA1     \n \n ETA1\n+        3.32E-01\n \n\n\n SIGMA - CORR MATRIX FOR RANDOM EFFECTS - EPSILONS  ***\n\n\n         EPS1     \n \n EPS1\n+        3.16E-01\n \n1\n ************************************************************************************************************************\n ********************                                                                                ********************\n ********************                         LAPLACIAN CONDITIONAL ESTIMATION                       ********************\n ********************                            STANDARD ERROR OF ESTIMATE                          ********************\n ********************                                                                                ********************\n ************************************************************************************************************************\n \n\n\n THETA - VECTOR OF FIXED EFFECTS PARAMETERS   *********\n\n\n         TH 1      TH 2     \n \n         2.59E-02 .........\n \n\n\n OMEGA - COV MATRIX FOR RANDOM EFFECTS - ETAS  ********\n\n\n         ETA1     \n \n ETA1\n+        4.87E-02\n \n\n\n SIGMA - COV MATRIX FOR RANDOM EFFECTS - EPSILONS  ****\n\n\n         EPS1     \n \n EPS1\n+       .........\n \n1\n\n\n OMEGA - CORR MATRIX FOR RANDOM EFFECTS - ETAS  *******\n\n\n         ETA1     \n \n ETA1\n+        7.35E-02\n \n\n\n SIGMA - CORR MATRIX FOR RANDOM EFFECTS - EPSILONS  ***\n\n\n         EPS1     \n \n EPS1\n+       .........\n \n1\n ************************************************************************************************************************\n ********************                                                                                ********************\n ********************                         LAPLACIAN CONDITIONAL ESTIMATION                       ********************\n ********************                          COVARIANCE MATRIX OF ESTIMATE                         ********************\n ********************                                                                                ********************\n ************************************************************************************************************************\n \n\n            TH 1      TH 2      OM11      SG11  \n \n TH 1\n+        6.72E-04\n \n TH 2\n+       ......... .........\n \n OM11\n+        7.98E-04 .........  2.37E-03\n \n SG11\n+       ......... ......... ......... .........\n \n1\n ************************************************************************************************************************\n ********************                                                                                ********************\n ********************                         LAPLACIAN CONDITIONAL ESTIMATION                       ********************\n ********************                          CORRELATION MATRIX OF ESTIMATE                        ********************\n ********************                                                                                ********************\n ************************************************************************************************************************\n \n\n            TH 1      TH 2      OM11      SG11  \n \n TH 1\n+        2.59E-02\n \n TH 2\n+       ......... .........\n \n OM11\n+        6.32E-01 .........  4.87E-02\n \n SG11\n+       ......... ......... ......... .........\n \n1\n ************************************************************************************************************************\n ********************                                                                                ********************\n ********************                         LAPLACIAN CONDITIONAL ESTIMATION                       ********************\n ********************                      INVERSE COVARIANCE MATRIX OF ESTIMATE                     ********************\n ********************                                                                                ********************\n ************************************************************************************************************************\n \n\n            TH 1      TH 2      OM11      SG11  \n \n TH 1\n+        2.48E+03\n \n TH 2\n+       ......... .........\n \n OM11\n+       -8.32E+02 .........  7.01E+02\n \n SG11\n+       ......... ......... ......... .........\n \n1\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n ************************************************************************************************************************\n ********************                                                                                ********************\n ********************                         LAPLACIAN CONDITIONAL ESTIMATION                       ********************\n ********************                      EIGENVALUES OF COR MATRIX OF ESTIMATE                     ********************\n ********************                                                                                ********************\n ************************************************************************************************************************\n \n\n             1         2\n \n         3.68E-01  1.63E+00\n \n Elapsed finaloutput time in seconds:     0.01\n #CPUT: Total CPU Time in Seconds,        0.016\nStop Time: \nMon 12/30/2024 \n10:54 PM\n\n\n\n\nIt is still quite a long file and a lot of text, that’s why the output is collapsed.\n\n\n4.2.2 PSN sumo (run summary)\nAs a next step we can run the PsN sumo command to get a quick run summary. It tells us if the minimization was successful, if there has been any rounding errors, zero gradients, and so on.\n\n\n\n\n\n\nFigure 3: PsN sumo output summarizing the minimization process.\n\n\n\nIn this screenshot we can also already see our maximum likelihood estimates for THETA and OMEGA and their associated relative standard errors.\n\n\n4.2.3 Iteration information\nBut which steps did the NONMEM algorithm take to end up in the maximum likelihood estimate? We can read in the .ext file, which contains information about the iterations of the estimation process and also the objective function value at each iteration. Here is how it looks like:\n\n\nCode\n# Read the data, skipping the first line\next_file &lt;- read_table(paste0(base_path, \"models/estimation/1cmt_iv_est.ext\"), skip = 1)\n\n# keep only positive iterations\next_file &lt;- ext_file |&gt; \n  filter(ITERATION &gt;= 0)\n\n# rename columns\next_file &lt;- ext_file |&gt; \n  rename(\n    CL = \"THETA1\",\n    V = \"THETA2\",\n    RUV_VAR = \"SIGMA(1,1)\",\n    IIV_VAR = \"OMEGA(1,1)\"\n  )\n\n# Show the tibble\next_file |&gt; \n  head(n=10) |&gt; \n  mytbl()\n\n\n\n\n\nITERATION\nCL\nV\nRUV_VAR\nIIV_VAR\nOBJ\n\n\n\n\n0\n0.100000\n3.15\n0.1\n0.1500000\n41.845437\n\n\n1\n0.168370\n3.15\n0.1\n0.4839690\n-2.743076\n\n\n2\n0.163689\n3.15\n0.1\n0.3881430\n-3.068198\n\n\n3\n0.216552\n3.15\n0.1\n0.1465500\n-11.297115\n\n\n4\n0.216552\n3.15\n0.1\n0.1465500\n-11.297115\n\n\n5\n0.253493\n3.15\n0.1\n0.0862428\n-12.418728\n\n\n6\n0.243287\n3.15\n0.1\n0.1171540\n-12.789118\n\n\n7\n0.246139\n3.15\n0.1\n0.1111950\n-12.823615\n\n\n8\n0.246715\n3.15\n0.1\n0.1099530\n-12.824589\n\n\n9\n0.246682\n3.15\n0.1\n0.1100200\n-12.824593\n\n\n\n\n\nIn total, we have 10 entries/rows in the .ext file. The first row, iteration 0, contains the initial estimates (which we have provided in the model code) and its associated objective function value. Then we have iterations 1-8, which are intermediate steps. Finally, we are going to end up in iteration 9, at which point the convergence criterium was fulfilled and the estimation process has ended.\nThe columns carry the following information:\n\nITERATION = iteration number\nCL = Typical value of clearance in the population\nV = Volume of distribution (fixed)\nRUV_VAR = Variance of the residual unexplained variability (fixed)\nIIV_VAR = Variance of the inter-individual variability\nOBJ = Objective function value\n\nWe can use this output file to visualize the change in parameter values and objective function value over the iteration number, so we get a better understanding what is going on during the estimation steps:\n\n\nCode\n# Visualize\next_file |&gt; \n  pivot_longer(cols = c(CL, V, RUV_VAR, IIV_VAR, OBJ), names_to = \"parameter\", values_to = \"value\") |&gt;\n  ggplot(aes(x=ITERATION, y=value))+\n  geom_line()+\n  geom_point()+\n  facet_wrap(~parameter, scales=\"free\")+\n  theme_bw()+\n  labs(x=\"Iteration\", y=\"Estimation diagnostic\")+\n  ggtitle(\"Estimation diagnostics over iterations\")\n\n\n\n\n\n\n\n\nFigure 4: Iterative diagnostics of the NONMEM estimation process, showing the progression of clearance, inter-individual variability, and objective function values over 10 iterations. Fixed parameters (RUV_VAR, V) remain constant.\n\n\n\n\n\nIn this exercise, the reproduction of the objective function value for a given set of parameters will be the main goal and this .ext file provides us with the reference solution. I actually don’t want to go down the rabbit hole of trying to reproduce the math behind these optimization algorithms. However, in Section 8, we are going to at least use the optim function in R to partly reproduce the search algorithm.\nIn the next big chapter we will try to understand the theory behind calculating the log-likelihood needed for the objective function based on our simple example."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-theory-statmod",
    "href": "posts/understanding_nlme_estimation/index.html#sec-theory-statmod",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "5.1 Statistical model",
    "text": "5.1 Statistical model\nIn our little example we assume to have a (simple) hierarchical nonlinear mixed-effects (NLME) model, for which we want to conduct the parameter estimation. To my understanding the hierarchical structure is given by having variability defined on a population (=parameter) level and an individual (=observation) level, while the individual level depends on the parameter level. Let’s have a closer look to both of these levels.\n\n5.1.1 Population (parameter) level\nThe population level is represented by an inter-individual variability (IIV) term, which assumes a log-normal distribution around a typical parameter value. In this simplified example we only consider IIV on clearance and do not consider any other random effects. The population (or parameter) level can be defined as follows:\n\\[CL_i = \\theta_{TVCL} \\cdot e^{\\eta_{i}},~~~~~\\eta_{i} \\sim N(0, \\omega^2) \\tag{1}\\]\nHere, the individual clearance (\\(CL_i\\)) is modeled as a log-normally distributed parameter, where (\\(\\theta_{\\text{TVCL}}\\)) is the typical clearance value and \\(\\eta_{i}\\) is a random effect accounting for inter-individual variability (IIV). We assume that this random effect follows a normal distribution with mean zero and variance \\(\\omega^2\\). An example plot of such a population level is shown below:\n\n\nCode\n# Set seed for reproducibility\nset.seed(123)\n\n# Define population parameters\ntheta_TVCL &lt;- 10  # Typical clearance (L/h)\nomega_sq &lt;- 0.2   # Variance of IIV on clearance\nomega &lt;- sqrt(omega_sq)  # Standard deviation of IIV\nnum_individuals &lt;- 5000    # Number of individuals in the population\n\n# Simulate individual random effects\neta_i &lt;- rnorm(num_individuals, mean = 0, sd = omega)\n\n# Compute individual clearances\nCL_i &lt;- theta_TVCL * exp(eta_i)\n\n# Create a data frame for plotting\npopulation_data &lt;- data.frame(\n  Individual = 1:num_individuals,\n  eta_i = eta_i,\n  CL = CL_i\n)\n\n# Calculate density for annotation placement\ndensity_CL &lt;- density(CL_i)\nmax_density &lt;- max(density_CL$y)\n\n# Create the ggplot\np_pop &lt;- ggplot(population_data, aes(x = CL)) +\n  # Histogram of clearance values\n  geom_histogram(aes(y = ..density..), binwidth = 0.5, fill = \"lightgreen\", color = \"black\", alpha = 0.7) +\n  # Density curve\n  geom_density(color = \"darkgreen\", size = 1) +\n  # Vertical line for typical clearance\n  geom_vline(xintercept = theta_TVCL, color = \"blue\", linetype = \"dashed\", size = 1) +\n  # Labels and theme\n  labs(\n    title = \"Population level: Clearance\",\n    x = \"Clearance (L/h)\",\n    y = \"Density\"\n  ) +\n  theme_bw() +\n  # Annotation for Typical Clearance\n  annotate(\"text\", x = theta_TVCL + 5, y = max_density * 0.9,\n           label = expression(theta[TVCL]~\": Typical Clearance\"),\n           color = \"blue\", hjust = 0) +\n  geom_segment(aes(x = theta_TVCL + 5, y = max_density * 0.9,\n                   xend = theta_TVCL, yend = max_density * 0.8),\n               arrow = arrow(length = unit(0.2, \"cm\")), color = \"blue\") +\n  # Annotation for IIV\n  annotate(\"text\", x = theta_TVCL + 13, y = max_density * 0.6,\n           label = expression(IIV~\"(\"~omega^2~\")\"),\n           color = \"darkgreen\", hjust = 0.5) +\n  geom_segment(aes(x = theta_TVCL + 10, y = max_density * 0.6,\n                   xend = theta_TVCL+3, yend = max_density * 0.55),\n               arrow = arrow(length = unit(0.2, \"cm\")), color = \"darkgreen\")\n\n# Display the plot\nprint(p_pop)\n\n\n\n\n\n\n\n\nFigure 5: Distribution of clearance values at the population level, modeled as a log-normal distribution. The dashed blue line indicates the typical clearance value , while the green histogram and curve represent the density of clearance values in the population, accounting for inter-individual variability.\n\n\n\n\n\nThis plot represents the population level of our model, where the clearance values are sampled from a log-normal distribution around the typical clearance value. The dashed line represents the typical clearance value \\(\\theta_{TVCL}\\), and the green curve/bars represents the distribution of clearances in the population.\nThe random effect \\(\\eta_i\\) itself follows a normal distribution \\(N(0, \\omega^2)\\) and is visualized below:\n\n\nCode\n# plot histogram\npopulation_data |&gt; \n  ggplot(aes(x=eta_i)) +\n  geom_histogram(aes(y = ..density..), color=\"black\", fill=\"lightblue\", binwidth = 0.05, alpha=0.7) +\n  labs(x=\"ETA\", y = \"Density\", title = \"Population level: ETA\",)+\n  geom_vline(xintercept = 0, linetype = \"dashed\", linewidth = 1)+\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure 6: Distribution of ETA values at the population level, following a normal distribution centered around 0. The dashed line indicates 0.\n\n\n\n\n\n\n\n5.1.2 Individual (observation) level\nThe individual level on the other hand is defined by the observed and predicted concentrations for each subject. The predictions are based on the structural model and dependent on the individual parameters (which can be treated as a random variable, in our case CL). The individual level also incorporates residual unexplained variability (RUV), which distribution tells us how to define the likelihood function in the end. The individual level can be defined by:\n\\[Y_{ij} \\mid CL_i = f(x_{ij}; CL_i) + \\epsilon_{ij},~~~~~\\epsilon_{ij} \\sim N(0, \\sigma^2) \\tag{2}\\] where we can note that:\n\n\\(Y_{ij}\\) is the observed concentration for the \\(i^{th}\\) individual at the \\(j^{th}\\) time point, conditionally distributed given the individual’s clearance \\(CL_i\\).\n\\(f(x_{ij}; CL_i)\\) is the predicted concentration. It depends on \\(CL_i\\) (the individual clearance) and \\(x_{ij}\\) (all the information about covariates, dosing and sampling events for the \\(i^{th}\\) individual at the \\(j^{th}\\) time point).\n\\(\\epsilon_{ij}\\) is the realization of the residual unexplained variability for the \\(i^{th}\\) individual at the \\(j^{th}\\) time point. It typically follows a normal distribution \\(N(0, \\sigma^2)\\)\n\nIn our example we have two random variables, \\(Y_{ij}\\) and \\(CL_i\\), with parameters \\(\\beta := (\\theta_{TVCL}, \\omega^2, \\sigma^2)\\). In our example we just want to estimate the typical clearance \\(\\theta_{TVCL}\\) and the IIV on clearance \\(\\omega^2\\). The residual unexplained variability \\(\\sigma^2\\) is assumed to be known and fixed. The individual level with RUV is illustrated below:\n\n\nCode\n# Set seed for reproducibility\nset.seed(123)\n\n# Define model parameters\ntheta_TVCL &lt;- 10  # Typical clearance (L/h)\nomega &lt;- 0.447    # IIV on clearance (sqrt(omega^2) where omega^2 = 0.2)\nsigma &lt;- 0.5      # Residual unexplained variability (standard deviation)\nV &lt;- 20           # Volume of distribution (L)\nDose &lt;- 100       # Dose administered (mg)\n\n# Simulate data for one individual\nindividual_id &lt;- 1\neta_i &lt;- rnorm(1, mean = 0, sd = omega)  # Individual random effect\nCL_i &lt;- theta_TVCL * exp(eta_i)          # Individual clearance\n\n# Define time points\ntime &lt;- seq(0, 10, by = 1)  # From 0 to 10 hours\n\n# Compute predicted concentrations based on the 1 cmt model\nC_pred &lt;- (Dose / V) * exp(- (CL_i / V) * time)\n\n# Add residual unexplained variability\nepsilon_ij &lt;- rnorm(length(time), mean = 0, sd = sigma)\nC_obs &lt;- C_pred + epsilon_ij\n\n# Create a data frame\nind_lvl_data &lt;- data.frame(\n  Time = time,\n  Predicted = C_pred,\n  Observed = C_obs\n)\n\n# Compute the upper and lower bounds for the normal distribution around prediction\nind_lvl_data &lt;- ind_lvl_data |&gt;\n  mutate(\n    Upper = Predicted + sigma,\n    Lower = Predicted - sigma\n  )\n\n# Create the ggplot\np &lt;- ggplot(ind_lvl_data, aes(x = Time)) +\n  # Shaded area for normal distribution around prediction\n  geom_ribbon(aes(ymin = Lower, ymax = Upper), fill = \"lightblue\", alpha = 0.5) +\n  # Predicted concentration line\n  geom_line(aes(y = Predicted), color = \"black\", size = 1) +\n  # Observed data points\n  geom_point(aes(y = Observed), color = \"red\", size = 2) +\n  # Labels and theme\n  labs(\n    title = \"Individual level\",\n    x = \"Time (hours)\",\n    y = \"Concentration (mg/L)\"\n  ) +\n  theme_bw() +\n  # Adjusted annotation for f(x)\n  annotate(\"text\", x = 6, y = 1.8, label = \"f(x): Predicted Concentration\", color = \"black\", hjust = 0) +\n  geom_segment(aes(x = 7.2, y = 1.6, xend = 6, yend = 0.5),\n               arrow = arrow(length = unit(0.2, \"cm\")), color = \"black\") +\n  # Adjusted annotation for Y_ij\n  annotate(\"text\", x = 2, y = 4.15, label = \"Yij: Observed Concentration\", color = \"red\", hjust = 0) +\n  geom_segment(aes(x = 2, y = 4.1, xend = 1.1, yend = 4.15),\n               arrow = arrow(length = unit(0.2, \"cm\")), color = \"red\") +\n  # Adjusted annotation for Residual Variability\n  annotate(\"text\", x = 1.8, y = 0.6, label = \"Residual Unexplained Variability (σ)\", color = \"blue\", hjust = 0.5) +\n  geom_segment(aes(x = 2, y = 0.8, xend = 3, yend = 1),\n               arrow = arrow(length = unit(0.2, \"cm\")), color = \"blue\") +\n  # Add a legend manually\n  scale_fill_manual(\n    name = \"Components\",\n    values = c(\"lightblue\" = \"lightblue\"),\n    labels = c(\"±1σ around Prediction\")\n  ) +\n  guides(fill = guide_legend(override.aes = list(alpha = 0.5)))+\n  scale_x_continuous(breaks=seq(0,10,2))\n\n# Display the plot\nprint(p)\n\n\n\n\n\n\n\n\nFigure 7: Illustrative example of observed and predicted concentrations at the individual level, with residual unexplained variability shown as the shaded area around predictions.\n\n\n\n\n\nThis plot shows exemplary shows the predicted and observed concentrations as well as the residual unexplained variability around the prediction. It represents our individual or observation level of the model. This is a simple illustration; typically, datasets would not include negative concentrations and would be rather flagged as below the quantification limit."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-theory-mle",
    "href": "posts/understanding_nlme_estimation/index.html#sec-theory-mle",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "5.2 Maximum likelihood estimation",
    "text": "5.2 Maximum likelihood estimation\nIn our case, we have only two parameters to estimate: \\(\\theta_{TVCL}\\) and \\(\\omega^2\\). The overall goal is to infer the parameters of interest \\((\\hat{\\theta}_{TVCL}, \\hat{\\omega^2})\\) from our observed data \\(y_{1:n}\\). In this case, \\(y_{1}\\) would denote the vector of \\(m_i\\) observations for the first individual out of n total individuals. Ideally, we would like to infer the parameters by directly maximizing the complete data log-likelihood (\\(\\ln L\\)) function:\n\\[(\\hat{\\theta}_{TVCL}, \\hat{\\omega^2})_{ML} = \\underset{\\theta_{TVCL},~ \\omega^2}{\\mathrm{argmax}}~\\ln L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}, CL_{i:n}\\right) \\tag{3}\\]\nTo align more with the notation in Wang (2007), we can re-write the expression based on \\(\\eta_i\\) values instead of \\(CL_i\\). We can re-write:\n\\[(\\hat{\\theta}_{TVCL}, \\hat{\\omega^2})_{ML} = \\underset{\\theta_{TVCL},~ \\omega^2}{\\mathrm{argmax}}~\\ln L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}, \\eta_{i:m}\\right) \\tag{4}\\]\nPlease note that Equation 3 and Equation 4 represent the complete data log-likelihood, but more to that later in Section 5.2.2. The reason why we deal with log-likelihood is that it makes a lot of calculations a bit easier (e.g., products become sums). Furthermore, likelihood terms can become very small and this can lead to numerical difficulties. By taking the logarithm, we can avoid this issue.\nBefore we continue, let’s first remind ourselves what likelihood is all about and what is the difference compared to probability.\n\n5.2.1 Likelihood vs probability\nI personally see the difference between likelihood and probability as a matter of from which “direction” we are looking at the things. Probability is about looking forward (into the future): “If we have a fully specified model and set of parameters, what are the probabilities of certain future events happening?”. On the other hand, Likelihood is about looking backward: “Now that we have the data / made that particular observation, what is the most likely model or parameters that could have produced it?”\nLet’s take a simple example to visualize the likelihood. We assume that the height of a human being is normally distributed and we randomly picked a guy on the streets with a height of 180 cm. Which set of parameters is more likely to lead to this height measurement? A set of parameters that assumes a mean height of 170 cm and a standard deviation of 30 cm or a set of parameters that assumes a mean height of 190 cm and a standard deviation of 5 cm?\n\n\nCode\n# Define the parameter sets\nparams &lt;- data.frame(\n  parameter_set = c(\"Mean = 175 cm, SD = 30 cm\", \"Mean = 190 cm, SD = 5 cm\"),\n  mean = c(175, 190),\n  sd = c(30, 5)\n)\n\n# Define the range of heights for plotting\nx_values &lt;- seq(100, 250, by = 0.1)\n\n# Generate density data for each parameter set\ndensity_data &lt;- params |&gt;\n  rowwise() |&gt;\n  do(data.frame(\n    parameter_set = .$parameter_set,\n    x = x_values,\n    density = dnorm(x_values, mean = .$mean, sd = .$sd)\n  )) |&gt;\n  ungroup()\n\n# Calculate the density (likelihood) at 180 cm for each parameter set\nlikelihoods &lt;- params |&gt;\n  rowwise() |&gt;\n  mutate(density_at_180 = dnorm(180, mean = mean, sd = sd)) |&gt;\n  select(parameter_set, density_at_180)\n\n# Merge the likelihoods with the density data for plotting\ndensity_data &lt;- density_data |&gt;\n  left_join(likelihoods, by = \"parameter_set\")\n\n# Create the plot\nggplot(density_data, aes(x = x, y = density)) +\n  geom_line(color = \"darkblue\") +  # Plot the density curves\n  facet_wrap(~ parameter_set, ncol = 1) +  # Create separate panels\n  geom_vline(xintercept = 180, linetype = \"dashed\", color = \"red\") +  # Dashed line at 180 cm\n  geom_point(data = likelihoods, aes(x = 180, y = density_at_180), color = \"blue\", size = 2, pch = 8) +  # Point at 180 cm\n  geom_text(data = likelihoods,\n            aes(x = 180, y = density_at_180,\n                label = sprintf(\"Likelihood: %.5f\", density_at_180)),\n            hjust = 1.1, vjust = -0.5, color = \"blue\") +  # Likelihood label on the left\n  labs(title = \"Likelihood of Observing a Height of 180 cm\",\n       x = \"Height (cm)\",\n       y = \"Density\") +\n  theme_bw()  # Apply black-and-white theme\n\n\n\n\n\n\n\n\nFigure 8: Comparison of likelihoods for observing a height of 180 cm under two parameter sets: (1) mean = 175 cm, SD = 30 cm and (2) mean = 190 cm, SD = 5 cm.\n\n\n\n\n\nWe can see the parameters \\(\\mu = 175~cm\\) and \\(\\sigma = 30~cm\\) are more likely to have produced the observed height of 180 cm than the alternative set of parameters. The likelihood can be calculated with its respective probability density or probability mass function. More to that later. What becomes clear is that the concept of likelihood fundamentally requires observed data to be meaningful. And this fact leads to an issue when trying to calculate the joint likelihood for our NLME model.\n\n\n5.2.2 The problem with the joint likelihood\nWe are trying to estimate the joint log-likelihood \\(\\ln L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}, \\eta_{i:n}\\right)\\), which is the likelihood of the parameters \\(\\theta_{TVCL}\\) and \\(\\omega^2\\) given that we have observed \\(y_{1:n}\\) and \\(\\eta_{i:n}\\). Now we have a problem. While we directly observe \\(y_{1:n}\\), we do not observe \\(\\eta_i\\) (or its respective \\(CL_i\\)) values directly. In other words: You will never receive a dataset where you have an “observed” individual clearance or an “observed” individual random effect parameter. This is why \\(\\eta_i\\) can be called an unobserved latent variable. Previously we have defined the complete data log-likelihood in Equation 3 and Equation 4, which is the log-likelihood of both the observed data and the unobserved latent variables. But without observations for \\(\\eta_{i:n}\\) we cannot compute the complete data likelihood.\nNow what? Our approach would be to somehow get rid of the general dependence on \\(\\eta_i\\). This is where the so-called marginal likelihood comes into play, which does not longer depend on an \\(\\eta_i\\) observation. In our case the maximum likelihood estimates are based on the marginal likelihood:\n\\[(\\hat{\\theta}_{TVCL}, \\hat{\\omega^2})_{ML} = \\underset{\\theta_{TVCL}, \\omega^2}{\\mathrm{argmax}}~\\ln L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right) \\tag{5}\\]\nPlease note that we do not depend on an actual \\(\\eta_{i:n}\\) observation anymore, only on \\(y_{1:n}\\). To set up the actual equation let’s first rewrite the Likelihood as a probability:\n\\[L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right) = p(y_{1:n}; \\theta_{TVCL}, \\omega^2) \\tag{6}\\]\nAssuming that the observations \\(y_1, y_2, ..., y_n\\) are independent, the likelihood can be expressed as the product of the individual likelihoods:\n\\[L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right) = \\prod_{i=1}^n p(y_{i}; \\theta_{TVCL},~ \\omega^2) \\tag{7}\\]\nFor each individual, we have \\(m_i\\) observations and \\(y_{i}\\) represents a vector of these \\(m_i\\) observations. We can also write it more explicitly to avoid misunderstandings, again assuming independence across observations.\n\\[L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right) = \\prod_{i=1}^n \\prod_{j=1}^{m_i}  p(y_{ij}; \\theta_{TVCL},~ \\omega^2) \\tag{8}\\]\nTo align the likelihood expression with the structure of our model in Equation 1 and Equation 2, we need to reformulate Equation 8 to explicitly include the individual random effects \\(\\eta_i\\). This allows us to express the likelihood in terms of both the individual-level and population-level components, with which we can actually do calculations. In other words: Equation 8 does not yet reflect the specific hierarchical form of our model. By reformulating the likelihood (see next steps), we transform this generic form into a structure that directly incorporates our model’s individual and population components and allows us to use our previously specified model.\n\\[L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right) = \\prod_{i=1}^n \\int \\left( \\prod_{j=1}^{m_i} p(y_{ij}, \\eta_i; \\theta_{TVCL}, \\omega^2) \\right) \\cdot d\\eta_i \\tag{9}\\]\nIn this first step, we are integrating over all possible values of \\(\\eta_i\\) (since we can’t directly observe \\(\\eta_i\\)). We can now further split this marginal likelihood equation by using the chain rule of probability, which brings us closer to the population and individual level structure of our model and closer to a state where we can use the model:\n\\[L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right) = \\prod_{i=1}^n \\int \\left( \\prod_{j=1}^{m_i} p(y_{ij}| \\eta_i; \\theta_{TVCL}, \\omega^2)\\right) \\cdot p(\\eta_i | \\theta_{TVCL}, \\omega^2) \\cdot d\\eta_i \\tag{10}\\]\nAs \\(p(y_{ij}| \\eta_i; \\theta_{TVCL}, \\omega^2)\\) does not actually depend on \\(\\omega^2\\), and \\(p(\\eta_i | \\theta_{TVCL}, \\omega^2)\\) does not actually depend on \\(\\theta_{TVCL}\\), we can then simplify the equation to:\n\\[L\\left(\\theta_{TVCL}, \\omega^2| y_{1:n}\\right) = \\prod_{i=1}^n  \\int \\left( \\prod_{j=1}^{m_i} p(y_{ij}| \\eta_i; \\theta_{TVCL})\\right) \\cdot p(\\eta_i | \\omega^2) \\cdot d\\eta_i \\tag{11}\\]\nThe integral now consists of two parts: The individual level term \\(p(y_{ij}| \\eta_i; \\theta_{TVCL})\\) and the population level term \\(p(\\eta_i | \\omega^2)\\). The intuition behind this can be seen as something like this: For a given \\(\\eta_i\\) within the integral, the population term \\(p(\\eta_i |\\omega^2)\\) tells us how likely it is to observe this particular \\(\\eta_i\\) value in the population. The individual term \\(p(y_{ij}| \\eta_i; \\theta_{TVCL})\\) on the other hand tells us how likely it is to observe the \\(j^{th}\\) observation of the \\(i^{th}\\) individual given that particular \\(\\eta_i\\) value. The Likelihood will be maximal when the product of both terms is maximal, so that it is very likely to have this particular \\(\\eta_i\\) value in the population and that it is very likely to observe the set of \\(y_{ij}\\) values given this \\(\\eta_i\\) value.\nSo is it all good now? Kind of. We got rid of the dependence on directly observing \\(\\eta_i\\) for the complete data log-likelihood and have instead a nice marginal likelihood equation. However, solving the marginal likelihood is much harder due to this complex integral. So the next important task is to find a way to deal with this integral. And if it is hard to calculate, why not just approximate it?"
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-theory-intapprox",
    "href": "posts/understanding_nlme_estimation/index.html#sec-theory-intapprox",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "5.3 Approximating the integral",
    "text": "5.3 Approximating the integral\n\n5.3.1 Laplacian approximation\nSo far we’ve understood that we cannot solve the integral in the marginal likelihood equation easily. I am not sure if it is impossible or if it is just very hard to do so. But either way, in reality we need to approximate it somehow. To simplify things a bit, we will focus on the individual Likelihood \\(L_i\\) for now (for one single individual i) to avoid writing out the product (for the population) every time:\n\\[L_i\\left(\\theta_{TVCL}, \\omega^2| y_{i}\\right) = \\int \\left( \\prod_{j=1}^{m_i} p(y_{ij}| \\eta_i; \\theta_{TVCL})\\right) \\cdot p(\\eta_i | \\omega^2) \\cdot d\\eta_i \\tag{12}\\]\nIn the end, we have to take the product of all individual Likelihoods to get the likelihood for the population. Now we are going to tackle the integral. Apparently, one way of approximating integrals is the Laplacian approximation. It is a method which simplifies difficult integrals by focusing on the most important part (contribution-wise) of the function being integrated, which is the point where the function reaches its peak (the maximum or mode of the function). The idea is to approximate the function around this maximum point by a so-called second-order Taylor expansion. This approximation assumes that the integrand behaves like a Gaussian (bell-shaped) function around its maximum. The Taylor expansion with different orders (n = 1/2/3/10) is visualized here (2024):\n\n\n\n\n\n\nFigure 9: Visualization of second-order Taylor expansions (n=1,2,3,10) around the logarithmic function ln(x).\n\n\n\nNow, there seems to exist an useful feature of integrals that they often become analytically solvable if they take a certain form. If we can express the integral in the following exponential form:\n\\[\\int{\\exp(f(x))~dx} \\tag{13}\\]\nwhere \\(f(x)\\) is a second-order polynomial of the form \\(f(x) = ax^2 + bx + c\\) with a negative quadratic coefficient (\\(a &lt; 0\\)), and \\(f(x)\\) is twice differentiable with a unique maximum, we can solve it analytically. Therefore, the next step would be to bring our integral into this form, so we can make use of this nice property to kick out the annoying integral. That would allow us to get rid of any integration and just directly deal with the analytical solution.\n\n\n5.3.2 Bringing the expression in the right form\nTo bring our Likelihood expression into the right form, we can use a \\(\\exp(\\ln())\\) operator. In total this won’t do anything, but it allows us to simplify our inner expression a little, as products turn into sums due to the logarithm. And (as mentioned above), we need the exponential form of Equation 13 to solve the integral analytically. Thus, we take Equation 12 and apply the \\(\\exp(\\ln())\\) operator, which leads to a re-expression of the individual Likelihood as:\n\\[L_i\\left(\\theta_{TVCL}, \\omega^2| y_{i}\\right) = \\int \\exp \\left(\\ln \\left(\\left( \\prod_{j=1}^{m_i} p(y_{ij}| \\eta_i; \\theta_{TVCL})\\right)\\cdot p(\\eta_i | \\omega^2)\\right)\\right) \\cdot d\\eta_i \\tag{14}\\]\nAfter applying the log to each element, we get a sum:\n\\[L_i\\left(\\theta_{TVCL}, \\omega^2| y_{i}\\right) = \\int \\exp \\left(\\ln \\left(\\left( \\prod_{j=1}^{m_i} p(y_{ij}| \\eta_i; \\theta_{TVCL})\\right)\\right) + \\ln \\left(p(\\eta_i | \\omega^2)\\right)\\right)\\cdot d\\eta_i \\tag{15}\\]\nAlso the product sum of the individual level term turns into a sum:\n\\[L_i\\left(\\theta_{TVCL}, \\omega^2| y_{i}\\right) = \\int \\exp \\left( \\left(\\sum_{j=1}^{m_i}  \\ln \\left(p(y_{ij}| \\eta_i; \\theta_{TVCL})\\right)\\right) + \\ln \\left(p(\\eta_i | \\omega^2)\\right)\\right) \\cdot d\\eta_i \\tag{16}\\]\nWe can now substitute the inner term with a function \\(g_i(\\eta_i)\\), which allows us to write:\n\\[L_i\\left(\\theta_{TVCL}, \\omega^2| y_{i}\\right) = \\int \\exp\\left(g_i(\\eta_i)\\right) \\cdot d\\eta_i \\tag{17}\\]\nwith\n\\[g_i(\\eta_i) = \\left(\\sum_{j=1}^{m_i}  \\ln \\left(p(y_{ij}| \\eta_i; \\theta_{TVCL})\\right)\\right) + \\ln \\left(p(\\eta_i | \\omega^2)\\right) \\tag{18}\\]\nWe have successfully brought our expression into a format which allows us to later get rid of the integral itself. Please note the similarity between Equation 13 and Equation 17. In a next step we want to approximate \\(g_i(\\eta_i)\\) as a Gaussian (second-order polynomial) function around its maximum point (mode) \\(\\eta_i^*\\) via a Taylor expansion.\n\n\n5.3.3 Taylor expansion\nOkay, let’s approximate \\(g_i(\\eta_i)\\) by a second order Taylor expansion of \\(g_i(\\eta_i)\\) at point \\(\\eta_i^*\\), as we cannot explicitly and directly calculate that integral1. A second-order Taylor expansion at point \\(\\eta_i^*\\) is given by the following expression:\n\\[g_i(\\eta_i) \\approx g_i(\\eta_i^*) + g_i'(\\eta_i^*) \\cdot (\\eta_i - \\eta_i^*) + \\frac{1}{2} g_i''(\\eta_i^*)(\\eta_i-\\eta_i^*)^2 \\tag{19}\\]\nDuring Laplacian estimation, we want to choose \\(\\eta_i^*\\) so that we are in the mode of \\(g_i\\). First of all, around this point we have the biggest contribution to the integral. Since a Taylor approximation is always most accurate at the point for which we are expanding it, it makes sense that this should be the point at which the integral is the most sensitive to. Second, the mode is the point where the first derivative (\\(g_i'(\\eta_i^*)\\)) is zero. Therefore, it allows us to drop the second term of the Taylor expansion:\n\\[g_i(\\eta_i) \\approx g_i(\\eta_i^*) + \\frac{1}{2} g_i''(\\eta_i^*)(\\eta_i-\\eta_i^*)^2 \\tag{20}\\]\ngiven that\n\\[\\eta_i^* = \\underset{\\eta_i}{\\mathrm{argmax}}\\left[g_i(\\eta_i)\\right] = \\underset{\\eta_i}{\\mathrm{argmax}}\\left[\\left(\\sum_{j=1}^{m_i}  \\ln \\left(p(y_{ij}| \\eta_i; \\theta_{TVCL})\\right)\\right) + \\ln \\left(p(\\eta_i | \\omega^2)\\right)\\right] \\tag{21}\\]\nGreat! Now we have approximated the complex expression \\(g_i(\\eta_i)\\) and can now try to get rid of the integral in a next step.\n\n\n5.3.4 Kicking out the integral\nOkay, so the first thing we do is to take Equation 17 and substitute \\(g_i(\\eta_i)\\) with the 2nd-order approximation we obtained via Equation 20. This gives us:\n\\[L_i\\left(\\theta_{TVCL}, \\omega^2| y_{i}\\right) = \\int \\exp\\left(g_i(\\eta_i^*) + \\frac{1}{2} g_i''(\\eta_i^*)(\\eta_i-\\eta_i^*)^2\\right) \\cdot d\\eta_i \\tag{22}\\]\nOur first goal is to isolate expressions which are dependent on \\(\\eta_i\\) and those which are not. This will later allow us to get rid of the integral. The term \\(g_i(\\eta_i^*)\\) is independent on \\(\\eta_i\\) (it is just being evaluated at the mode \\(\\eta_i^*\\)), while the term \\(\\frac{1}{2} g_i''(\\eta_i^*)(\\eta_i-\\eta_i^*)^2\\) is actually dependent on \\(\\eta_i\\). Therefore, we are now going to split the expression into two parts:\n\\[L_i\\left(\\theta_{TVCL}, \\omega^2| y_{i}\\right) = \\int \\exp\\left(g_i(\\eta_i^*)\\right) \\cdot \\exp\\left( \\frac{1}{2} g_i''(\\eta_i^*)(\\eta_i-\\eta_i^*)^2\\right) \\cdot d\\eta_i \\tag{23}\\]\nNow in order that our plan (kicking out the integral) works, we need to ensure that the exponent of the second term is negative. This is important, because we want to approximate the integral as a Gaussian function, which is only possible if this exponent is negative. Technically, we can be sure that this is the case, because we are expanding around the mode and the second derivative at this point is negative for our function. However, since I want to align more with the reference solution in Wang (2007), I re-write the expression in a way that makes this more explicit:\n\\[L_i\\left(\\theta_{TVCL}, \\omega^2| y_{i}\\right) = \\int \\exp\\left(g_i(\\eta_i^*)\\right) \\cdot \\exp\\left( -\\frac{1}{2} \\left| g_i''(\\eta_i^*)\\right| (\\eta_i-\\eta_i^*)^2\\right) \\cdot d\\eta_i \\tag{24}\\]\nWe just introduced a negative sign and took the absolute value of the second derivative. Now we can substitute \\(g_i(\\eta_i^*)\\) with the respective expression given in Equation 18 (and evaluated at the mode \\(\\eta_i^*\\)) and get:\n\\[\\begin{split} L_i\\left(\\theta_{TVCL}, \\omega^2| y_{i}\\right) = \\int \\exp\\left(\\left(\\sum_{j=1}^{m_i}  \\ln \\left(p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right)\\right) + \\ln \\left(p(\\eta_i^* | \\omega^2)\\right)\\right) \\\\\n\\cdot \\exp\\left( -\\frac{1}{2} \\left| g_i''(\\eta_i^*)\\right| (\\eta_i-\\eta_i^*)^2\\right) \\cdot d\\eta_i \\end{split} \\tag{25}\\]\nNow the term \\(\\exp\\left(\\left(\\sum_{j=1}^{m_i}  \\ln \\left(p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right)\\right) + \\ln \\left(p(\\eta_i^* | \\omega^2)\\right)\\right)\\) does not depend anymore on \\(\\eta_i\\) (over which we are integrating), since the expression is just evaluated at a given value of \\(\\eta_i^*\\). This means it is a constant and can be factored out of the integral:\n\\[ \\begin{split}  L_i\\left(\\theta_{TVCL}, \\omega^2| y_{i}\\right) = \\exp\\left(\\left(\\sum_{j=1}^{m_i}  \\ln \\left(p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right)\\right) + \\ln \\left(p(\\eta_i^* | \\omega^2)\\right)\\right) \\\\ \\cdot\\int  \\exp\\left( -\\frac{1}{2} \\left| g_i''(\\eta_i^*)\\right| (\\eta_i-\\eta_i^*)^2\\right) \\cdot d\\eta_i \\end{split} \\tag{26}\\]\nRemember the trick we have used to bring the integral in the right form? This operator \\(\\exp(\\ln())\\) is not needed for the term which has been factored out, so we can simplify it to \\(\\left(\\prod_{j=1}^{m_i}  p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right) \\cdot p(\\eta_i^* | \\omega^2)\\). Please note, that the summation becomes a product again as we transform to the normal domain:\n\\[L_i\\left(\\theta_{TVCL}, \\omega^2| y_{i}\\right) = \\left(\\prod_{j=1}^{m_i}  p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right) \\cdot p(\\eta_i^* | \\omega^2) \\cdot \\int \\exp\\left( -\\frac{1}{2} \\left| g_i''(\\eta_i^*)\\right| (\\eta_i-\\eta_i^*)^2\\right) \\cdot d\\eta_i \\tag{27}\\]\nNow the whole plan of those people, who came up with this derivation, works out: The remaining integral is the integral of a Gaussian function and can be solved analytically. We now shortly just focus on the integral part of Equation 27:\n\\[\\int \\exp\\left( -\\frac{1}{2} \\left| g_i''(\\eta_i^*)\\right| (\\eta_i-\\eta_i^*)^2\\right) \\cdot d\\eta_i \\tag{28}\\]\nIn general, a Gaussian integral has the following form (and analytical solution):\n\\[\\int \\exp\\left(-\\frac{1}{2} \\frac{x^2}{\\sigma^2} \\right) dx = \\sqrt{2\\pi\\sigma^2} \\tag{29}\\]\nTo see how our expression fits this form, we notice that\n\\[-\\frac{1}{2} \\left| g_i''(\\eta_i^*)\\right| (\\eta_i-\\eta_i^*)^2 \\tag{30}\\]\ncan be written as \\(-\\frac{1}{2} x^2\\) via the substitution\n\\[x = \\sqrt{\\left| g_i''(\\eta_i^*)\\right|} (\\eta_i-\\eta_i^*), ~~ d\\eta_i = \\frac{1}{\\sqrt{\\left| g_i''(\\eta_i^*)\\right|}} \\cdot dx \\tag{31}\\]\nHence, we obtain:\n\\[\\int \\exp\\left(-\\frac{1}{2} \\left| g_i''(\\eta_i^*)\\right| (\\eta_i-\\eta_i^*)^2 \\right) \\cdot d\\eta_i = \\int \\exp\\left(-\\frac{1}{2} x^2\\right) \\cdot \\frac{1}{\\sqrt{\\left| g_i''(\\eta_i^*)\\right|}} dx \\tag{32}\\]\nwhich evaluates to\n\\[\\frac{1}{\\sqrt{\\left| g_i''(\\eta_i^*)\\right|}} \\cdot \\sqrt{2 \\pi} = \\sqrt{2 \\pi \\cdot \\frac{1}{\\left| g_i''(\\eta_i^*)\\right|}} = \\sqrt{\\frac{2\\pi}{\\left| g_i''(\\eta_i^*)\\right|}} \\tag{33}\\]\nFinally the magic happens. With all the work we have invested we can finally harvest the fruits. The integral disappears and we are left with a simple expression.\n\\[\\int \\exp\\left( -\\frac{1}{2} \\left| g_i''(\\eta_i^*)\\right| (\\eta_i-\\eta_i^*)^2\\right) \\cdot d\\eta_i = \\sqrt{2\\pi\\cdot \\frac{1}{\\left| g_i''(\\eta_i^*)\\right|} } = \\sqrt{\\frac{2\\pi}{\\left| g_i''(\\eta_i^*)\\right|}} \\tag{34}\\]\nTo my understanding we have now achieved an important goal by turning a hard-to calculate integral into an analytical expression, which is easier to evaluate. The only challenge remaining is the calculation of the second derivative \\(g_i''(\\eta_i^*)\\), which is not very straightforward. Substituting the simplified integral part from Equation 34 back into our individual likelihood expression (Equation 27), we get:\n\\[L_i\\left(\\theta_{TVCL}, \\omega^2| y_{i}\\right) = \\left(\\prod_{j=1}^{m_i}  p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right) \\cdot p(\\eta_i^*|\\omega^2) \\cdot \\sqrt{\\frac{2\\pi}{\\left| g_i''(\\eta_i^*)\\right|}} \\tag{35}\\]\nAlso here, we need to be careful. As mentioned above, \\(g_i''(\\eta_i^*)\\) is expected to be negative, since the function is concave and has a negative curvature at their maximum. I saw in Wang (2007) that they write \\(-g_i''(\\eta_i^*)\\), however, this should be the same as \\(|g_i''(\\eta_i^*)|\\) and I am going to stick to the absolute value out of convenience and because I find it a bit easier to follow.\nNow we can move a step further and translate it into something which is more familiar to us: The objective function in NONMEM. In the next section, we are going to spell out the missing expressions and also define the actual equation for the second derivative \\(g_i''(\\eta_i^*)\\)."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-theory-of",
    "href": "posts/understanding_nlme_estimation/index.html#sec-theory-of",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "5.4 Defining the Objective Function",
    "text": "5.4 Defining the Objective Function\n\n5.4.1 General\nOkay, so our next task is to define the objective function in NONMEM. We know that NONMEM calculates the -2 log likelihood (Bauer 2020). To my understanding the main reason to use the log of the Likelihood is to make it numerically more stable and the main reason to take the -2 is to make it asymptotically chi-square distributed (and thus, it allows some statistical testing). The negative sign turns our maximization problem into a minimization problem, which is mathematically easier to solve. The -2 log likelihood for a single individual is defined as:\n\\[OF_i = -2\\ln L_i\\left(\\theta_{TVCL}, \\omega^2| y_{i}\\right) \\tag{36}\\]\nFor the sake of simplicity we are still focusing on a single individual \\(i\\) and its contribution to the objective function. In the end, we would have to sum up the individual contributions to the objective function \\(OF_i\\) to get the final objective function. When we substitute the individual likelihood from Equation 35 into Equation 36, we get:\n\\[OF_i = -2\\ln\\left(\\left(\\prod_{j=1}^{m_i}  p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right) \\cdot p(\\eta_i^*|\\omega^2) \\cdot \\sqrt{\\frac{2\\pi}{\\left| g_i''(\\eta_i^*)\\right|}}\\right) \\tag{37}\\]\nWe can apply the \\(-2\\ln()\\) operation to each element and are left with:\n\\[OF_i = -2 \\ln \\left(\\prod_{j=1}^{m_i}  p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right) -2 \\ln \\left(p(\\eta_i^*|\\omega^2)\\right) -2 \\ln \\left(\\sqrt{\\frac{2\\pi}{\\left| g_i''(\\eta_i^*)\\right|}}\\right) \\tag{38}\\]\nThis can be re-written as:\n\\[OF_i = \\left(\\sum_{j=1}^{m_i} -2  \\ln\\left(p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right)\\right) -2 \\ln \\left(p(\\eta_i^*|\\omega^2)\\right) -2 \\ln \\left(\\sqrt{\\frac{2\\pi}{\\left| g_i''(\\eta_i^*)\\right|}}\\right) \\tag{39}\\]\nFrom here on it makes sense to split up the terms in order to better understand what is going on. We can identify three terms in the expression given by Equation 39:\n\nFirst term: \\(\\left(\\sum_{j=1}^{m_i} -2 \\ln\\left(p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right)\\right)\\)\nSecond term: \\(-2 \\ln \\left(p(\\eta_i^*|\\omega^2)\\right)\\)\nThird term: \\(-2 \\ln \\left(\\sqrt{\\frac{2\\pi}{\\left| g_i''(\\eta_i^*)\\right|}}\\right)\\)\n\nLet’s spell out each of these terms in more detail and see how they can be calculated.\n\n\n5.4.2 Term 1\nLet’s tackle the first term of Equation 39. It is given by:\n\\[\\left(\\sum_{j=1}^{m_i} -2  \\ln\\left(p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right)\\right) \\tag{40}\\]\nThe question we are asking is: “How likely is it to observe a certain set of data points \\(y_i\\) (= a series of observations for an individual) given the individual (most likely) parameter \\(\\eta_i^*\\) and \\(\\theta_{TVCL}\\)?” Please note: It seems that this expression is independent of \\(\\omega^2\\), but it is not. Our most likely \\(\\eta_i^*\\) is the mode of the distribution characterized by \\(\\omega^2\\), so we have a hidden dependence here. Typically, the our first term would also involve the residual unexplained variance (RUV) given by \\(\\sigma^2\\), but in our simple example it is fixed to a certain variance and thus, we don’t have to estimate it.\nWe assume a normal distribution (given by the RUV) around our model predictions \\(f(x_{ij}; \\theta_i)\\). Please refer to Figure 7 for a refresher. Now we can illustrate this concept for one single data point of a single individual (e.g., after 10 h a concentration of 2 mg/L was measured) as a case example to understand the concept a bit better:\n\n\nCode\n# Set seed for reproducibility (optional)\nset.seed(123)\n\n# Define fixed parameters\nDose &lt;- 100            # Dose administered\nV_D &lt;- 10              # Volume of distribution\neta_i &lt;- 0             # Individual random effect\nsigma &lt;- sqrt(0.1)     # Residual unexplained variance (standard deviation)\n\n# Define population parameters (Theta_TVCL)\nTheta_TVCL_values &lt;- c(10, 15, 20)\n\n# Define fixed time point (t)\nt &lt;- 1  # You can change this as needed\n\n# Compute the structural model predictions for each Theta_TVCL\n# Using the formula: C(t) = (Dose / V_D) * exp(-Theta_TVCL / V_D * t)\nmodel_predictions &lt;- data.frame(\n  Theta_TVCL = Theta_TVCL_values,\n  C_t = (Dose / V_D) * exp(-Theta_TVCL_values / V_D * t)\n)\n\n# Define the observed data point y_i\ny_i &lt;- 2\n\n# Create a sequence of y values for plotting the PDFs\ny_values &lt;- seq(min(model_predictions$C_t) - 3*sigma, \n               max(model_predictions$C_t) + 3*sigma, \n               length.out = 1000)\n\n# Create a data frame with density values for each Theta_TVCL\ndensity_data &lt;- model_predictions |&gt; \n  rowwise() |&gt;\n  do(data.frame(\n    Theta_TVCL = .$Theta_TVCL,\n    y = y_values,\n    density = dnorm(y_values, mean = .$C_t, sd = sigma)\n  )) |&gt;\n  ungroup()\n\n# Calculate the likelihood and -2 log likelihood for the observed y_i\nlikelihood_data &lt;- model_predictions |&gt;\n  mutate(\n    likelihood = dnorm(y_i, mean = C_t, sd = sigma),\n    neg2_log_likelihood = -2 * log(likelihood)\n  )\n\n# Merge likelihood data with density_data for annotation purposes\ndensity_data &lt;- density_data |&gt;\n  left_join(likelihood_data, by = \"Theta_TVCL\")\n\n# Create a named vector for custom facet labels\nfacet_labels &lt;- setNames(\n  paste(\"TVCL =\", Theta_TVCL_values),\n  Theta_TVCL_values\n)\n\n# Start plotting\nggplot(density_data, aes(x = y, y = density)) +\n  # Plot the density curves\n  geom_line(color = \"blue\", size = 1) +\n  \n  # Add a vertical line for the observed data point y_i\n  geom_vline(xintercept = y_i, linetype = \"dashed\", color = \"red\") +\n  \n  # Facet the plot by Theta_TVCL\n  facet_wrap(~ Theta_TVCL, scales = \"free_y\", labeller = as_labeller(facet_labels), nrow=3) +\n  \n  # Add annotations for the likelihood and -2 log likelihood\n  geom_text(data = density_data |&gt; distinct(Theta_TVCL, likelihood, neg2_log_likelihood),\n            aes(x = Inf, y = Inf, \n                label = paste0(\"Likelihood: \", round(likelihood, 6),\n                               \"\\n-2 ln(L): \", round(neg2_log_likelihood, 3))),\n            hjust = 1.1, vjust = 1.1, size = 4, color = \"black\") +\n  \n  # Customize labels and theme\n  labs(\n    title = \"Likelihood for different TVCL values\",\n    subtitle = paste(\"Observed data point yi =\", y_i, \", eta_i* = 0\"),\n    x = \"Concentration (C(t))\",\n    y = \"Probability Density\"\n  ) +\n  theme_bw() +\n  scale_y_continuous(limits=c(NA, 2))+\n  theme(\n    strip.text = element_text(size = 12, face = \"bold\"),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 12)\n  )\n\n\n\n\n\n\n\n\nFigure 10: Likelihood of observing a concentration of 2 mg/L at different typical clearance values.\n\n\n\n\n\nFor simplicity we have assumed an \\(\\eta_i^*\\) of 0 to generate this plot. We can see that we have observed a concentration of 2 mg/L at a given time point and want to know how likely it is to observe this particular concentration given different values of \\(\\theta_{TVCL}\\) with a fixed \\(\\eta_i^*\\). It becomes apparent that it is much more likely to have a clearance of 15 L/h given our data than to have a clearance of 10 or 20 L/h. But how can we explicitly calculate that likelihood given by Equation 40?\nSince we deal with a normal distribution, the likelihood is given by its probability density function (PDF). The general form is given by:\n\\[pdf(\\text{obs}) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\, \\exp\\left(-\\frac{(\\text{obs} - \\mu)^2}{2\\sigma^2}\\right) \\tag{41}\\]\nIn our case we would re-write it as follows for a single observation \\(y_{ij}\\):\n\\[p(y_{ij} | \\eta_i^*; \\theta_{TVCL}) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_{ij} - f(x_{ij}, \\theta_{TVCL}, \\eta_i^*))^2}{2\\sigma^2}\\right) \\tag{42}\\]\nTaking the log leads to:\n\\[\\ln \\left(p(y_{ij} | \\eta_i^*; \\theta_{TVCL})\\right) = -\\frac{1}{2} \\ln(2\\pi\\sigma^2) - \\frac{(y_{ij} - f(x_{ij}, \\theta_{TVCL}, \\eta_i^*))^2}{2\\sigma^2} \\tag{43}\\]\nMultiplying with -2 and simplifying leads to:\n\\[-2 \\ln \\left(p(y_{ij}  | \\eta_i^*; \\theta_{TVCL})\\right) = \\ln(2\\pi\\sigma^2) + \\frac{(y_{ij}  - f(x_{ij}, \\theta_{TVCL}, \\eta_i^*))^2}{\\sigma^2} \\tag{44}\\]\nIn a last step, we would have to take the sum of all observations to consider the likelihood contributions of all datapoints:\n\\[\\left(\\sum_{j=1}^{m_i} -2  \\ln\\left(p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right)\\right) = \\sum_{j=1}^{m_i} \\left[\\ln(2\\pi\\sigma^2) + \\frac{(y_{ij}  - f(x_{ij}, \\theta_{TVCL}, \\eta_i^*))^2}{\\sigma^2}\\right] \\tag{45}\\]\nGreat! If someone would give us a \\(\\theta_{TVCL}\\) value and an \\(\\eta_i^*\\) value, we would be able to calculate the -2 log likelihood term for that particular data point. Actually, we would need to have the model prediction function at hand for this (or an ODE-solver). The model prediction \\(f(x_{ij}, \\theta_{TVCL}, \\eta_i^*)\\) was already defined in Equation 69 (see above). It will be used at various points of the final objective function.\n\n\n5.4.3 Term 2\nA very similar concept applies to the second term of Equation 39. It is given by:\n\\[-2 \\ln \\left(p(\\eta_i^*|\\omega^2)\\right) \\tag{46}\\]\nWe typically assume that \\(\\eta_i\\) is normally distributed with a mean of 0 with a variance \\(\\omega^2\\). We could do a similar illustration as above, where we have a look at a certain \\(\\eta_i^*\\) value and want to know how likely it is to observe this value given the population variance \\(\\omega^2\\) (which we have at the moment of evaluation):\n\n\nCode\n# Set seed for reproducibility (optional)\nset.seed(123)\n\n# Define fixed parameters\neta_i_star &lt;- 0.3          # Observed eta_i value\nomega_squared_values &lt;- c(0.01, 0.2, 0.5)  # Population variances\n\n# Define the mean for eta_i (assumed to be 0)\nmu_eta &lt;- 0\n\n# Compute the structural model predictions for each omega^2\n# Using the formula: eta_i ~ N(0, omega^2)\nmodel_predictions &lt;- data.frame(\n  omega_squared = omega_squared_values,\n  mean = mu_eta\n)\n\n# Create a sequence of eta values for plotting the PDFs\n# Extending the range to cover the tails adequately\neta_values &lt;- seq(\n  mu_eta - 4 * sqrt(max(omega_squared_values)), \n  mu_eta + 4 * sqrt(max(omega_squared_values)), \n  length.out = 1000\n)\n\n# Create a data frame with density values for each omega^2\ndensity_data &lt;- model_predictions |&gt;\n  rowwise() |&gt;\n  do(data.frame(\n    omega_squared = .$omega_squared,\n    eta = eta_values,\n    density = dnorm(eta_values, mean = .$mean, sd = sqrt(.$omega_squared))\n  )) |&gt;\n  ungroup()\n\n# Calculate the likelihood and -2 log likelihood for the observed eta_i*\nlikelihood_data &lt;- model_predictions |&gt;\n  mutate(\n    likelihood = dnorm(eta_i_star, mean = mean, sd = sqrt(omega_squared)),\n    neg2_log_likelihood = -2 * log(likelihood)\n  )\n\n# Merge likelihood data with density_data for annotation purposes\ndensity_data &lt;- density_data |&gt;\n  left_join(likelihood_data, by = \"omega_squared\")\n\n# Create a named vector for custom facet labels\nfacet_labels &lt;- setNames(\n  paste(\"ω² =\", omega_squared_values),\n  omega_squared_values\n)\n\n# Start plotting\nggplot(density_data, aes(x = eta, y = density)) +\n  # Plot the density curves\n  geom_line(color = \"blue\", size = 1) +\n  \n  # Add a vertical line for the observed eta_i*\n  geom_vline(xintercept = eta_i_star, linetype = \"dashed\", color = \"red\") +\n  \n  # Highlight the point (eta_i*, density at eta_i*)\n  geom_point(\n    data = density_data |&gt; filter(abs(eta - eta_i_star) &lt; 1e-6), \n    aes(x = eta, y = density), \n    color = \"darkgreen\", size = 3  \n  ) +\n  \n  # Facet the plot by omega_squared with custom labels\n  facet_wrap(\n    ~ omega_squared, \n    scales = \"free_y\",\n    labeller = as_labeller(facet_labels),\n    nrow=3\n  ) +\n  \n  # Add annotations for the likelihood and -2 log likelihood\n  geom_text(\n    data = likelihood_data,\n    aes(\n      x = Inf, y = Inf, \n      label = paste0(\n        \"Likelihood: \", round(likelihood, 4),\n        \"\\n-2 log(L): \", round(neg2_log_likelihood, 2)\n      )\n    ),\n    hjust = 1.1, vjust = 1.1, size = 4, color = \"black\"\n  ) +\n  \n  # Customize labels and theme\n  labs(\n    title = \"Likelihood term for different ω² values\",\n    subtitle = paste(\"etai* =\", eta_i_star),\n    x = expression(eta),\n    y = \"Probability Density\"\n  ) +\n  theme_bw() +\n  scale_y_continuous(limits=c(NA, 4.5))+\n  theme(\n    strip.text = element_text(size = 12, face = \"bold\"),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 12)\n  )\n\n\n\n\n\n\n\n\nFigure 11: Likelihood of observing ETA=0.3 for different variances of the inter-individual variability term.\n\n\n\n\n\nIn our little example we can observe that the likelihood of “observing”2 \\(\\eta_i^* = 0.3\\) is much higher when the IIV variance is 0.2 compared to 0.01 or 0.5. This is the information we want to capture with this second likelihood term. Similarly to the first term of Equation 39, we are applying the general form of the pdf (Equation 41), which leads to this expression:\n\\[p(\\eta_i^*|\\omega^2) = \\frac{1}{\\sqrt{2\\pi\\omega^2}} \\exp\\left(-\\frac{(\\eta_i^* - 0)^2}{2\\omega^2}\\right) \\tag{47}\\]\nPlease note that the normal distribution is centered around 0 (\\(\\mu = 0\\)), which is in contrast to the individual level where it was centered around the predicted value \\(f(x_{ij}, \\theta_{TVCL}, \\eta_i^*)\\) (see Equation 42). Because of \\(\\mu = 0\\), we can simplify this to:\n\\[p(\\eta_i^*|\\omega^2) = \\frac{1}{\\sqrt{2\\pi\\omega^2}} \\exp\\left(-\\frac{\\eta_i^{*2}}{2\\omega^2}\\right) \\tag{48}\\]\nTaking the log:\n\\[\\ln \\left(p(\\eta_i^*|\\omega^2)\\right) = -\\frac{1}{2} \\ln(2\\pi\\omega^2) - \\frac{\\eta_i^{*2}}{2\\omega^2} \\tag{49}\\]\nMultiplying with -2 and simplifying:\n\\[-2 \\ln \\left(p(\\eta_i^*|\\omega^2)\\right) = \\ln(2\\pi\\omega^2) + \\frac{\\eta_i^{*2}}{\\omega^2} \\tag{50}\\]\nVery good. We have our population-level likelihood term. Similar to Term 1, we are now able to calculate this expression if someone would give us a value for \\(\\eta_i^*\\) and \\(\\omega^2\\). The remaining term is part 3 of the objective function, which is a bit more tricky. Let’s move on to this term.\n\n\n5.4.4 Term 3\nThe derivative term is the third and last term of Equation 39 and is given by\n\\[-2 \\ln \\left(\\sqrt{\\frac{2\\pi}{\\left| g_i''(\\eta_i^*)\\right|}}\\right) \\tag{51}\\]\nWe can take the square root out of the logarithm:\n\\[-2 \\ln \\left(\\sqrt{\\frac{2\\pi}{\\left| g_i''(\\eta_i^*)\\right|}}\\right) = -2 \\cdot \\frac{1}{2} \\cdot \\ln \\left(\\frac{2\\pi}{\\left| g_i''(\\eta_i^*)\\right|}\\right) \\tag{52}\\]\nWhich can be simplified to:\n\\[-2 \\ln \\left(\\sqrt{\\frac{2\\pi}{\\left| g_i''(\\eta_i^*)\\right|}}\\right) = - \\ln \\left(\\frac{2\\pi}{\\left| g_i''(\\eta_i^*)\\right|}\\right) \\tag{53}\\]\nThis expression can be further simplified by applying log-rules to:\n\\[-2 \\ln \\left(\\sqrt{\\frac{2\\pi}{\\left| g_i''(\\eta_i^*)\\right|}}\\right) = - \\ln \\left(2\\pi\\right) + \\ln\\left(\\left|g_i''(\\eta_i^*)\\right|\\right) \\tag{54}\\]\nWe still need to find a way to calculate the second derivative of the individual model function \\(g_i(\\eta_i^*)\\). However, defining the second derivative \\(g_i''(\\eta_i^*)\\) evaluated at \\(\\eta_i^*\\) is not straightforward and in many cases (once we deal with more complex models), this would be approximated numerically (e.g., using finite difference methods). We have luckily chosen a very simple model and can actually calculate the second derivative of the closed form expression. However, it is still quite complicated and cumbersome (at least to me). Therefore, we will simply use WolframAlpha / Mathematica to find these derivatives and help us out with the derivation rules.\nThe function \\(g_i(\\eta_i)\\) was already defined in Equation 18. Given that, the second derivative is expressed as the second derivatives of its two terms:\n\\[g_i''(\\eta_i^*) = \\left[\\left(\\sum_{j=1}^{m_i}  \\ln \\left(p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right)\\right)\\right]'' +  \\left[ln\\left(p(\\eta_i^*|\\omega^2)\\right)\\right]'' \\tag{55}\\]\nWe will use WolframAlpha to calculate the second derivative for both parts.\n\n5.4.4.1 First part of term 3\nLet’s start with the first part, which is given by:\n\\[\\left[\\left(\\sum_{j=1}^{m_i}  \\ln \\left(p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right)\\right)\\right]'' \\tag{56}\\]\nIt doesn’t matter whether we differentiate each term of a sum individually and then add the results, or add the terms first and then differentiate the sum as a whole. The result remain the same due to the linearity property of differentiation. For this reason, we first define the second derivative of a single term (by ignoring the sum). Our expression for \\(\\ln \\left(p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right)\\) is given in Equation 43.\nAccording to WolframAlpha, the second derivative of Equation 43 is given by:\n\\[\\begin{multline}\n\\left[\\ln \\left(p(y_{ij}| \\eta_i^*; \\theta_{TVCL})\\right)\\right]'' = \\\\\n\\frac{-(f'(x_{ij}, \\theta_{TVCL}, \\eta_i^*))^2 + \\left(y_{ij} - f(x_{ij}, \\theta_{TVCL}, \\eta_i^*)\\right) \\cdot f''(x_{ij}, \\theta_{TVCL}, \\eta_i^*)}{\\sigma^2}\n\\end{multline} \\tag{57}\\]\nSubsequently, we have to take the sum of these derivatives over all \\(j\\) (see above):\n\\[\\begin{multline} \\left[\\left(\\sum_{j=1}^{m_i}  \\ln \\left(p(y_{ij}| \\eta_i; \\theta_{TVCL})\\right)\\right)\\right]'' = \\\\ \\sum_{j=1}^{m_i} \\left[ \\frac{-(f'(x_{ij}, \\theta_{TVCL}, \\eta_i^*))^2 + \\left(y_{ij} - f(x_{ij}, \\theta_{TVCL}, \\eta_i^*)\\right) \\cdot f''(x_{ij}, \\theta_{TVCL}, \\eta_i^*)}{\\sigma^2} \\right] \\end{multline} \\tag{58}\\]\nThe closed form expression for the model prediction function is defined in Equation 69. However, we haven’t defined its first and second derivative yet. Similar, we are going to use WolframAlpha to define these derivatives:\n\\[f'(x_{ij}, \\theta_{TVCL}, \\eta_i^*) = -\\frac{\\theta_{TVCL} \\cdot t_{ij} \\cdot e^{\\eta_i^*}}{V_D} \\cdot f(x_{ij}, \\theta_{TVCL}, \\eta_i^*) \\tag{59}\\]\n\\[f''(x_{ij}, \\theta_{TVCL}, \\eta_i^*) = \\frac{\\theta_{TVCL} \\cdot t_{ij} \\cdot e^{\\eta_i^*}}{V_D} \\cdot \\left( \\frac{\\theta_{TVCL} \\cdot t_{ij} \\cdot e^{\\eta_i^*}}{V_D} -1\\right) \\cdot f(x_{ij}, \\theta_{TVCL}, \\eta_i^*) \\tag{60}\\]\nThis provides us with everything we need for the first part of term 3.\n\n\n5.4.4.2 Second part of term 3\nThe second part of the second derivative is given by:\n\\[\\left[ln\\left(p(\\eta_i^*|\\omega^2)\\right)\\right]'' \\tag{61}\\]\nIts full expression is given by Equation 49 and the second derivative is defined as:\n\\[\\left[ln\\left(p(\\eta_i^*|\\omega^2)\\right)\\right]'' = - \\frac{1}{\\omega^2} \\tag{62}\\]\n\n\n5.4.4.3 Combining both parts\nNow we can substitute the terms in Equation 55 to get the full expression for \\(\\left[g_i(\\eta_i^*)\\right]''\\):\n\\[\\begin{multline} \\left[g_i(\\eta_i^*)\\right]'' = \\\\ \\sum_{j=1}^{m_i} \\left[ \\frac{-(f'(x_{ij}, \\theta_{TVCL}, \\eta_i^*))^2 + \\left(y_{ij} - f(x_{ij}, \\theta_{TVCL}, \\eta_i^*)\\right) \\cdot f''(x_{ij}, \\theta_{TVCL}, \\eta_i^*)}{\\sigma^2} \\right]\n- \\frac{1}{\\omega^2} \\end{multline} \\tag{63}\\]\nwhich depends on our previous definitions for \\(f'(x_{ij}, \\theta_{TVCL}, \\eta_i^*)\\) in Equation 59 and \\(f''(x_{ij}, \\theta_{TVCL}, \\eta_i^*)\\) in Equation 60.\nNow we have to pluck our results back into Equation 54 to finalize term 3:\n\\[\\begin{multline}  -2 \\ln \\left(\\sqrt{\\frac{2\\pi}{\\left| g_i''(\\eta_i^*)\\right|}}\\right) = - \\ln \\left(2\\pi\\right) + \\\\  \\ln\\left(\\left| \\sum_{j=1}^{m_i} \\left[ \\frac{-(f'(x_{ij}, \\theta_{TVCL}, \\eta_i^*))^2 + \\left(y_{ij} - f(x_{ij}, \\theta_{TVCL}, \\eta_i^*)\\right) \\cdot f''(x_{ij}, \\theta_{TVCL}, \\eta_i^*)}{\\sigma^2} \\right]\n- \\frac{1}{\\omega^2}  \\right|\\right) \\end{multline} \\tag{64}\\]\nGreat! We have found the final expression for the second derivative of \\(g_i(\\eta_i^*)\\).\n\n\n\n5.4.5 Defining \\(\\eta_i^*\\)\nOur first task is to find the particular value of \\(\\eta_i\\) which maximizes the expression in Equation 18. This should be nothing else than a Bayesian maximum a-posteriori (MAP) estimation (or empirical Bayes estimation (EBE)). We can find the mode of \\(g_i(\\eta_i)\\) by:\n\\[\\eta_i^* = \\underset{\\eta_i}{\\mathrm{argmax}}g_i(\\eta_i) = \\underset{\\eta_i}{\\mathrm{argmax}}\\left[\\left(\\sum_{j=1}^{m_i}  \\ln \\left(p(y_{ij}| \\eta_i; \\theta_{TVCL})\\right)\\right) + \\ln \\left(p(\\eta_i | \\omega^2)\\right)\\right] \\tag{65}\\]\nThis means that we are searching over all possible values for \\(\\eta_i\\) (per individual) and try to find the value that maximizes our log likelihood function. We have already previously defined \\(\\ln\\left(p(y_{ij} | \\eta_i; \\theta_{TVCL})\\right)\\) and \\(ln\\left(p(\\eta_i|\\omega^2)\\right)\\) in Equation 43 and Equation 49, respectively3. We can substitute these expressions into Equation 65:\n\\[\\eta_i^* = \\underset{\\eta_i}{\\mathrm{argmax}} ~\\left[\\sum_{j=1}^{m_i} \\left[-\\frac{1}{2} \\ln(2\\pi\\sigma^2) - \\frac{(y_{ij} - f(x_{ij}, \\theta_{TVCL}, \\eta_i))^2}{2\\sigma^2}\\right] -\\frac{1}{2} \\ln(2\\pi\\omega^2) - \\frac{\\eta_i^{2}}{2\\omega^2}\\right] \\tag{66}\\]\nNow this is an optimization (maximization) problem, so we don’t care about the constants. This simplifies the expression to:\n\\[\\eta_i^* = \\underset{\\eta_i}{\\mathrm{argmax}} ~\\left[\\sum_{j=1}^{m_i} \\left[ - \\frac{(y_{ij} - f(x_{ij}, \\theta_{TVCL}, \\eta_i))^2}{2\\sigma^2}\\right]  - \\frac{\\eta_i^{2}}{2\\omega^2}\\right] \\tag{67}\\]\nMost numerical optimization algorithms are designed to minimize functions, because it is conceptually simpler to identify a minimum than a maximum. As we will later reproduce this function in R, we are already now turning the maximization problem into a minimization problem by negation:\n\\[\\eta_i^* = \\underset{\\eta_i}{\\mathrm{argmin}} ~\\left[\\sum_{j=1}^{m_i} \\left[\\frac{(y_{ij} - f(x_{ij}, \\theta_{TVCL}, \\eta_i))^2}{2\\sigma^2}\\right]  + \\frac{\\eta_i^{2}}{2\\omega^2}\\right] \\tag{68}\\]\nIn our simple example, we luckily do not need to rely on ODE solvers (as we have an analytical solution at hand) and can later replace \\(f(x_{ij}, \\theta_{TVCL}, \\eta_i)\\) with the closed form expression of a 1 cmt iv bolus model:\n\\[f(x_{ij}, \\theta_{TVCL}, \\eta_i) = \\frac{Dose}{V_D} \\cdot \\exp(-\\frac{\\theta_{TVCL} \\cdot e^{\\eta_{i}}}{V_D}t_{ij}) \\tag{69}\\]\nEquation 68 needs to be numerically optimized for each individual at each iteration of the algorithm. This is typically done by using a numerical optimization algorithm. The optimization algorithm will search the parameter space to numerically find the value of \\(\\eta_i^*\\) that maximizes the function.\n\n\n5.4.6 Putting the pieces together\nPreviously, we have taken the three pieces of Equation 39 and expressed them in an explicit way. Let’s put them together. After simplification, our objective function is represented by:\n\\[OF_i = a + b + c \\tag{70}\\]\nwhere \\(a\\), \\(b\\), and \\(c\\) are defined based on Equation 45, Equation 50, and Equation 64:\n\\[a  = \\sum_{j=1}^{m_i} \\left[\\ln(2\\pi\\sigma^2) + \\frac{(y_{ij}  - f(x_{ij}, \\theta_{TVCL}, \\eta_i^*))^2}{\\sigma^2}\\right] \\tag{71}\\]\n\\[b = \\ln(2\\pi\\omega^2) + \\frac{\\eta_i^{*2}}{\\omega^2} \\tag{72}\\]\n\\[\\begin{multline} c = - \\ln \\left(2\\pi\\right) + \\\\  \\ln\\left(\\left| \\sum_{j=1}^{m_i} \\left[ \\frac{-(f'(x_{ij}, \\theta_{TVCL}, \\eta_i^*))^2 + \\left(y_{ij} - f(x_{ij}, \\theta_{TVCL}, \\eta_i^*)\\right) \\cdot f''(x_{ij}, \\theta_{TVCL}, \\eta_i^*)}{\\sigma^2} \\right] - \\frac{1}{\\omega^2}  \\right|\\right) \\end{multline} \\tag{73}\\]\nleading to\n\\[\\begin{multline} OF_i = \\sum_{j=1}^{m_i} \\left[\\ln(2\\pi\\sigma^2) + \\frac{(y_{ij}  - f(x_{ij}, \\theta_{TVCL}, \\eta_i^*))^2}{\\sigma^2}\\right] + \\ln(2\\pi\\omega^2) + \\frac{\\eta_i^{*2}}{\\omega^2}  - \\ln \\left(2\\pi\\right) +   \\\\ \\ln\\left(\\left| \\sum_{j=1}^{m_i} \\left[ \\frac{-(f'(x_{ij}, \\theta_{TVCL}, \\eta_i^*))^2 + \\left(y_{ij} - f(x_{ij}, \\theta_{TVCL}, \\eta_i^*)\\right) \\cdot f''(x_{ij}, \\theta_{TVCL}, \\eta_i^*)}{\\sigma^2} \\right] - \\frac{1}{\\omega^2}  \\right|\\right) \\end{multline} \\tag{74}\\]\nIt seems that NONMEM ignores all constants during the optimization process (Bauer (2020) and Wang (2007)), which is why we can get rid of the \\(\\ln(2\\pi)\\) terms. We can simplify to:\n\\[\\begin{multline} OF_i = \\sum_{j=1}^{m_i} \\left[\\ln(\\sigma^2) + \\frac{(y_{ij}  - f(x_{ij}, \\theta_{TVCL}, \\eta_i^*))^2}{\\sigma^2}\\right] + \\ln(\\omega^2) + \\frac{\\eta_i^{*2}}{\\omega^2} +   \\\\ \\ln\\left(\\left| \\sum_{j=1}^{m_i} \\left[ \\frac{-(f'(x_{ij}, \\theta_{TVCL}, \\eta_i^*))^2 + \\left(y_{ij} - f(x_{ij}, \\theta_{TVCL}, \\eta_i^*)\\right) \\cdot f''(x_{ij}, \\theta_{TVCL}, \\eta_i^*)}{\\sigma^2} \\right] - \\frac{1}{\\omega^2}  \\right|\\right) \\end{multline} \\tag{75}\\]\nPlease note that the objective function \\(OF_i\\) is a function with multiple input parameters. Technically, we would have to write something like this:\n\\[OF_i\\left(\\theta_{TVCL}, \\omega^2, \\sigma^2, y_i, \\eta_i^*, x_i = \\left[DOSE_i, V_D, t_i\\right]\\right) = (...)\\]\nto clearly state the dependencies. However, for the sake of readability we are simply denoting it \\(OF_i\\). The last step would be now to calculate the objective function for the whole population, including n individuals:\n\\[OF = \\sum_{i=1}^{n} OF_i \\tag{76}\\]\nVery good. We have now set up the final equation for our objective function and we can now reproduce these function in R!"
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-rrepro-func",
    "href": "posts/understanding_nlme_estimation/index.html#sec-rrepro-func",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "6.1 Function definitions",
    "text": "6.1 Function definitions\nIn order to reproduce the iteration of the NONMEM algorithm, we need to define a couple of functions inside R. Technically, we need all equations which are part of Equation 75. In the next steps we are going to define these functions in R.\n\n6.1.1 Structural model function and its derivatives\nThe first function we are defining in R is Equation 69, which gives us a prediction for a given \\(x_{ij}, \\theta_{TVCL}, \\eta_i\\) based on our structural model.\n\n\n\nfunction: f_pred()\n\n# Define structural model prediction function\nf_pred &lt;- function(eta_i, dose, vd, theta_tvcl, t) {\n  exp_eta_i &lt;- exp(eta_i)\n  exponent &lt;- -1 * (theta_tvcl * exp_eta_i / vd) * t\n  result &lt;- (dose / vd) * exp(exponent)\n  return(result)\n}\n\n\nWe also have to define the derivatives of f_pred(). This is the definition of the first derivative function (reproduction of Equation 59):\n\n\n\nfunction: f_prime()\n\n# Define the first derivative of the structural model prediction function\nf_prime &lt;- function(eta_i, dose, vd, theta_tvcl, t) {\n  term &lt;- (theta_tvcl * t * exp(eta_i)) / vd\n  derivative &lt;- -term * f_pred(eta_i, dose, vd, theta_tvcl, t)\n  return(derivative)\n}\n\n\nThis is the definition of the second derivative function (reproduction of Equation 60):\n\n\n\nfunction: f_double_prime()\n\n# Define the second derivative of the structural model prediction function\nf_double_prime &lt;- function(eta_i, dose, vd, theta_tvcl, t) {\n  term &lt;- (theta_tvcl * t * exp(eta_i)) / vd\n  second_derivative &lt;- term * (term - 1) * f_pred(eta_i, dose, vd, theta_tvcl, t)\n  return(second_derivative)\n}\n\n\nWe now need to pay attention what R is actually returning in these functions. For all individuals we have multiple data points, so t is going to be a vector. This will propagate then throughout the calculations and the results/derivative/second_derivative object, which is being returned, will also be a vector.\n\n\n6.1.2 Function to find the mode \\(\\eta_i^*\\)\nNow we need to define two functions to reproduce Equation 68. Since finding the mode \\(\\eta_i^*\\) is an optimization problem, we need to define it’s own objective function for which we are going to numerically solve for the maximum:\n\n\n\nfunction: obj_fun_eta_i_star()\n\n# Objective function for optimization to find eta_i_star\nobj_fun_eta_i_star &lt;- function(eta_i, y_i, t, theta_tvcl, vd, dose, sigma2, omega2, f_pred) {\n  f_pred_value &lt;- f_pred(eta_i = eta_i, dose = dose, vd = vd, theta_tvcl = theta_tvcl, t = t)\n  term1 &lt;- sum((y_i - f_pred_value)^2 / (2 * sigma2))\n  term2 &lt;- eta_i^2 / (2 * omega2)\n  obj_value &lt;- term1 + term2 \n  return(obj_value)\n}\n\n\nSince our optimizer function (optim) will actually minimize the objective function, we already previously turned the objective function into a minimization problem by negation (see Section 5.4.5). In the end it doesn’t matter if we maximize the positive version of the term or if we minimize the negative version given by Equation 68. The actual optimizer is then being encoded in a separate function and takes as an argument the MAP objective function we have defined earlier:\n\n\n\nfunction: compute_eta_i_star()\n\n# Define optimizer function to find eta_i_star\ncompute_eta_i_star &lt;- function(y_i, t, theta_tvcl, vd, dose, sigma2, omega2, f_pred, eta_i_init = 0, obj_fun_eta_i_star) {\n  res &lt;- optim(\n    par = eta_i_init,\n    fn = obj_fun_eta_i_star,\n    y_i = y_i,\n    t = t,\n    theta_tvcl = theta_tvcl,\n    vd = vd,\n    dose = dose,\n    sigma2 = sigma2,\n    omega2 = omega2,\n    f_pred = f_pred,\n    method = \"BFGS\"\n  )\n  eta_i_star &lt;- res$par\n  return(eta_i_star)\n}\n\n\nPlease note that the compute_eta_i_star() function takes as arguments the functions which we have defined prior to that: f_pred() and obj_fun_eta_i_star(). Another hint: We need to find \\(\\eta_i^*\\) for each individual. That’s why we’ll apply our function compute_eta_i_star() on an individual level and it will return a single value of \\(\\eta_i^*\\). Please note that each individual has multiple observations and therefore obj_fun_eta_i_star() calculates the sum of the squared residual terms.\n\n\n6.1.3 Second derivative function\nNow we need to define an R function for the second derivative \\(g_i''(\\eta_i)\\), which is given by Equation 64. Again, we need to take the sum of the likelihood term to get a single value for each individual with multiple observations. Even with multiple observations, the prior term is only being accounted for once. This explains why with a larger number of observations the likelihood is going to dominate the prior.\n\n\n\nfunction: gi_double_prime()\n\n# Define second derivative of gi\ngi_double_prime &lt;- function(eta_i, y_i, t, theta_tvcl, vd, dose, sigma2, omega2) {\n  # Calculate f_pred, f_prime, and f_double_prime for all time points\n  f_pred_val &lt;- f_pred(eta_i, dose, vd, theta_tvcl, t)\n  f_prime_val &lt;- f_prime(eta_i, dose, vd, theta_tvcl, t)\n  f_double_prime_val &lt;- f_double_prime(eta_i, dose, vd, theta_tvcl, t)\n  \n  # Compute the numerator for each observation\n  numerator &lt;- (-f_prime_val^2 + (y_i - f_pred_val) * f_double_prime_val)\n  \n  # Divide by sigma^2 for each observation\n  term &lt;- numerator / sigma2\n  \n  # Sum the terms across all observations for the individual\n  sum_term &lt;- sum(term)\n  \n  # Subtract the term 1 / omega^2\n  second_derivative &lt;- sum_term - (1 / omega2)\n  \n  # return the second derivative\n  return(second_derivative)\n}\n\n\nFrom here on, we only need to define one last function: the objective function, which brings all the pieces together.\n\n\n6.1.4 Objective function\nWhen defining the objective function (see Equation 75), we are going to rely on all other functions defined above. Please note, that the ofv_function defined below will calculate the objective function value for a single individual. Later on we will have to loop over all individuals and sum up the OFV as defined in Equation 76.\n\n\n\nfunction: ofv_function()\n\n# Define the objective function OFV\nofv_function &lt;- function(y_i, t, theta_tvcl, vd, dose, sigma2, omega2, f_pred, gi_double_prime, compute_eta_i_star, obj_fun_eta_i_star, eta_i_init = 0, return_val = \"ofv\") {\n  \n  # Compute eta_i_star\n  eta_i_star &lt;- compute_eta_i_star(\n    y_i = y_i,\n    t = t,\n    theta_tvcl = theta_tvcl,\n    vd = vd,\n    dose = dose,\n    sigma2 = sigma2,\n    omega2 = omega2,\n    f_pred = f_pred,\n    eta_i_init = eta_i_init,\n    obj_fun_eta_i_star = obj_fun_eta_i_star\n  )\n  \n  # Compute f(eta_i_star)\n  f_pred_value &lt;- f_pred(eta_i_star, dose, vd, theta_tvcl, t)\n  \n  # Compute residual term\n  residual &lt;- y_i - f_pred_value\n  residual_squared &lt;- residual^2\n  \n  # Compute the first term: ln(sigma^2)\n  term1 &lt;- length(y_i) * log(sigma2)\n  \n  # Compute the second term: (y_i - f(eta_i_star))^2 / sigma2\n  term2 &lt;- sum(residual_squared) / sigma2\n  \n  # Compute the third term: ln(omega^2)\n  term3 &lt;- log(omega2)\n  \n  # Compute the fourth term: (eta_i_star)^2 / omega2\n  term4 &lt;- (eta_i_star^2) / omega2\n  \n  # Compute gi''(eta_i_star)\n  gi_double_prime_value &lt;- gi_double_prime(\n    eta_i = eta_i_star,\n    y_i = y_i,\n    t = t,\n    theta_tvcl = theta_tvcl,\n    vd = vd,\n    dose = dose,\n    sigma2 = sigma2,\n    omega2 = omega2\n  )\n  \n  # Compute term5: ln of absolute value of gi_double_prime_value \n  term5 &lt;- log(abs(gi_double_prime_value))\n  \n  # Sum all terms to compute the objective function value\n  ofv &lt;- term1 + term2 + term3 + term4 + term5\n  \n  # return by default ofv, but we can also request to return the eta_i_star\n  if(return_val == \"ofv\"){\n    return(ofv)\n  } else if(return_val == \"eta_i_star\"){\n    return(eta_i_star)\n  }\n}\n\n\nGreat! Now we are all set to reproduce the objective function values for each iteration."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-rrepro-ofvrepro",
    "href": "posts/understanding_nlme_estimation/index.html#sec-rrepro-ofvrepro",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "6.2 Reproducing OFVs for iterations",
    "text": "6.2 Reproducing OFVs for iterations\nLet’s start with a little recap. Our simulated concentration-time data was given by this data frame:\n\n\nCode\n# show simulated data\nsim_data_reduced |&gt; \n  select(ID, TIME, DV) |&gt; \n  head() |&gt; \n  mytbl()\n\n\n\n\n\nID\nTIME\nDV\n\n\n\n\n1\n0.01\n30.8160\n\n\n1\n3.00\n24.5520\n\n\n1\n6.00\n17.9250\n\n\n1\n12.00\n10.1100\n\n\n1\n24.00\n3.5975\n\n\n2\n0.01\n31.4550\n\n\n\n\n\nThe following .ext file contained the information about the objective function values, which we are trying to reproduce in this chapter:\n\n\nCode\n# show first row\next_file |&gt;\n  mytbl()\n\n\n\n\n\nITERATION\nCL\nV\nRUV_VAR\nIIV_VAR\nOBJ\n\n\n\n\n0\n0.100000\n3.15\n0.1\n0.1500000\n41.845437\n\n\n1\n0.168370\n3.15\n0.1\n0.4839690\n-2.743076\n\n\n2\n0.163689\n3.15\n0.1\n0.3881430\n-3.068198\n\n\n3\n0.216552\n3.15\n0.1\n0.1465500\n-11.297115\n\n\n4\n0.216552\n3.15\n0.1\n0.1465500\n-11.297115\n\n\n5\n0.253493\n3.15\n0.1\n0.0862428\n-12.418728\n\n\n6\n0.243287\n3.15\n0.1\n0.1171540\n-12.789118\n\n\n7\n0.246139\n3.15\n0.1\n0.1111950\n-12.823615\n\n\n8\n0.246715\n3.15\n0.1\n0.1099530\n-12.824589\n\n\n9\n0.246682\n3.15\n0.1\n0.1100200\n-12.824593\n\n\n10\n0.246682\n3.15\n0.1\n0.1100200\n-12.824593\n\n\n\n\n\nNow we are going to loop over all iterations and calculate the objective function values for each individual. We will then sum up the individual objective function values to get the total objective function value for each iteration.\n\n\nCode\n# define empty tibble\nofv_values &lt;- tibble(ITERATION = numeric(), OBJ_R = numeric())\n\n# define constants\nDOSE &lt;- 100\n\n# loop over iterations\nfor(iter in 1:nrow(ext_file)){\n  \n  # retrieve information from first row\n  TVCL &lt;- ext_file |&gt; slice(iter) |&gt; pull(CL)\n  VD &lt;- ext_file |&gt; slice(iter) |&gt; pull(V)\n  RUV_VAR &lt;- ext_file |&gt; slice(iter) |&gt; pull(RUV_VAR)\n  IIV_VAR &lt;- ext_file |&gt; slice(iter) |&gt; pull(IIV_VAR)\n  \n  # create empty vector for individual ofv values\n  ofv_values_iter &lt;- numeric()\n  \n  # Loop over individuals\n  for(i in unique(sim_data_reduced$ID)){\n    \n    # Filter data for individual i\n    data_i &lt;- sim_data_reduced |&gt;\n      filter(ID == i)\n    \n    # Extract individual parameters\n    y_i &lt;- data_i$DV\n    t &lt;- data_i$TIME\n    \n    # Compute the objective function value\n    ofv_i &lt;- ofv_function(\n      y_i = y_i,\n      t = t,\n      theta_tvcl = TVCL,\n      vd = VD,\n      dose = DOSE,\n      sigma2 = RUV_VAR,\n      omega2 = IIV_VAR,\n      f_pred = f_pred,\n      gi_double_prime = gi_double_prime,\n      compute_eta_i_star = compute_eta_i_star,\n      obj_fun_eta_i_star = obj_fun_eta_i_star\n    )\n    \n    # append to vector\n    ofv_values_iter &lt;- c(ofv_values_iter, ofv_i)\n  }\n  \n  # calculate sum of ofv_values\n  OFV_sum &lt;- sum(ofv_values_iter)\n  \n  # store in tibble with add row\n  ofv_values &lt;- ofv_values |&gt;\n    add_row(ITERATION = iter-1, OBJ_R = OFV_sum)\n}\n\n# add columns of ofv_values (join) to ext_file\next_file &lt;- ext_file |&gt;\n  left_join(ofv_values, by = \"ITERATION\")\n\n\nWith the last lines of code, we have joined our estimated objective function values to the .ext file and can now compare the reproduced objective function values with the original ones:\n\n\nCode\n# show ext_file\next_file |&gt; \n  mutate(DIFF = OBJ_R - OBJ) |&gt; \n  mytbl()\n\n\n\n\n\nITERATION\nCL\nV\nRUV_VAR\nIIV_VAR\nOBJ\nOBJ_R\nDIFF\n\n\n\n\n0\n0.100000\n3.15\n0.1\n0.1500000\n41.845437\n41.845437\n0.00e+00\n\n\n1\n0.168370\n3.15\n0.1\n0.4839690\n-2.743076\n-2.743092\n-1.64e-05\n\n\n2\n0.163689\n3.15\n0.1\n0.3881430\n-3.068198\n-3.068141\n5.70e-05\n\n\n3\n0.216552\n3.15\n0.1\n0.1465500\n-11.297115\n-11.297124\n-9.10e-06\n\n\n4\n0.216552\n3.15\n0.1\n0.1465500\n-11.297115\n-11.297124\n-9.10e-06\n\n\n5\n0.253493\n3.15\n0.1\n0.0862428\n-12.418728\n-12.418735\n-7.90e-06\n\n\n6\n0.243287\n3.15\n0.1\n0.1171540\n-12.789118\n-12.789115\n3.20e-06\n\n\n7\n0.246139\n3.15\n0.1\n0.1111950\n-12.823615\n-12.823615\n2.00e-07\n\n\n8\n0.246715\n3.15\n0.1\n0.1099530\n-12.824589\n-12.824589\n-2.00e-07\n\n\n9\n0.246682\n3.15\n0.1\n0.1100200\n-12.824593\n-12.824594\n-1.00e-07\n\n\n10\n0.246682\n3.15\n0.1\n0.1100200\n-12.824593\n-12.824594\n-1.00e-07\n\n\n\n\n\nIn this table, the OBJ column represents the values from the NONMEM output, while OBJ_R represents our reproduced values. We can now compare the two columns to see if we have done a good job in reproducing the objective function values!\nIn general, I would say the reproduced values match the original values quite well. There are some small differences, mainly associated to the fifth digit after the decimal point. Of course there is always the possibility that I have made a mistake in the implementation, but I rather expect the differences to be related to e.g., the difference between analytical solution of the second derivative and the numerical approximation in NONMEM."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-surface-grid",
    "href": "posts/understanding_nlme_estimation/index.html#sec-surface-grid",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "7.1 Define the grid",
    "text": "7.1 Define the grid\nIn order to get a nice surface plot, we will plot all possible combination for \\(\\theta_{TVCL}\\) and \\(\\omega^2\\) within a certain range. Here is the head of the respective grid table:\n\n\nCode\n# Define the grid\ntheta_tvcl_values &lt;- seq(0.01, 0.6, length.out = 100)\nomega2_values &lt;- seq(0.01, 0.8, length.out = 100)\ngrid &lt;- expand.grid(theta_tvcl = theta_tvcl_values, omega2 = omega2_values)\n\n# show grid\ngrid |&gt; \n  head() |&gt; \n  mytbl()\n\n\n\n\n\ntheta_tvcl\nomega2\n\n\n\n\n0.0100000\n0.01\n\n\n0.0159596\n0.01\n\n\n0.0219192\n0.01\n\n\n0.0278788\n0.01\n\n\n0.0338384\n0.01\n\n\n0.0397980\n0.01\n\n\n\n\n\nIn a next step, we can calculate the objective function value for each grid element."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-surface-gridofv",
    "href": "posts/understanding_nlme_estimation/index.html#sec-surface-gridofv",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "7.2 Calculate OFV for each grid element",
    "text": "7.2 Calculate OFV for each grid element\nNow, we will loop over all grid elements and calculate the objective function value for each combination of \\(\\theta_{TVCL}\\) and \\(\\omega^2\\). We will then store the results in a tibble and convert it to a matrix for plotting.\n\n\nCode\n# Define empty tibble for grid + OFV values\nofv_grid_values &lt;- tibble(THETA_TVCL = numeric(), OMEGA2 = numeric(), OBJ = numeric())\n\n# Loop over grid elements\nfor(i in 1:nrow(grid)){\n  \n  # Extract theta_tvcl and omega2\n  theta_tvcl &lt;- grid |&gt; slice(i) |&gt; pull(theta_tvcl)\n  omega2 &lt;- grid |&gt; slice(i) |&gt; pull(omega2)\n  \n  # create empty vector for individual ofv values\n  ofv_values_iter &lt;- numeric()\n  \n  # Loop over individuals\n  for(i in unique(sim_data_reduced$ID)){\n    \n    # Filter data for individual i\n    data_i &lt;- sim_data_reduced |&gt;\n      filter(ID == i)\n    \n    # Extract individual parameters\n    y_i &lt;- data_i$DV\n    t &lt;- data_i$TIME\n    \n    # Compute the objective function value\n    ofv_i &lt;- ofv_function(\n      y_i = y_i,\n      t = t,\n      theta_tvcl = theta_tvcl,\n      vd = VD,\n      dose = DOSE,\n      sigma2 = RUV_VAR,\n      omega2 = omega2,\n      f_pred = f_pred,\n      gi_double_prime = gi_double_prime,\n      compute_eta_i_star = compute_eta_i_star,\n      obj_fun_eta_i_star = obj_fun_eta_i_star\n    )\n    \n    # append to vector\n    ofv_values_iter &lt;- c(ofv_values_iter, ofv_i)\n  }\n  \n  # calculate sum of ofv_values\n  OFV_sum &lt;- sum(ofv_values_iter)\n  \n  # store in tibble with add row\n  ofv_grid_values &lt;- ofv_grid_values |&gt;\n    add_row(THETA_TVCL = theta_tvcl, OMEGA2 = omega2, OBJ = OFV_sum)\n}\n\n# Convert to matrix for z-values\nx_unique &lt;- unique(ofv_grid_values$THETA_TVCL)\ny_unique &lt;- unique(ofv_grid_values$OMEGA2)\nz_matrix &lt;- matrix(ofv_grid_values$OBJ, nrow = length(x_unique), byrow = TRUE)"
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#sec-surface-vis",
    "href": "posts/understanding_nlme_estimation/index.html#sec-surface-vis",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "7.3 Visualizing the surface",
    "text": "7.3 Visualizing the surface\nGreat. Now we have everything available to plot the 3D surface and to illustrate the steps which the estimation algorithm has taken to find the maximum likelihood estimate:\n\n\nCode\n# with tracer\n# Create 3D surface plot using plotly\nplot_ly(\n  x = ~x_unique,\n  y = ~y_unique,\n  z = ~z_matrix,\n  type = \"surface\",\n  cmin = -20, cmax = 60, name = \"Surface\",\n  showscale = TRUE,\n  colorbar = list(title = \"OFV\")\n) |&gt; \n  add_trace(\n    data = ext_file,\n    x = ~CL,\n    y = ~IIV_VAR,\n    z = ~OBJ,\n    type = 'scatter3d',\n    mode = 'markers',\n    marker = list(symbol = 'cross', color = 'red', size = 8),\n    name = \"NONMEM\"\n  ) |&gt;\n  add_trace(\n    data = ext_file,\n    x = ~CL,\n    y = ~IIV_VAR,\n    z = ~OBJ,\n    type = 'scatter3d',\n    mode = 'lines',\n    line = list(color = 'black', width = 3),\n    connectgaps = TRUE,\n    name = \"NONMEM\"\n  ) |&gt;\n  layout(\n    scene = list(\n      xaxis = list(title = \"Typical Clearance\"),\n      yaxis = list(title = \"IIV Variance\"),\n      zaxis = list(title = \"Objective Function Value\", range = c(-20, 60))\n    )\n  )\n\n\n\n\n\n\nWe can nicely see the surface and also the location of the maximum likelihood estimate. The red crosses represent the individual iterations of the estimation algorithm. The black line connects the individual iterations in the order they were performed. Apparently the optimization algorithm in NONMEM was quite efficient in finding a suitable path to the maximum likelihood estimate."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#modify-objective-function",
    "href": "posts/understanding_nlme_estimation/index.html#modify-objective-function",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "8.1 Modify objective function",
    "text": "8.1 Modify objective function\nTo perform the estimation entirely in R, we will encapsulate much of our previous logic into a single objective function which takes the current set of parameters as a vector (starting with the initials) and is returning the sum of the objective function across all individuals. This sum is what we will ask optim() in R to minimize by searching the parameter space. Essentially, nothing changes but we have to wrap everything in one single function which can be passed to optim().\n\n\n\nfunction: ofv_function()\n\n# define objective function for r-based parameter optimization\nparam_est_obj &lt;- function(par, \n                          sim_data_reduced,\n                          VD,\n                          DOSE,\n                          RUV_VAR,\n                          f_pred,\n                          gi_double_prime,\n                          compute_eta_i_star,\n                          obj_fun_eta_i_star) {\n  \n  \n  # Extract theta_tvcl and omega2\n  theta_tvcl &lt;- par[[1]]\n  omega2 &lt;- par[[2]]\n  \n  # create empty vector for individual ofv values\n  ofv_values_iter &lt;- numeric()\n  \n  # Loop over individuals\n  for(i in unique(sim_data_reduced$ID)){\n    \n    # Filter data for individual i\n    data_i &lt;- sim_data_reduced |&gt;\n      filter(ID == i)\n    \n    # Extract individual parameters\n    y_i &lt;- data_i$DV\n    t &lt;- data_i$TIME\n    \n    # Compute the objective function value\n    ofv_i &lt;- ofv_function(\n      y_i = y_i,\n      t = t,\n      theta_tvcl = theta_tvcl,\n      vd = VD,\n      dose = DOSE,\n      sigma2 = RUV_VAR,\n      omega2 = omega2,\n      f_pred = f_pred,\n      gi_double_prime = gi_double_prime,\n      compute_eta_i_star = compute_eta_i_star,\n      obj_fun_eta_i_star = obj_fun_eta_i_star\n    )\n    \n    # append to vector\n    ofv_values_iter &lt;- c(ofv_values_iter, ofv_i)\n  }\n  \n  # calculate sum of ofv_values\n  OFV_sum &lt;- sum(ofv_values_iter)\n  \n  # return OFV sum\n  return(OFV_sum)\n}\n\n\nGreat! Now we have the param_est_obj() function ready and it can be passed to the optimizer to start our own estimation."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#perform-estimation",
    "href": "posts/understanding_nlme_estimation/index.html#perform-estimation",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "8.2 Perform estimation",
    "text": "8.2 Perform estimation\nTo perform the estimation, we will pass the param_est_obj() function to optim function in R. For this part, we will select initial values that are close to the actual parameter estimates and additionally apply lower and upper bounds using the L-BFGS-B method. While playing around with the initial values, it became evident that the optim function and our current setup are considerably more sensitive to initial values than NONMEM. When the initial values were too far from the true estimates or when the optimizer was run without boundaries, I often encountered numerical issues, leading to termination of the estimation process. In contrast, NONMEM proved to be significantly more robust, suggesting that its optimizer is finely tuned for the pharmacometric problems we typcially face. In contrast, optim is rather a general purpose optimizer.\n\n\nCode\n# Starting guesses for theta_tvcl and omega2\ninitials &lt;- c(0.2, 0.1) \n\n# perform estimation\noptim_res &lt;- optim(\n  par     = initials,\n  fn      = param_est_obj,\n  sim_data_reduced = sim_data_reduced,\n  VD      = ext_file$V |&gt; unique(),\n  DOSE    = DOSE,\n  RUV_VAR = ext_file$RUV_VAR |&gt; unique(),\n  f_pred  = f_pred,\n  gi_double_prime         = gi_double_prime,\n  compute_eta_i_star= compute_eta_i_star,\n  obj_fun_eta_i_star= obj_fun_eta_i_star,\n  method  = \"L-BFGS-B\",\n  lower   = rep(0.001, length(initials)),\n  upper   = rep(1000, length(initials)),\n  hessian = TRUE,\n  control = list(\n    maxit = 10000,           \n    factr = 1e-7,           \n    pgtol = 1e-7,           \n    trace = 1,              \n    REPORT = 1              \n  )\n)\n\n\niter    1 value -12.819995\niter    2 value -12.822838\niter    3 value -12.824104\niter    4 value -12.824484\niter    5 value -12.824594\niter    6 value -12.824594\niter    7 value -12.824594\niter    8 value -12.824594\nfinal  value -12.824594 \nconverged\n\n\nThe optimization successfully converged and we can also see the trace of the optimization problem. The optim_res object now contains all the information we need about the optimization:\n\n\nCode\n# show optim_res object\noptim_res\n\n\n$par\n[1] 0.2466737 0.1100256\n\n$value\n[1] -12.82459\n\n$counts\nfunction gradient \n      16       16 \n\n$convergence\n[1] 0\n\n$message\n[1] \"CONVERGENCE: NORM OF PROJECTED GRADIENT &lt;= PGTOL\"\n\n$hessian\n              [,1]          [,2]\n[1,]  2.979341e+03  -0.003861313\n[2,] -3.861313e-03 821.681234002\n\n\nGreat! We can now compare the parameter estimates against the NONMEM estimates to see how well we were doing."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#comparison-against-nonmem",
    "href": "posts/understanding_nlme_estimation/index.html#comparison-against-nonmem",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "8.3 Comparison against NONMEM",
    "text": "8.3 Comparison against NONMEM\nTo compare our results against NONMEM, we will use the final iteration from the .ext file to extract the final parameter estimates. We will then incorporate the information obtained from the optim object in R, allowing us to directly compare the two sets of parameter estimates.\n\n\nCode\n# retrieve final parameter estimates from NM\nnm_par_est &lt;- ext_file |&gt; \n  filter(ITERATION == max(ext_file$ITERATION)) |&gt; \n  select(CL, IIV_VAR, OBJ) |&gt; \n  rename(\n    \"CL_NM\" = CL,\n    \"IIV_VAR_NM\" = IIV_VAR,\n    \"OBJ_NM\" = OBJ\n  ) |&gt; \n  mutate(\n    CL_R = optim_res$par[[1]],\n    IIV_VAR_R = optim_res$par[[2]],\n    OBJ_R = optim_res$value,\n    CL_DIFF = CL_R - CL_NM,\n    IIV_VAR_DIFF = IIV_VAR_R - IIV_VAR_NM,\n    OBJ_DIFF = OBJ_NM - OBJ_R\n  ) |&gt; \n  select(\n    CL_NM, CL_R, CL_DIFF, IIV_VAR_NM, IIV_VAR_R, IIV_VAR_DIFF, OBJ_NM, OBJ_R, OBJ_DIFF\n  )\n\n# show\nnm_par_est |&gt; \n  mytbl()\n\n\n\n\n\nCL_NM\nCL_R\nCL_DIFF\nIIV_VAR_NM\nIIV_VAR_R\nIIV_VAR_DIFF\nOBJ_NM\nOBJ_R\nOBJ_DIFF\n\n\n\n\n0.246682\n0.2466737\n-8.3e-06\n0.11002\n0.1100256\n5.6e-06\n-12.82459\n-12.82459\n3e-07\n\n\n\n\n\nOur R-based implementation achieves convergence to parameter estimates that are very similar to those obtained with NONMEM."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#retrieving-the-hessian-matrix",
    "href": "posts/understanding_nlme_estimation/index.html#retrieving-the-hessian-matrix",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "9.1 Retrieving the Hessian matrix",
    "text": "9.1 Retrieving the Hessian matrix\nWe can directly retrieve the hessian matrix from the optim_res object by assessing it via $hessian. Please note, that we did request the hessian during optimization with hessian = TRUE. This is the output:\n\n\nCode\n# store hessian\nhessian &lt;- optim_res$hessian\n\n# show\nhessian\n\n\n              [,1]          [,2]\n[1,]  2.979341e+03  -0.003861313\n[2,] -3.861313e-03 821.681234002"
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#solving-the-matrix",
    "href": "posts/understanding_nlme_estimation/index.html#solving-the-matrix",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "9.2 Solving the matrix",
    "text": "9.2 Solving the matrix\nWe can now go ahead and solve the matrix to get its inverse. The inverse of the Hessian serves as an approximation to the covariance matrix.\n\n\nCode\n# Standard errors (square root of variances)\ninverse_hessian &lt;- optim_res$hessian |&gt; solve()\n\n# show inverse hessian\ninverse_hessian\n\n\n             [,1]         [,2]\n[1,] 3.356447e-04 1.577289e-09\n[2,] 1.577289e-09 1.217017e-03\n\n\nWe are mainly interested in the variances for now, so we only care about the diagonals. We are going to retrieve the diagonals and store them alongside the parameter estimates inside a tibble:\n\n\nCode\n# get diagonal elements\nvariances_vec &lt;- diag(inverse_hessian)\n\n# define row\nrow1 &lt;- tibble(\n  PAR = \"TVCL\",\n  EST_R = optim_res$par[[1]],\n  VAR_R = variances_vec[[1]]\n)\n\n# define row\nrow2 &lt;- tibble(\n  PAR = \"IIV_VAR\",\n  EST_R = optim_res$par[[2]],\n  VAR_R = variances_vec[[2]]\n)\n\n# bind rows for parameter precicsion table\npar_prec &lt;- bind_rows(row1, row2)\n\n# show\npar_prec |&gt; \n  mytbl()\n\n\n\n\n\nPAR\nEST_R\nVAR_R\n\n\n\n\nTVCL\n0.2466737\n0.0003356\n\n\nIIV_VAR\n0.1100256\n0.0012170\n\n\n\n\n\nWe now have the parameter estimates and the associated variances stored in a single tibble."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#calculating-standard-errors",
    "href": "posts/understanding_nlme_estimation/index.html#calculating-standard-errors",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "9.3 Calculating standard errors",
    "text": "9.3 Calculating standard errors\nNow we can go ahead to calculate the standard error. The standard errors are the square root of the variances, so we can add a column with SE_R:\n\n\nCode\n# calculate standard errors\npar_prec &lt;- par_prec |&gt; \n  mutate(\n    SE_R = sqrt(VAR_R)\n  )\n\n# show\npar_prec |&gt; \n  mytbl()\n\n\n\n\n\nPAR\nEST_R\nVAR_R\nSE_R\n\n\n\n\nTVCL\n0.2466737\n0.0003356\n0.0183206\n\n\nIIV_VAR\n0.1100256\n0.0012170\n0.0348858\n\n\n\n\n\nFinally, the relative standard errors (RSE) can be calculated based on the standard error and the estimate itself. We are going to add RSE%_R to the tibble:\n\n\nCode\n# Calculate relative standard error\npar_prec &lt;- par_prec |&gt; \n  mutate(\"RSE%_R\" = (SE_R / EST_R)*100)\n\n# show table\npar_prec |&gt; \n  mytbl()\n\n\n\n\n\nPAR\nEST_R\nVAR_R\nSE_R\nRSE%_R\n\n\n\n\nTVCL\n0.2466737\n0.0003356\n0.0183206\n7.427062\n\n\nIIV_VAR\n0.1100256\n0.0012170\n0.0348858\n31.706964\n\n\n\n\n\nApparently, the parameter precision for the typical value of clearance is much higher than the variance estimate for the inter-individual variability. However, both estimates would be within an acceptable range and parameter precision would not be a particular concern."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#comparison-against-nonmem-1",
    "href": "posts/understanding_nlme_estimation/index.html#comparison-against-nonmem-1",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "9.4 Comparison against NONMEM",
    "text": "9.4 Comparison against NONMEM\nIn the previous steps, we have assessed the parameter precision of our R-based optimization run. Now, we want to compare the results against our reference solution. In a first step, we are going to load in the data from the .cov file from NONMEM, which contains the variance-covariance matrix of the parameter estimates from the estimation step\n\n\nCode\n# load nonmem .cov output\nnm_cov &lt;- nonmem2R::covload(\n  model = paste0(base_path, \"models/estimation/1cmt_iv_est.cov\"), \n  theta.only = FALSE\n) |&gt; \n  as_tibble()\n\n# show\nnm_cov |&gt; \n  mytbl()\n\n\n\n\n\nTHETA1\nTHETA2\nOMEGA.1.1.\nSIGMA.1.1.\n\n\n\n\n0.0006715\n0\n0.0007977\n0\n\n\n0.0000000\n0\n0.0000000\n0\n\n\n0.0007977\n0\n0.0023749\n0\n\n\n0.0000000\n0\n0.0000000\n0\n\n\n\n\n\nThis is a representation of the covariance matrix from NONMEM. We are now going to perform the same steps as done above and then augment the results with the R-based results to compare both results.\n\n\nCode\n# augment table\npar_prec &lt;- par_prec |&gt; \n  mutate(\n    EST_NM = c(nm_par_est$CL_NM, nm_par_est$IIV_VAR_NM),\n    VAR_NM = c(nm_cov$THETA1[[1]], nm_cov$OMEGA.1.1.[[3]]),\n    SE_NM = sqrt(VAR_NM),\n    \"RSE%_NM\" = (SE_NM/EST_NM)*100,\n    \"RSE%_DIFF\" = `RSE%_R` - `RSE%_NM`\n  ) |&gt; \n  select(PAR, EST_R, EST_NM, VAR_R, VAR_NM, SE_R, SE_NM, `RSE%_R`, `RSE%_NM`, `RSE%_DIFF`)\n\n# show table\npar_prec |&gt; \n  mytbl()\n\n\n\n\n\nPAR\nEST_R\nEST_NM\nVAR_R\nVAR_NM\nSE_R\nSE_NM\nRSE%_R\nRSE%_NM\nRSE%_DIFF\n\n\n\n\nTVCL\n0.2466737\n0.246682\n0.0003356\n0.0006715\n0.0183206\n0.0259135\n7.427062\n10.50480\n-3.077739\n\n\nIIV_VAR\n0.1100256\n0.110020\n0.0012170\n0.0023749\n0.0348858\n0.0487325\n31.706964\n44.29425\n-12.587288\n\n\n\n\n\nWith our R-based reproduction, we can capture the general trend of higher parameter imprecision for $OMEGA estimates. However, we are unable to replicate the exact numerical values from the NONMEM output. The differences in relative standard errors (RSEs) are substantial, with R showing a consistent bias toward lower RSEs compared to NONMEM. It remains unclear whether these discrepancies indicate an error in my implementation or if they can be attributed solely to differences in how the Hessian matrix is approximated in the maximum likelihood estimate. I personally lack enough knowledge about the specific numerical approximation methods used by NONMEM versus the optim function in R, and I am unsure whether such differences are expected or not."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#reading-in-nonmems-individual-etas",
    "href": "posts/understanding_nlme_estimation/index.html#reading-in-nonmems-individual-etas",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "10.1 Reading in NONMEMs individual ETAs",
    "text": "10.1 Reading in NONMEMs individual ETAs\nIn a first step, we are going to read in the sdtab which contains the individual realization of the random variable, ETA1. We are then only keeping one distinct row per ID.\n\n\nCode\n# load nonmem output with posthoc ETA\nsdtab &lt;- read_nm_table(paste0(base_path, \"models/estimation/estim_out\"))\n\n# summarise nonmem eta_i values\nnm_eta_i_star_values &lt;- sdtab |&gt; \n  select(ID, ETA1) |&gt; \n  distinct() |&gt; \n  rename(ETA1_NM = ETA1)\n\n# show table\nnm_eta_i_star_values |&gt; \n  mytbl()\n\n\n\n\n\nID\nETA1_NM\n\n\n\n\n1\n0.173900\n\n\n2\n-0.110320\n\n\n3\n-0.043933\n\n\n4\n-0.153930\n\n\n5\n0.725750\n\n\n6\n-0.033077\n\n\n7\n-0.349690\n\n\n8\n-0.427840\n\n\n9\n-0.183560\n\n\n10\n0.402510"
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#reproducing-ebe-map-estimates",
    "href": "posts/understanding_nlme_estimation/index.html#reproducing-ebe-map-estimates",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "10.2 Reproducing EBE / MAP estimates",
    "text": "10.2 Reproducing EBE / MAP estimates\nNow it is time to reproduce the Empirical Bayes Estimates (EBE) or Maximum A Posteriori (MAP) step using the custom function we defined earlier when constructing the objective function. For this, we will use the final parameter estimates obtained at convergence during our estimation process.\n\n\nCode\n# define iter as maximum \niter &lt;- nrow(ext_file)\n\n# retrieve information from first row\nTVCL &lt;- ext_file |&gt; slice(iter) |&gt; pull(CL)\nVD &lt;- ext_file |&gt; slice(iter) |&gt; pull(V)\nRUV_VAR &lt;- ext_file |&gt; slice(iter) |&gt; pull(RUV_VAR)\nIIV_VAR &lt;- ext_file |&gt; slice(iter) |&gt; pull(IIV_VAR)\n\n# define constants\nDOSE &lt;- 100\n\n# define initials\neta_i_init &lt;- 0\n\n# create empty list for individual eta_i_star values\neta_i_star_list &lt;- list()\n\n# Loop over individuals\nfor(i in unique(sim_data_reduced$ID)){\n  \n  # Filter data for individual i\n  data_i &lt;- sim_data_reduced |&gt;\n    filter(ID == i)\n  \n  # Extract individual parameters\n  y_i &lt;- data_i$DV\n  t &lt;- data_i$TIME\n\n  # compute eta_i_star  \n  eta_i_star &lt;- compute_eta_i_star(\n    y_i = y_i,\n    t = t,\n    theta_tvcl = TVCL,\n    vd = VD,\n    dose = DOSE,\n    sigma2 = RUV_VAR,\n    omega2 = IIV_VAR,\n    f_pred = f_pred,\n    eta_i_init = eta_i_init,\n    obj_fun_eta_i_star = obj_fun_eta_i_star\n  )\n  \n  # append to vector\n  eta_i_star_list[[i]] &lt;- tibble(\n    ID = i,\n    ETA1_R = eta_i_star\n  )\n}\n\n# store in tibble with add row\neta_i_star_values &lt;-  bind_rows(eta_i_star_list)\n\n# show the results\neta_i_star_values |&gt; \n  mytbl()\n\n\n\n\n\nID\nETA1_R\n\n\n\n\n1\n0.1738965\n\n\n2\n-0.1103184\n\n\n3\n-0.0439348\n\n\n4\n-0.1539363\n\n\n5\n0.7257483\n\n\n6\n-0.0330794\n\n\n7\n-0.3496945\n\n\n8\n-0.4278383\n\n\n9\n-0.1835616\n\n\n10\n0.4025086"
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#merging-datasets-and-comparison",
    "href": "posts/understanding_nlme_estimation/index.html#merging-datasets-and-comparison",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "10.3 Merging datasets and comparison",
    "text": "10.3 Merging datasets and comparison\nNow that we have the estimates ready for both NONMEM and R, we can compare both and calculate the difference.\n\n\nCode\n# left_join\neta_i_star_values &lt;- eta_i_star_values |&gt; \n  left_join(nm_eta_i_star_values, by = \"ID\") |&gt; \n  mutate(DIFF = ETA1_R - ETA1_NM)\n\n# show tibble\neta_i_star_values |&gt; \n  mytbl()\n\n\n\n\n\nID\nETA1_R\nETA1_NM\nDIFF\n\n\n\n\n1\n0.1738965\n0.173900\n-3.5e-06\n\n\n2\n-0.1103184\n-0.110320\n1.6e-06\n\n\n3\n-0.0439348\n-0.043933\n-1.8e-06\n\n\n4\n-0.1539363\n-0.153930\n-6.3e-06\n\n\n5\n0.7257483\n0.725750\n-1.7e-06\n\n\n6\n-0.0330794\n-0.033077\n-2.4e-06\n\n\n7\n-0.3496945\n-0.349690\n-4.5e-06\n\n\n8\n-0.4278383\n-0.427840\n1.7e-06\n\n\n9\n-0.1835616\n-0.183560\n-1.6e-06\n\n\n10\n0.4025086\n0.402510\n-1.4e-06\n\n\n\n\n\nThe differences between the two implementations are tiny, only showing up in distant decimal places. This gives me confidence that our MAP objective function is working as it should."
  },
  {
    "objectID": "posts/understanding_nlme_estimation/index.html#footnotes",
    "href": "posts/understanding_nlme_estimation/index.html#footnotes",
    "title": "My attempt to understand the NLME estimation algorithm behind NONMEM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe only need to approximate \\(g_i()\\) if we cannot explicitly calculate that integral. If we would be in a linear setting (which we are not), we wouldn’t need any approximation and could directly work with it.↩︎\nRemember: We actually deal with a latent, unobserved variable.↩︎\nThey are defined for the case of \\(\\eta_i^*\\), but here we need the more general form of \\(\\eta_i\\). But the expression itself is the same.↩︎"
  },
  {
    "objectID": "posts/command_line_chatgpt/index.html",
    "href": "posts/command_line_chatgpt/index.html",
    "title": "Using ChatGPT from the command line",
    "section": "",
    "text": "Large language models (LLMs) are sneaking more and more into our daily workflows. There is surely a debate whether they deserve the hype, but in general they seem to be quite useful for low-risk but repetitive tasks that tend to eat up our time and focus. While we can use the graphical user interface to run these models, a more effective and much more powerful way is to run them directly from the command line using R or Python. By doing so we can supercharge their usefulness and create tailored, automated solutions for our specific solutions without the pain of clicking through a web interface. I have recently digged a bit into this topic and I am surely still scratching the surface of what is possible. In this post, I’ll simply walk through my first steps using ChatGPT from the command line in Python, and highlight a few features/funcitons that I found helpful. In the end, we will have a very simplistic agentic workflow that is able to extract structured information from some NONMEM model code. Sounds good? Let’s get started!\n\n\n\nOpenAI provides official Software Development Kits (SDKs) for interacting with their API in several programming languages, such as JavaScript and Python. These SDKs make sending requests and processing responses relatively painless and have the official support of OpenAI. While Python is not super popular in our pharmacometric community and also I am personally much more more comfortable in R, I chose Python for these kind of tasks. The existence of a well-maintained, well-documented and ready-to-use SDK simply makes our lives much easier. You can interact with their API directly via raw HTTP requests, but I guess that is just painful and comes with a lot of overhead.\n\n\n\nA snake, not Python.\n\n\nFor those wanting to stick to R, there’s also the promising ellmer package which has been recently released. It integrates LLM access directly into the tidyverse ecosystem (which we all love). I haven’t tried it out yet, but it could become a very attractive option for R users. Currently they are in version 0.3.0 and I guess there is still a lot of development happening, but with the support of Posit and the tidyverse development team it will surely be a great option in the future! Feel free to comment if you have gathered some experience."
  },
  {
    "objectID": "posts/command_line_chatgpt/index.html#motivation",
    "href": "posts/command_line_chatgpt/index.html#motivation",
    "title": "Using ChatGPT from the command line",
    "section": "",
    "text": "Large language models (LLMs) are sneaking more and more into our daily workflows. There is surely a debate whether they deserve the hype, but in general they seem to be quite useful for low-risk but repetitive tasks that tend to eat up our time and focus. While we can use the graphical user interface to run these models, a more effective and much more powerful way is to run them directly from the command line using R or Python. By doing so we can supercharge their usefulness and create tailored, automated solutions for our specific solutions without the pain of clicking through a web interface. I have recently digged a bit into this topic and I am surely still scratching the surface of what is possible. In this post, I’ll simply walk through my first steps using ChatGPT from the command line in Python, and highlight a few features/funcitons that I found helpful. In the end, we will have a very simplistic agentic workflow that is able to extract structured information from some NONMEM model code. Sounds good? Let’s get started!"
  },
  {
    "objectID": "posts/command_line_chatgpt/index.html#why-python",
    "href": "posts/command_line_chatgpt/index.html#why-python",
    "title": "Using ChatGPT from the command line",
    "section": "",
    "text": "OpenAI provides official Software Development Kits (SDKs) for interacting with their API in several programming languages, such as JavaScript and Python. These SDKs make sending requests and processing responses relatively painless and have the official support of OpenAI. While Python is not super popular in our pharmacometric community and also I am personally much more more comfortable in R, I chose Python for these kind of tasks. The existence of a well-maintained, well-documented and ready-to-use SDK simply makes our lives much easier. You can interact with their API directly via raw HTTP requests, but I guess that is just painful and comes with a lot of overhead.\n\n\n\nA snake, not Python.\n\n\nFor those wanting to stick to R, there’s also the promising ellmer package which has been recently released. It integrates LLM access directly into the tidyverse ecosystem (which we all love). I haven’t tried it out yet, but it could become a very attractive option for R users. Currently they are in version 0.3.0 and I guess there is still a lot of development happening, but with the support of Posit and the tidyverse development team it will surely be a great option in the future! Feel free to comment if you have gathered some experience."
  },
  {
    "objectID": "posts/command_line_chatgpt/index.html#creating-an-account",
    "href": "posts/command_line_chatgpt/index.html#creating-an-account",
    "title": "Using ChatGPT from the command line",
    "section": "Creating an account",
    "text": "Creating an account\nIf we want to access ChatGPT from the command line, we first need an OpenAI account. We can simply head to the OpenAI platform and create an account. So far, creating an account is free. However, once we start sending prompts via our API key, usage will be billed. For our use-cases this will just be a couple of cents, and you can always check the expected billing in your dashboard."
  },
  {
    "objectID": "posts/command_line_chatgpt/index.html#retrieve-your-api-key",
    "href": "posts/command_line_chatgpt/index.html#retrieve-your-api-key",
    "title": "Using ChatGPT from the command line",
    "section": "Retrieve your API key",
    "text": "Retrieve your API key\nIf we want to use ChatGPT from the command line, we will have to authenticate ourselves so that OpenAI knows whom to bill. An API key is a long, random string used to authenticate and authorize access to the API. Think of it like a password (and handle it with the same care to avoid excessive bills from some stranger). Since every prompt we send incurs a (usually small) cost, we don’t want to share our API keys with anyone. Once we’ve successfully created the account, we can now generate an API key at this page of the dashboard.\n\n\n\nA screenshot of the API keys dashboard.\n\n\nFrom the screenshot, you can see that I have an API key for my laptop starting with sk-....YLUA. Now we have to make it accessible to Python, so we can authenticate ourselves when making requests."
  },
  {
    "objectID": "posts/command_line_chatgpt/index.html#saving-your-api-key-as-an-environmental-variable",
    "href": "posts/command_line_chatgpt/index.html#saving-your-api-key-as-an-environmental-variable",
    "title": "Using ChatGPT from the command line",
    "section": "Saving your API key as an environmental variable",
    "text": "Saving your API key as an environmental variable\nWe want to avoid typing the API key each time or hard-coding it into a script. Hard-coding it would be like pasting a password in plain text or putting it on a post-it somewhere. A much safer approach is to store the API key as an environment variable (and there are even safer options). On Windows, we can do this by searching for System environment variables in the Windows search bar or we simply use PowerShell to set it:\n\n\n\npowershell\n\nsetx OPENAI_API_KEY \"your_api_key_here\"\n\n\nWe have to make sure to name the environment variable exactly OPENAI_API_KEY. The openai Python package looks for this specific name when we initialize it to link the prompts to our account. Very nice, our API key is ready to be used!"
  },
  {
    "objectID": "posts/command_line_chatgpt/index.html#setting-up-python",
    "href": "posts/command_line_chatgpt/index.html#setting-up-python",
    "title": "Using ChatGPT from the command line",
    "section": "Setting up Python",
    "text": "Setting up Python\nI won’t go into the details of installing Python itself, as there are many excellent tutorials out there for that. Of course we have to take care of some packages and dependencies, and we install it with Python’s package installer pip:\n\n\n\npowershell\n\npip install openai\n\n\nAnd that’s it! We’ve now installed the OpenAI package and set up our API key as an environment variable. Next, we can move on to sending our first prompt."
  },
  {
    "objectID": "posts/about_this_blog/index.html",
    "href": "posts/about_this_blog/index.html",
    "title": "About this blog",
    "section": "",
    "text": "In pharmacometrics, I’ve found that some concepts are either poorly documented or explained in such technical language that they become hard to grasp (at least for me). This can be quite frustrating when you’re trying to learn or understand something new for which you don’t have a suitable mathematical and statistical background.\nLuckily, there are many publicly available resources, such as blogs, podcast and websites, that explain these topics in a more accessible way or generally share knowledge about pharmacometrics. Some of my favorites include:\n\nDanielle Navarro’s Blog\nClinical Pharmacology Podcast\nmrgsolve blog\nPMX Solutions Tutorials\nTingjie Guo’s NMHelp\n\nFor instance, Danielle Navarro’s post on population pharmacokinetic models in stan was incredibly helpful in my understanding of implementing pop-PK models in Stan. She just explained the concepts in a very easy-to-understand way and it is at the same time quite entertaining to read. Please check out her blog, if you haven’t already!\nI often explain new concepts to myself using Quarto or RMarkdown documents and I thought I might as well share them as little blog posts. While I don’t expect to match the quality of the blogs I’ve mentioned, I still hope that some posts can be helpful to others who are tackling similar challenges."
  },
  {
    "objectID": "posts/about_this_blog/index.html#motivation",
    "href": "posts/about_this_blog/index.html#motivation",
    "title": "About this blog",
    "section": "",
    "text": "In pharmacometrics, I’ve found that some concepts are either poorly documented or explained in such technical language that they become hard to grasp (at least for me). This can be quite frustrating when you’re trying to learn or understand something new for which you don’t have a suitable mathematical and statistical background.\nLuckily, there are many publicly available resources, such as blogs, podcast and websites, that explain these topics in a more accessible way or generally share knowledge about pharmacometrics. Some of my favorites include:\n\nDanielle Navarro’s Blog\nClinical Pharmacology Podcast\nmrgsolve blog\nPMX Solutions Tutorials\nTingjie Guo’s NMHelp\n\nFor instance, Danielle Navarro’s post on population pharmacokinetic models in stan was incredibly helpful in my understanding of implementing pop-PK models in Stan. She just explained the concepts in a very easy-to-understand way and it is at the same time quite entertaining to read. Please check out her blog, if you haven’t already!\nI often explain new concepts to myself using Quarto or RMarkdown documents and I thought I might as well share them as little blog posts. While I don’t expect to match the quality of the blogs I’ve mentioned, I still hope that some posts can be helpful to others who are tackling similar challenges."
  },
  {
    "objectID": "posts/about_this_blog/index.html#disclaimers",
    "href": "posts/about_this_blog/index.html#disclaimers",
    "title": "About this blog",
    "section": "Disclaimers",
    "text": "Disclaimers\nBefore diving in, a few things to note:\n\nI’m really not an expert in everything I write about; I’m simply learning as I go while documenting my process. So please expect some mistakes along the way!\nEnglish is not my first language, so I rely on tools like ChatGPT and DeepL to help with writing. I also use tools like ChatGPT to generate ideas or to find good ways to visualize concepts.\nI do my best to give credit and cite sources when appropriate. Much of what I share builds on others’ work, and I’m not reinventing the wheel. However, a blog post shouldn’t become a scientific paper, so I might not always provide a full reference list. If you feel that I haven’t given proper credit, please let me know."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Marian Klose",
    "section": "",
    "text": "Hi there! I’m Marian Klose, currently a PhD student at Freie Universitaet Berlin, and part of the PharMetrX graduate research training program.\nHow did I end up here? During my pharmacy studies I got into pharmacometrics pretty early on and loved the mix of computer science, pharmacy, and math. After my graduation, I headed to Florida for six months to work at the University of Florida’s Center for Pharmacometrics and Systems Pharmacology, and also did a three-month internship at Boehringer Ingelheim to see if my interest would remain steady. And I was still fascinated by pharmacometrics, so I decided to start my PhD and applied to the PharMetrX program — and here I am today!\nThankfully, there are many helpful blogs and articles available that break down complex topics into simpler, more accessible concepts. Whenever I come across a concept I don’t fully understand (which happens quite often), I try to explain it to myself using a Quarto or RMarkdown document. Since I’m anyways creating these documents for myself, I thought I might as well share them as little blog posts. My goal really isn’t to produce perfectly polished content, but rather to document my own learning process and potentially help others who might be facing the same challenges."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Marian Klose",
    "section": "",
    "text": "Hi there! I’m Marian Klose, currently a PhD student at Freie Universitaet Berlin, and part of the PharMetrX graduate research training program.\nHow did I end up here? During my pharmacy studies I got into pharmacometrics pretty early on and loved the mix of computer science, pharmacy, and math. After my graduation, I headed to Florida for six months to work at the University of Florida’s Center for Pharmacometrics and Systems Pharmacology, and also did a three-month internship at Boehringer Ingelheim to see if my interest would remain steady. And I was still fascinated by pharmacometrics, so I decided to start my PhD and applied to the PharMetrX program — and here I am today!\nThankfully, there are many helpful blogs and articles available that break down complex topics into simpler, more accessible concepts. Whenever I come across a concept I don’t fully understand (which happens quite often), I try to explain it to myself using a Quarto or RMarkdown document. Since I’m anyways creating these documents for myself, I thought I might as well share them as little blog posts. My goal really isn’t to produce perfectly polished content, but rather to document my own learning process and potentially help others who might be facing the same challenges."
  },
  {
    "objectID": "cv/cv.html",
    "href": "cv/cv.html",
    "title": "CV",
    "section": "",
    "text": "2023 – Present\n\n\nDoctoral Studies\n\n\nPharMetrX program\n\n\n\n\n2022\n\n\nLicense to Practise as a Pharmacist\n\n\nLaGeSo, Berlin\n\n\n\n\n2017 – 2021\n\n\nPharmacy Studies (8 semesters)\n\n\nFreie Universität Berlin\n\n\n\n\n2015 – 2017\n\n\nChemistry and Biochemistry Studies (3 semesters)\n\n\nFU and HU Berlin\n\n\n\n\n2006 – 2014\n\n\nSecondary School\n\n\nSt. Ursula Gymnasium Attendorn"
  },
  {
    "objectID": "cv/cv.html#education",
    "href": "cv/cv.html#education",
    "title": "CV",
    "section": "",
    "text": "2023 – Present\n\n\nDoctoral Studies\n\n\nPharMetrX program\n\n\n\n\n2022\n\n\nLicense to Practise as a Pharmacist\n\n\nLaGeSo, Berlin\n\n\n\n\n2017 – 2021\n\n\nPharmacy Studies (8 semesters)\n\n\nFreie Universität Berlin\n\n\n\n\n2015 – 2017\n\n\nChemistry and Biochemistry Studies (3 semesters)\n\n\nFU and HU Berlin\n\n\n\n\n2006 – 2014\n\n\nSecondary School\n\n\nSt. Ursula Gymnasium Attendorn"
  },
  {
    "objectID": "cv/cv.html#work-experience",
    "href": "cv/cv.html#work-experience",
    "title": "CV",
    "section": "Work Experience",
    "text": "Work Experience\n\n\nOct 22 – Dec 22\n\n\nIntern\nWorked on automated evaluation and reporting of NLME modelling runs.\n\n\nBoehringer Ingelheim, TMCP Data Science\n\n\n\n\nNov 21 – Apr 22\n\n\nShort-term Scholar\nPerformed PBPK modeling and simulation to explore drug-drug and gene-drug interactions of oxycodone.\n\n\nCPSP, University of Florida\n\n\n\n\nMay 21 – Oct 21\n\n\nPre-registration Pharmacist\nInternship in a community pharmacy.\n\n\nMedios Apotheke an der Charité, Berlin\n\n\n\n\nJul 20 – Mar 20\n\n\nTeaching Assistant\nSupported lab exercises and oral exams in “Medical Microbiology, Virology, and Infection Prevention.”\n\n\nCharité University Hospital, Berlin\n\n\n\n\nDec 17 – Dec 19\n\n\nStudent Assistant\nCoordinated aspects of the PharMetrX anniversary celebration and provided administrative support.\n\n\nPharMetrX program\n\n\n\n\nAug 19 – Sep 19\n\n\nIntern\nEvaluation of a risk-assessment tool for meropenem administration in critically ill patients.\n\n\nPharMetrX program\n\n\n\n\nSep 18 – Oct 18\n\n\nIntern\nContributed to a white paper on challenges and opportunities of modeling and simulation in pediatric type 2 diabetes.\n\n\nSanofi"
  },
  {
    "objectID": "cv/cv.html#miscellaneous",
    "href": "cv/cv.html#miscellaneous",
    "title": "CV",
    "section": "Miscellaneous",
    "text": "Miscellaneous\n\n\n2018 – 2021\n\n\nMember of the Institutes and Students Council\nRepresented student interests.\n\n\nFreie Universität Berlin\n\n\n\n\n2017 – 2021\n\n\nElected Speaker of the Semester and Treasurer\nOrganized semester affairs and finances; mediated between students and lecturers.\n\n\nFreie Universität Berlin\n\n\n\n\n2014 – 2015\n\n\nWork and Travel\n12 months of work and travel.\n\n\nAustralia, New Zealand, Singapore"
  },
  {
    "objectID": "cv/cv.html#language-skills",
    "href": "cv/cv.html#language-skills",
    "title": "CV",
    "section": "Language Skills",
    "text": "Language Skills\n\n\nGerman\n\n\nNative\n\n\n\n\nEnglish\n\n\nFluent"
  },
  {
    "objectID": "cv/cv.html#computer-skills",
    "href": "cv/cv.html#computer-skills",
    "title": "CV",
    "section": "Computer Skills",
    "text": "Computer Skills\n\n\nMicrosoft Office\n\n\nIntermediate\n\n\n\n\nLaTeX\n\n\nIntermediate\n\n\n\n\nGastroPlus\n\n\nIntermediate\n\n\n\n\nNONMEM\n\n\nIntermediate\n\n\n\n\nR\n\n\nIntermediate\n\n\n\n\nPython\n\n\nBeginner"
  },
  {
    "objectID": "cv/cv.html#courses-and-training",
    "href": "cv/cv.html#courses-and-training",
    "title": "CV",
    "section": "Courses and Training",
    "text": "Courses and Training\n\n\n2025\n\n\nNonlinear Mixed Effects Modeling Workflows using Pumas\n\n\n\n\n2023 – 2024\n\n\nPharMetrX A1 – A5 Modules\n\n\n\n\n2022\n\n\nPharmacometrics Spring School – Modeling Using MonolixSuite (Lixoft)\n\n\n\n\n2022\n\n\nMATLAB/SimBiology Workshop – Introduction to PBPK Modeling with SimBiology\n\n\n\n\n2021\n\n\nPython – Data Science, Machine Learning & Neural Networks (Udemy)"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Using ChatGPT from the command line\n\n\n\nChatGPT\n\nopenai\n\nLLM\n\n\n\nSupercharging LLMs by building agentic workflows.\n\n\n\nMarian Klose\n\n\nJul 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nReproducing NONMEM’s MAP estimation in R\n\n\n\nNONMEM\n\nBayes\n\nMAP\n\nEBE\n\nEstimation\n\n\n\nHow to perform Bayesian MAP estimation yourself using R and why you should care.\n\n\n\nMarian Klose\n\n\nMar 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy attempt to understand the NLME estimation algorithm behind NONMEM\n\n\n\nEstimation\n\nNONMEM\n\nLaplacian\n\nR\n\n\n\nAn R-based reproduction using straight-line equations.\n\n\n\nMarian Klose\n\n\nJan 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nExpressing RUV as $THETA in NONMEM\n\n\n\nRUV\n\nError\n\nNONMEM\n\n\n\nHave you ever been confused why some people use the $THETA block to code their RUV in NONMEM? You are not alone!\n\n\n\nMarian Klose\n\n\nDec 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbout this blog\n\n\n\nMotivation\n\nDisclaimer\n\n\n\nSome motivations and disclaimers\n\n\n\nMarian Klose\n\n\nDec 5, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/bayes_map_estimation_in_r/index.html",
    "href": "posts/bayes_map_estimation_in_r/index.html",
    "title": "Reproducing NONMEM’s MAP estimation in R",
    "section": "",
    "text": "Code\n# load packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr) \nlibrary(kableExtra)\nlibrary(xpose4)\nlibrary(tidyr)"
  },
  {
    "objectID": "posts/bayes_map_estimation_in_r/index.html#sec-bayesian-idea-example",
    "href": "posts/bayes_map_estimation_in_r/index.html#sec-bayesian-idea-example",
    "title": "Reproducing NONMEM’s MAP estimation in R",
    "section": "3.1 Starting with an example",
    "text": "3.1 Starting with an example\nI would say that many of us naturally think Bayesian, even if we are not aware of it or if we don’t label it that way. At its core, Bayesian statistics is about updating your beliefs once new data or information becomes available. Let’s consider an example from the real-world to illustrate this idea.\nImagine you drive to work every day. If someone would ask you “How long will it take you to get to work tomorrow?”, you would probably have a good idea, simply based on your past experiences. 30 minutes might be your best guess, but would you bet money that it will be exactly 30 minutes? Probably not, as you will always have some uncertainty associated with it. Perhaps something between 20 and 40 minutes would be reasonable based on your previous trips. Now, let’s assume you get into your car the next morning and after 25 minutes, you have made it halfway.\n\n\n\n\n\n\nFigure 2: A traffic jam on your way to work. Credits: Aayush Srivastava, Pexels.\n\n\n\nWould you stick to your initial belief that it will take you 30 minutes? Probably not, you would likely revise it (or try to break all speed limits). But would you simply double the 25 minutes to predict a 50-minute commute? This is also unlikely, given your experience suggesting that 40 minutes is usually the upper limit. So, maybe you would adjust your estimate to 38 minutes, assuming that the traffic will get better soon. This is Bayesian thinking in action: Updating your beliefs with new data. But so far without any equation and rather based on gut-feeling."
  },
  {
    "objectID": "posts/bayes_map_estimation_in_r/index.html#sec-bayesian-idea-terminology",
    "href": "posts/bayes_map_estimation_in_r/index.html#sec-bayesian-idea-terminology",
    "title": "Reproducing NONMEM’s MAP estimation in R",
    "section": "3.2 Terminology",
    "text": "3.2 Terminology\nNow, let’s map this real-world scenario to some Bayesian terminology. There are three key components in every Bayesian analysis:\n\nPrior: Your initial belief - 30 minutes, plus some uncertainty.\nLikelihood: The new data - taking 25 min to cover half the distance and suggesting 50 min for the full trip.\nPosterior: Your updated estimate - now around 38 minutes, combining prior and likelihood.\n\nBayesian analysis starts with a prior, which you revise using new data (likelihood) to obtain the posterior. In pharmacometrics, we often encounter two levels of complexity when it comes to Bayes:\n\nMAP / EBE: Simply focuses on the mode of the posterior distribution and ignores its uncertainty.\nFull Bayesian: Considers the entire posterior distribution, not just the mode or a point estimate.\n\nMany pharmacometricians use MAP estimates due to their computational simplicity and their direct integration into standard NLME routines, while full Bayesian methods offer a more comprehensive view by integrating uncertainty even after new data became available. Full Bayesian methods are more and more used in the field, also thanks to some nice tutorials by Margossian, Zhang, and Gillespie (2022) for Stan/Torsten and by Johnston et al. (2024) for NONMEM itself, both from the Metrum Research Group."
  },
  {
    "objectID": "posts/bayes_map_estimation_in_r/index.html#sec-bayesian-idea-goal",
    "href": "posts/bayes_map_estimation_in_r/index.html#sec-bayesian-idea-goal",
    "title": "Reproducing NONMEM’s MAP estimation in R",
    "section": "3.3 Our goal in pharmacometrics",
    "text": "3.3 Our goal in pharmacometrics\nSo, what role does Bayesian methodology play in pharmacometrics? Obviously it is not about optimizing your commute. Instead, we typically use Bayesian estimation to individualize the model parameters for a given individual \\(i\\), which has a set of unique observations \\(Y_{i}\\). This concept was first introduced to the pharmacometric community by Sheiner, Rosenberg, and Melmon (1972), if I am not mistaken. On the one hand, MAP estimation happens during model building, at each iteration step 2 and then after the actual estimation has finished, at the POSTHOC step. On the other hand, it is also used in MIPD settings as described above. The approach remains the same, and we will later have a look at a simple pharmacokinetic example to illustrate this.\nSo far we used a relatable example rather focusing on the intuition behind Bayesian thinking. Now we want to dive a bit into the mathematics behind it to be able to formally reproduce a simple Bayesian MAP estimation in R."
  },
  {
    "objectID": "posts/bayes_map_estimation_in_r/index.html#sec-pharmex-data",
    "href": "posts/bayes_map_estimation_in_r/index.html#sec-pharmex-data",
    "title": "Reproducing NONMEM’s MAP estimation in R",
    "section": "4.1 Data and visualization",
    "text": "4.1 Data and visualization\nWithout data, no Bayesian parameter individualization. For our MAP estimation, we are using the same simulated data set I have defined in the previous blogpost about NLME modelling. See Klose (2025) for more details about this data set. Let’s start by loading the simulated concentration-time data and showing the head of the data set:\n\n\nCode\n# read in simulated dataset from previous blogpost\nsim_data &lt;- read_csv(\"./data/sim_data.csv\")\n\n# show data \nsim_data |&gt;  \n  head() |&gt; \n  kable() |&gt; \n  kable_styling()\n\n\n\n\n\nID\nTIME\nEVID\nAMT\nRATE\nDV\nMDV\n\n\n\n\n1\n0.00\n1\n100\n0\n0.0000\n1\n\n\n1\n0.01\n0\n0\n0\n30.8160\n0\n\n\n1\n3.00\n0\n0\n0\n24.5520\n0\n\n\n1\n6.00\n0\n0\n0\n17.9250\n0\n\n\n1\n12.00\n0\n0\n0\n10.1100\n0\n\n\n1\n24.00\n0\n0\n0\n3.5975\n0\n\n\n\n\n\nWe will focus on ID 5, which has lower simulated concentrations than the average individual, making it easier to observe differences between typical and individualized profiles. Here’s a plot to visualize these concentration-time profiles of ID 5 and the other IDs of the virtual population:\n\n\nCode\n# show individual profiles\nsim_data |&gt; \n  filter(EVID == 0) |&gt; \n  mutate(flag = if_else(ID == 5, \"Reference\", \"Others\")) |&gt;\n  ggplot(aes(x=TIME, y=DV, group=ID, color=as.factor(flag))) +\n  geom_point()+\n  geom_line()+\n  theme_bw()+\n  scale_y_continuous(limits=c(0,NA))+\n  labs(x=\"Time after last dose [h]\", y=\"Concentration [mg/L]\")+\n  scale_color_manual(\"Individual\", values=c(\"grey\", \"darkblue\"))+\n  ggtitle(\"Simulated concentration time data after i.v. bolus injection\")\n\n\n\n\n\n\n\n\nFigure 3: Simulated concentration-time profiles for 10 individuals after i.v. bolus injection. ID 5 (blue) represents our exemplary ID.\n\n\n\n\n\nWe will later need this data set when running the NONMEM-based MAP estimation, so we have to save it to file. Prior to that, we will filter the data set to only include the data for ID 5:\n\n\nCode\n# define sim_data with ID == 5\nsim_data_id5 &lt;- sim_data |&gt; \n  filter(ID == 5)\n\n# save data for NONMEM\nsim_data_id5 |&gt; \n  write_csv(\"./data/sim_data_ID5.csv\") \n\n\nWith that being done, we are all set to define the NLME model in NONMEM in a next step!"
  },
  {
    "objectID": "posts/bayes_map_estimation_in_r/index.html#sec-pharmex-nlme",
    "href": "posts/bayes_map_estimation_in_r/index.html#sec-pharmex-nlme",
    "title": "Reproducing NONMEM’s MAP estimation in R",
    "section": "4.2 NLME model structure",
    "text": "4.2 NLME model structure\nFor our example and reference, we’ll use a simple one-compartment IV model with first-order kinetics, previously fitted to similar data (Klose (2025)). Here’s a sketch of the simple model structure:\n\n\n\n\n\n\nFigure 4: Model structure of our simple 1 cmt i.v. bolus model with first order kinetics.\n\n\n\nAssuming our model is already fitted, our goal is to individualize parameters for ID 5. The model parameter estimates are based on the fitted model from the previous blogpost and are as follows:\n\n\\(CL\\) = 0.247 L/h\n\\(V\\) = 3.15 L\n\\(\\omega^2_{CL}\\) = 0.11\n\\(\\sigma^2_{RUV\\_ADD}\\) = 2.00\n\\(\\sigma^2_{RUV\\_PROP}\\) = 0.50\n\nIn a next step, we will translate this into a NONMEM model. Note that we added a proportional error term to the previously used model to better illustrate the differences among prior, likelihood, and posterior distributions 3. For the same reason, we increased the additive error to 2 mg/L. Let’s store this information, along with the 100 mg intravenous bolus dose administered to the individual, in a list object for easy retrieval during subsequent calculations:\n\n\nCode\n# store model parameters in list\nmod_par &lt;- list(\n  tvcl = 0.247,\n  tvvd = 3.15,\n  omega2_CL = 0.11,\n  sigma2_add = 2,\n  sigma2_prop = 0.50, \n  dose = 100 \n)\n \n# show \nmod_par \n\n\n$tvcl\n[1] 0.247\n\n$tvvd\n[1] 3.15\n\n$omega2_CL\n[1] 0.11\n\n$sigma2_add\n[1] 2\n\n$sigma2_prop\n[1] 0.5\n\n$dose\n[1] 100\n\n\nGreat! Now we can go ahead and define the NONMEM model."
  },
  {
    "objectID": "posts/bayes_map_estimation_in_r/index.html#sec-pharmex-model",
    "href": "posts/bayes_map_estimation_in_r/index.html#sec-pharmex-model",
    "title": "Reproducing NONMEM’s MAP estimation in R",
    "section": "4.3 NONMEM model",
    "text": "4.3 NONMEM model\nWe have updated our NONMEM model slightly from the last blogpost:\n\n\n1cmt_iv_map_est.mod\n\n$PROBLEM 1cmt_iv_map_est\n\n$INPUT ID TIME EVID AMT RATE DV MDV\n\n$DATA C:\\Users\\mklose\\Desktop\\GitHub\\personal-website\\posts\\bayes_map_estimation_r\\data\\sim_data_ID5.csv IGNORE=@\n\n$SUBROUTINES ADVAN1 TRANS2\n\n$PK\n; define fixed effects parameters\nCL = THETA(1) * EXP(ETA(1))\nV = THETA(2)\n\n; scaling\nS1=V\n\n$THETA\n0.247 FIX               ; 1 TVCL\n3.15 FIX                ; 2 TVV\n\n$OMEGA \n0.11 FIX                ; 1 OM_CL\n\n$SIGMA\n0.50 FIX                ; 1 SIG_PROP\n2.00 FIX                ; 2 SIG_ADD\n\n$ERROR \n; add residual unexplained variability\nIPRED = F\nY = IPRED + IPRED * EPS(1) + EPS(2)\n\n$ESTIMATION METHOD=1 INTERACTION MAXEVAL=0 SIGDIG=3 PRINT=1 NOABORT POSTHOC\n\n$TABLE ID TIME EVID AMT RATE DV PRED IPRED MDV ETA1 CL NOAPPEND ONEHEADER NOPRINT FILE=map_estim_out\n\nWe have fixed the initial parameter estimates to the final estimates from before and incorporated a combined error model (as described above). By using MAXEVAL=0, we avoid any population parameter estimation and the model is simply being evaluated. The POSTHOC option computes individual MAP estimates as this is the core of our task. We need to make sure that we include the INTERACTION option in the $ESTIMATION block, as this tells NONMEM to include the effect of ETA on the residual error when performing MAP estimation 4. This is important as our error model now depends on IPRED, which in turn depends on ETA.\nAfter running the model, we’ll read the output through map_estim_out:\n\n\nCode\n# load simulated data\nnm_out &lt;- read_nm_table(\"./models/map_estim_out\")\n\n# show simulated data\nnm_out |&gt; \n  head() |&gt; \n  kable() |&gt; \n  kable_styling()\n\n\n\n\n\nID\nTIME\nEVID\nAMT\nRATE\nDV\nPRED\nIPRED\nMDV\nETA1\nCL\n\n\n\n\n5\n0.00\n1\n100\n0\n0.00000\n31.7460\n31.7460\n1\n0.55194\n0.42894\n\n\n5\n0.01\n0\n0\n0\n31.61900\n31.7210\n31.7030\n0\n0.55194\n0.42894\n\n\n5\n3.00\n0\n0\n0\n19.66000\n25.0920\n21.1000\n0\n0.55194\n0.42894\n\n\n5\n6.00\n0\n0\n0\n12.00700\n19.8320\n14.0230\n0\n0.55194\n0.42894\n\n\n5\n12.00\n0\n0\n0\n4.33360\n12.3890\n6.1947\n0\n0.55194\n0.42894\n\n\n5\n24.00\n0\n0\n0\n0.78148\n4.8349\n1.2088\n0\n0.55194\n0.42894\n\n\n\n\n\nThe ETA1 (= \\(\\eta_i\\)) value represents the individual random effect, while CL (= \\(CL_i\\)) is the individual clearance estimate:\n\\[CL_i = \\theta_{TVCL} \\cdot \\exp(\\eta_i) \\tag{1}\\]\nCalculating this in R yields:\n\\[CL_i = 0.247 \\cdot \\exp(0.55194) = 0.42894 \\tag{2}\\]\n\n# calculate individual MAP estimate\nmod_par$tvcl*exp(nm_out$ETA1 |&gt; unique())\n\n[1] 0.4289448\n\n\nGreat! We obtained our reference solution for the MAP parameter individualization. We can now visually compare DV, PRED, and IPRED:\n\n\nCode\n# plot DV, PRED, IPRED\nnm_out |&gt; \n  filter(TIME &gt; 0) |&gt; \n  pivot_longer(cols=c(PRED, IPRED, DV), names_to=\"variable\", values_to=\"value\") |&gt;\n  ggplot(aes(x=TIME, y=value, group=variable, color=variable))+\n  geom_point()+\n  geom_line()+\n  labs(\n    y = \"Concentration [mg/L]\",\n    x = \"Time after last dose [h]\",\n    title = \"Comparison of DV, PRED and IPRED for ID 5\",\n    color = \"Source\"\n  ) +\n  theme_bw() \n\n\n\n\n\n\n\n\nFigure 5: Comparison of observations (DV), population predictions (PRED), and individualized predictions based on the MAP estimate for CL (IPRED).\n\n\n\n\n\nNotably, our reference individual’s data (DV) diverges from typical predictions (PRED), while individualized predictions (IPRED) fall in between. Let’s dive deeper into the process to understand these differences."
  },
  {
    "objectID": "posts/bayes_map_estimation_in_r/index.html#sec-bayesth-general",
    "href": "posts/bayes_map_estimation_in_r/index.html#sec-bayesth-general",
    "title": "Reproducing NONMEM’s MAP estimation in R",
    "section": "5.1 General form of Bayes’ theorem",
    "text": "5.1 General form of Bayes’ theorem\nAs described earlier, the primary goal of a Bayesian approach is to obtain the posterior distribution, either as the mode or as the complete distribution. Bayes’ theorem is commonly presented as follows:\n\\[P(A|B) = \\frac{P(A) \\cdot P(B|A)}{P(B)} \\tag{3}\\]\nHere, conditional probabilities are utilized, and \\(P(A|B)\\) represents the probability of event \\(A\\) given event \\(B\\) has occurred. However, this remains quite theoretical. Let’s directly translate it into the context of a pharmacokinetic NLME model."
  },
  {
    "objectID": "posts/bayes_map_estimation_in_r/index.html#sec-bayesth-pharmetrx",
    "href": "posts/bayes_map_estimation_in_r/index.html#sec-bayesth-pharmetrx",
    "title": "Reproducing NONMEM’s MAP estimation in R",
    "section": "5.2 Pharmacometric context",
    "text": "5.2 Pharmacometric context\nIn pharmacometrics, we aim to determine the most likely individual random effect \\(\\eta_i\\) (which translates to the individual parameter \\(CL_i\\)), given the observed concentration data \\(Y_{i}\\) for the individual. Therefore, we are mostly interested to find the posterior distribution of the individual random effect \\(\\eta_i\\) given the data \\(Y_{i}\\):\n\\[p(\\eta_i|Y_{i}) = \\frac{p(\\eta_i) \\cdot p(Y_{i}|\\eta_i)}{p(Y_i)} \\tag{4}\\]\nwith\n\n\\(p(\\eta_i|Y_{i})\\): posterior distribution of parameter \\(\\eta_i\\) given the individual data \\(Y_{i}\\)\n\\(p(\\eta_i)\\): prior distribution of parameter \\(\\eta_i\\)\n\\(p(Y_{i}|\\eta_i)\\): likelihood of observing data \\(Y_{i}\\) given parameter \\(\\eta_i\\)\n\\(p(Y_{i})\\): marginal likelihood of data \\(Y_{i}\\)\n\nThe marginal likelihood \\(p(Y_{i})\\) is typically neglected because it just acts as a scaling factor, does not depend on \\(\\eta_i\\), and is computationally difficult due to a high-dimensional integral. Thus, the formula is often simplified to:\n\\[p(\\eta_i|Y_{i}) \\propto p(\\eta_i) \\cdot p(Y_{i}|\\eta_i) \\tag{5}\\]\nRemoving the marginal likelihood gives an unnormalized posterior distribution, indicated by the proportional sign. Working with an unnormalized posterior is acceptable when our when we solely care about finding the mode of the posterior distribution (MAP estimate), as normalization is unnecessary in this scenario. But how exactly do we calculate the MAP estimate?"
  },
  {
    "objectID": "posts/bayes_map_estimation_in_r/index.html#sec-bayesth-map",
    "href": "posts/bayes_map_estimation_in_r/index.html#sec-bayesth-map",
    "title": "Reproducing NONMEM’s MAP estimation in R",
    "section": "5.3 MAP estimation",
    "text": "5.3 MAP estimation\nTo find the most likely parameter for an individual, we must identify the maximum of the posterior distribution (MAP estimate). Mathematically, this involves finding the parameter \\(\\eta_i^*\\) that maximizes the posterior:\n\\[\\eta_i^* = \\underset{\\eta_i}{\\mathrm{argmax}}~ p(\\eta_i|Y_{i}) = \\underset{\\eta_i}{\\mathrm{argmax}}~ p(\\eta_i) \\cdot \\prod_{j=1}^{m} p(Y_{ij}|\\eta_i) \\tag{6}\\]\nDealing with products of small probability densities can be cumbersome (and mathematically instable), so taking the logarithm simplifies the calculation:\n\\[\\eta_i^* = \\underset{\\eta_i}{\\mathrm{argmax}}~ \\log(p(\\eta_i|Y_{i})) = \\underset{\\eta_i}{\\mathrm{argmax}}~ \\log(p(\\eta_i)) + \\sum_{j=1}^{m} \\log(p(Y_{ij}|\\eta_i)) \\tag{7}\\]\nIn the end, we would have to use a numerical optimizer function to explore the parameter space of \\(\\eta_i\\) and locate the maximum of the posterior distribution (\\(\\eta_i^*\\)). To fully express the equation, we must define both the log prior \\(\\log(p(\\eta_i))\\) and log likelihood \\(\\log(p(Y_{ij}|\\eta_i))\\) terms. Let’s explore these in detail.\n\n5.3.1 Prior term\nThe prior distribution \\(p(\\eta_i)\\) captures our inital beliefs and uncertainties regarding \\(\\eta_i\\) before observing the data. Bayesian methods allow to incorporate prior knowledge through various distributions and parameters (which is why Bayesians are sometimes being criticized of being too subjective). In pharmacometrics, a common approach is using a prior based on a previously estimated model, typically a normal distribution with mean 0 and variance \\(\\omega^2_{CL}\\) estimated from the NLME model. In contrast to other disciplines, the choice of our priors is generally less controversial, although there has been recent discussion regarding whether this one-approach-fits-all is always appropriate or if priors should be adjusted (e.g., flattened compared to the NLME estimate) in certain situations. See Hughes and Keizer (2021) for more information. The general “role” of the prior term is that strong deviations from the “typical” individual should only be considered when it allows us do substantially better explain the observed data. I like to see it as some kind of penalty function which prevents us from doing a potentially flawed curve fitting exercise. The probability density function (PDF) of a normal distribution can be used to calculate its contribution:\n\\[p(\\eta_i) = \\frac{1}{\\sqrt{2\\pi\\omega^2}} \\cdot \\exp\\left(-\\frac{(\\eta_i-\\mu)^2}{2\\omega^2}\\right) \\tag{8}\\]\nSince \\(\\mu\\) is typically 0, this simplifies to:\n\\[p(\\eta_i) = \\frac{1}{\\sqrt{2\\pi\\omega^2}} \\cdot \\exp\\left(-\\frac{\\eta_i^2}{2\\omega^2}\\right) \\tag{9}\\]\nTaking the log yields:\n\\[\\log(p(\\eta_i)) = -0.5 \\log(2\\pi\\omega^2) - \\frac{\\eta_i^2}{2\\omega^2} \\tag{10}\\]\nNow, we can compute the prior term for any given individual \\(\\eta_i\\), using variance \\(\\omega^2_{CL}\\) (e.g., 0.11) from our NLME model. If we have a good reason to believe that our prior should be different (e.g., we apply our model to another population), we could adjust the variance accordingly (as described in Hughes and Keizer (2021)).\n\n\n5.3.2 Likelihood term\nThe likelihood term \\(p(Y_{ij}|\\eta_i)\\) is the probability of observing the data point \\(Y_{ij}\\) given the parameter \\(\\eta_i\\). As we usually assume the residuals of our model predictions to be normally distributed, we again use the normal distribution PDF:\n\\[p(Y_{ij}|\\eta_i) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\cdot \\exp\\left(-\\frac{(Y_{ij}-f(x_{ij}; \\eta_i))^2}{2\\sigma^2}\\right) \\tag{11}\\]\nwhere\n\n\\(Y_{ij}\\): observed data point for individual \\(i\\) at time \\(j\\)\n\\(f(x_{ij}; \\eta_i)\\): model prediction for individual \\(i\\) at time \\(j\\), given parameter \\(\\eta_i\\)\n\\(\\sigma^2\\): variance representing residual unexplained variability (RUV)\n\nTaking the log yields:\n\\[\\log(p(Y_{ij}|\\eta_i)) = -0.5 \\log(2\\pi\\sigma^2) - \\frac{(Y_{ij}-f(x_{ij}; \\eta_i))^2}{2\\sigma^2} \\tag{12}\\]\nPlease note that in our case (a combined additive and proportional RUV model), \\(\\sigma^2\\) represents the combined variances at time \\(t_{ij}\\) and prediction \\(f(x_{ij})\\). As the structural solution of our model, \\(f(x_{ij})\\), is a constant for a given \\(t_{ij}\\), we can compute the combined variance as\n\\[\\sigma^2 = \\sigma^2_{prop} \\cdot f(x_{ij})^2 + \\sigma^2_{add} \\tag{13}\\]\nTherefore, we later have to calculate the \\(\\sigma^2\\) for each data point \\(Y_{ij}\\) individually."
  },
  {
    "objectID": "posts/bayes_map_estimation_in_r/index.html#sec-bayesth-map-final",
    "href": "posts/bayes_map_estimation_in_r/index.html#sec-bayesth-map-final",
    "title": "Reproducing NONMEM’s MAP estimation in R",
    "section": "5.4 Combining both terms",
    "text": "5.4 Combining both terms\nWriting out the log prior (Equation 10) and log likelihood (Equation 12) terms in our MAP estimation objective function (Equation 7), we obtain:\n\\[\\eta_i^* = \\underset{\\eta_i}{\\mathrm{argmax}}~\\left(\\left[-0.5 \\log(2\\pi\\omega^2) - \\frac{\\eta_i^2}{2\\omega^2}\\right] ~ + \\sum_{j=1}^m \\left[-0.5 \\log(2\\pi\\sigma^2) - \\frac{(Y_{ij}-f(x_{ij}; \\eta_i))^2}{2\\sigma^2}\\right]\\right) \\tag{14}\\]\nThis final equation will guide our numerical optimizer (optim function in R) to determine the maximum, providing us with the individual MAP estimate. Let’s implement this method in R and reproduce our NONMEM reference solution!"
  },
  {
    "objectID": "posts/bayes_map_estimation_in_r/index.html#sec-rrepro-prior",
    "href": "posts/bayes_map_estimation_in_r/index.html#sec-rrepro-prior",
    "title": "Reproducing NONMEM’s MAP estimation in R",
    "section": "6.1 Prior term",
    "text": "6.1 Prior term\nWe start by defining a function to calculate the prior term given by Equation 10. Here, we simply pass the individual \\(\\eta_i\\) and the model parameters (including \\(\\omega^2_{CL}\\)) as input arguments, and the function returns the log probability of the given \\(\\eta_i\\) under the prior distribution:\n\n\n\nfunction: prior_fun()\n\n# define prior term function\nprior_fun &lt;- function(eta_i, mod_par){\n  \n  # retrieve omega2 from model parameters\n  omega2 &lt;- mod_par$omega2_CL\n  \n  # calculate probability\n  log_prob &lt;- - 0.5 * log(2*pi*omega2) - eta_i^2/(2*omega2)\n  \n  # return log probability\n  return(log_prob)\n}\n\n\nWe can test this function by illustrating the prior term across a range of \\(\\eta_i\\) values:\n\n\nCode\n# define range\neta_i &lt;- seq(-2, 2, 0.01)\n\n# calculate prior term\nprior &lt;- prior_fun(eta_i = eta_i, mod_par = mod_par)\n\n# create tibble\nprior_tibble &lt;- tibble(\n  eta_i = eta_i,\n  p = prior,\n  source = \"prior\"\n)\n\n# plot prior term\nprior_tibble |&gt; \n  ggplot(aes(x=eta_i, y=exp(p)))+\n  geom_line(color = \"darkblue\", linewidth = 0.8)+\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"darkblue\", linewidth = 0.6)+\n  labs(\n    x = expression(eta[i]),\n    y = \"Probability density\",\n    title = \"Prior term (random effect)\"\n  )+\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure 6: Probability density of different random effects given a normal distribution centered around 0 with a variance of 0.11.\n\n\n\n\n\nThe following plot shows how these \\(\\eta_i\\) values map onto the clearance domain:\n\n\nCode\n# plot prior term\nprior_tibble |&gt; \n  ggplot(aes(x=mod_par$tvcl*exp(eta_i), y=exp(p)))+\n  geom_line(color = \"darkblue\", linewidth = 0.8)+\n  geom_vline(xintercept = mod_par$tvcl, linetype = \"dashed\", color = \"darkblue\", linewidth = 0.6)+\n  labs(\n    x = expression(CL[i]),\n    y = \"Probability density\",\n    title = \"Prior term (clearance)\"\n  )+\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure 7: Probability density of different clearance values.\n\n\n\n\n\nFrom the plots, we see that certain values of \\(\\eta_i\\) and \\(CL_i\\) are more likely than others, based on our prior beliefs derived from the NLME model estimates, before observing any individual data. Let’s continue with the likelihood term:"
  },
  {
    "objectID": "posts/bayes_map_estimation_in_r/index.html#sec-rrepro-likelihood",
    "href": "posts/bayes_map_estimation_in_r/index.html#sec-rrepro-likelihood",
    "title": "Reproducing NONMEM’s MAP estimation in R",
    "section": "6.2 Likelihood term",
    "text": "6.2 Likelihood term\nTo calculate the log likelihood term described in Equation 12, we first have to define a model prediction function. Given our simple one-compartment model, we use a closed-form expression, though the principles apply equally to ODE-based models. More details on the model prediction function can be found in the previous blogpost, see Klose (2025).\n\n\n\nfunction: model_fun()\n\n# define model function\nmodel_fun &lt;- function(eta_i, dose, vd, theta_tvcl, t) {\n  exp_eta_i &lt;- exp(eta_i)\n  exponent &lt;- -1 * (theta_tvcl * exp_eta_i / vd) * t\n  result &lt;- (dose / vd) * exp(exponent)\n  return(result)\n}\n\n\nNext, we construct the likelihood function for individual observations. Please note that we have to calculate the \\(\\sigma^2\\) for each data point \\(Y_{ij}\\) individually, as the variance contains an element proportional to the model prediction \\(f_i\\) (see Equation 12). The following equation calculates the log likelihood for a single observation:\n\n\n\nfunction: log_likelihood_fun_single()\n\n# define likelihood function for a single observation\nlog_likelihood_fun_single &lt;- function(eta_i, dose, vd, theta_tvcl, t_ij, Y_ij, sigma2_add, sigma2_prop){\n  \n  # get model predictions\n  f_ij &lt;- model_fun(eta_i, dose, vd, theta_tvcl, t_ij)\n  \n  # calculate resulting sigma2 for each timepoint (prop variance is scaled based on f_i^2)\n  sigma2 &lt;- sigma2_prop * f_ij^2 + sigma2_add\n  \n  # calculate probability\n  log_lik &lt;- - 0.5 * log(2*pi*sigma2) - (Y_ij - f_ij)^2/(2*sigma2)\n  \n  # return log likelihood\n  return(log_lik)\n}\n\n\nTo illustrate this, we calculate the likelihood for each observation, then sum these to obtain the total likelihood for a given \\(\\eta_i\\). We demonstrate this using an example with \\(\\eta_i = 0\\):\n\n\nCode\n# calculate lok lik per row\nsim_data_id5 &lt;- sim_data_id5 |&gt; \n  rowwise() |&gt; \n  mutate(\n    log_lik = case_when(\n      EVID == 0 ~ log_likelihood_fun_single(\n        eta_i = 0, \n        dose = mod_par$dose, \n        vd = mod_par$tvvd, \n        theta_tvcl = mod_par$tvcl, \n        t_ij = TIME, \n        Y_ij = DV, \n        sigma2_add = mod_par$sigma2_add, \n        sigma2_prop = mod_par$sigma2_prop\n      ),\n      EVID == 1 ~ NA_real_\n    )\n  ) |&gt; \n  ungroup() |&gt; \n  mutate(\n    sum_log_lik = sum(log_lik, na.rm = TRUE)\n  )\n\n# show sim_data_id5\nsim_data_id5 |&gt;\n  kable() |&gt; \n  kable_styling()\n\n\n\n\n\nID\nTIME\nEVID\nAMT\nRATE\nDV\nMDV\nlog_lik\nsum_log_lik\n\n\n\n\n5\n0.00\n1\n100\n0\n0.00000\n1\nNA\n-17.93624\n\n\n5\n0.01\n0\n0\n0\n31.61900\n0\n-4.031343\n-17.93624\n\n\n5\n3.00\n0\n0\n0\n19.66000\n0\n-3.844624\n-17.93624\n\n\n5\n6.00\n0\n0\n0\n12.00700\n0\n-3.718827\n-17.93624\n\n\n5\n12.00\n0\n0\n0\n4.33360\n0\n-3.514076\n-17.93624\n\n\n5\n24.00\n0\n0\n0\n0.78148\n0\n-2.827369\n-17.93624\n\n\n\n\n\nWe observe individual likelihood contributions and their sum. To streamline this process, we define a function that computes the summed likelihood across multiple observations:\n\n\n\nfunction: log_likelihood_fun_multiple()\n\n# define likelihood function for a set of observations\nlog_likelihood_fun_multiple &lt;- function(df, eta_i, mod_par){\n  \n  # retrieve information from mod_par\n  dose &lt;- mod_par$dose\n  vd &lt;- mod_par$tvvd\n  theta_tvcl &lt;- mod_par$tvcl\n  sigma2_add &lt;- mod_par$sigma2_add\n  sigma2_prop &lt;- mod_par$sigma2_prop\n  \n  # calculate log lik per row\n  log_lik_sum &lt;- df |&gt;\n    rowwise() |&gt; \n    mutate(\n      log_lik = case_when(\n        EVID == 0 ~ log_likelihood_fun_single(\n          eta_i = eta_i, \n          dose = dose, \n          vd = vd, \n          theta_tvcl = theta_tvcl, \n          t_ij = TIME, \n          Y_ij = DV, \n          sigma2_add = sigma2_add, \n          sigma2_prop = sigma2_prop\n        ),\n        EVID == 1 ~ NA_real_\n      )\n    ) |&gt; \n    ungroup() |&gt; \n    pull(log_lik) |&gt;\n    sum(na.rm = TRUE)\n  \n  # return log likelihood sum\n  return(log_lik_sum)\n}\n\n\nTesting this function quickly confirms consistency of the obtained summed up likelihood with the previous result from the table (see above):\n\n\nCode\n# calculate likelihood\nlog_likelihood_fun_multiple(\n  df = sim_data_id5,\n  eta_i = 0,\n  mod_par = mod_par\n)\n\n\n[1] -17.93624\n\n\nNow, we can visualize the likelihood term across a range of \\(\\eta_i\\) values, similar to what we have did for the prior term:\n\n\nCode\n# define range\neta_i &lt;- seq(-2, 2, 0.01)\n\n# empty list\nll_list &lt;- list()\n\n# loop over each element of eta_i\nfor(cur_eta_i in eta_i){\n  # calculate likelihood term\n  ll_list[[as.character(cur_eta_i)]] &lt;- log_likelihood_fun_multiple(\n    df = sim_data_id5,\n    eta_i = cur_eta_i,\n    mod_par = mod_par\n  )\n}\n\n# convert to vector\nll_vector &lt;- ll_list |&gt; unlist() |&gt; unname()\n\n# create tibble\nlikelihood_tibble &lt;- tibble(\n  eta_i = eta_i,\n  p = ll_vector,\n  source = \"likelihood\"\n)\n\n# plot likelihood term\nlikelihood_tibble |&gt; \n  ggplot(aes(x=eta_i, y=exp(p)))+\n  geom_line(color = \"darkred\", linewidth = 0.8)+\n  geom_vline(xintercept = eta_i[which.max(ll_vector)], linetype = \"dashed\", color = \"darkred\", linewidth = 0.8)+\n  labs(\n    x = expression(eta[i]),\n    y = \"Likelihood\",\n    title = \"Likelihood term\"\n  )+\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure 8: Likelihood of different random effects given the observed datapoints of our exemplary individual.\n\n\n\n\n\nThe likelihood plot clearly indicates which \\(\\eta_i\\) values best describe the observed data. Identifying the maximum likelihood \\(\\eta_i\\) is straightforward:\n\n\nCode\n# get eta_i with highest likelihood\neta_i_max_likelihood &lt;- eta_i[which.max(ll_vector)]\n\n# show\neta_i_max_likelihood\n\n\n[1] 0.94\n\n\nThe optimal \\(\\eta_i\\) value according to the likelihood term (and ignoring any prior information) is 0.94. Please note: As the likelihood is the product of many probabilities, each between 0 and 1, we typically deal with extremely small numerical values. Therefore, it is common practice to use logarithmic transformations for numerical stability. Next, we combine the prior and likelihood terms to perform MAP estimation."
  },
  {
    "objectID": "posts/bayes_map_estimation_in_r/index.html#map-rrepro-mapestim",
    "href": "posts/bayes_map_estimation_in_r/index.html#map-rrepro-mapestim",
    "title": "Reproducing NONMEM’s MAP estimation in R",
    "section": "6.3 MAP estimation (posterior)",
    "text": "6.3 MAP estimation (posterior)\nWe now have to define an objective function for numerical optimization, combining prior and likelihood terms to estimate the posterior (as shown in Equation 14).\n\n\n\nfunction: map_obj_fun()\n\n# define MAP estimation objective function\nmap_obj_fun &lt;- function(eta_i, df, mod_par){\n  \n  # calculate log prior term\n  log_prior &lt;- prior_fun(\n    eta_i = eta_i, \n    mod_par = mod_par\n  )\n  \n  # calculate log likelihood term\n  log_likelihood &lt;- log_likelihood_fun_multiple(\n    df = df,\n    eta_i = eta_i,\n    mod_par = mod_par\n  )\n  \n  # combine both\n  log_posterior &lt;- log_prior + log_likelihood\n  \n  # return negative log posterior\n  return(-log_posterior)\n}\n\n\nPlease note that we are returning the negative log posterior as it is easier to minimize a function than to maximize it. Using R’s optim function, we estimate the MAP numerically. Here is the output form the optim call:\n\n\nCode\n# run optimization\nmap_est &lt;- optim(\n  par = 0, \n  fn = map_obj_fun, \n  df = sim_data_id5,\n  mod_par = mod_par,\n  method = \"BFGS\"\n)\n\n# show estimation results\nmap_est \n\n\n$par\n[1] 0.5519271\n\n$value\n[1] 16.08691\n\n$counts\nfunction gradient \n      13        7 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nThe resulting MAP estimate for \\(\\eta_i\\) is 0.5519271, closely matching the reference solution (0.55194) with an acceptable difference (-0.0000128887). Similarly to the other terms, we can visualize the posterior term across a range of \\(\\eta_i\\) values:\n\n\nCode\n# define range\neta_i &lt;- seq(-2, 2, 0.01)\n\n# empty list\npost_list &lt;- list()\n\n# loop over each element of eta_i\nfor(cur_eta_i in eta_i){\n  # calculate likelihood term\n  post_list[[as.character(cur_eta_i)]] &lt;- map_obj_fun(\n    eta_i = cur_eta_i,\n    df = sim_data_id5,\n    mod_par = mod_par\n  )\n}\n\n# convert to vector\npost_vector &lt;- post_list |&gt; unlist() |&gt; unname()\n\n# create tibble\nposterior_tibble &lt;- tibble(\n  eta_i = eta_i,\n  p = -post_vector,\n  source = \"posterior\"\n)\n\n# plot likelihood term\nposterior_tibble |&gt; \n  ggplot(aes(x=eta_i, y=exp(p)))+\n  geom_line(color = \"darkgreen\", linewidth = 0.8)+\n  geom_vline(xintercept = map_est$par, linetype = \"dashed\", color = \"darkgreen\", linewidth = 0.8)+\n  labs(\n    x = expression(eta[i]),\n    y = \"Unnormalized posterior density\",\n    title = \"Posterior distribution\"\n  )+\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure 9: The unnormalized posterior density."
  },
  {
    "objectID": "posts/bayes_map_estimation_in_r/index.html#sec-map-rrepro-comp",
    "href": "posts/bayes_map_estimation_in_r/index.html#sec-map-rrepro-comp",
    "title": "Reproducing NONMEM’s MAP estimation in R",
    "section": "6.4 Comparison of prior, likelihood, and posterior",
    "text": "6.4 Comparison of prior, likelihood, and posterior\nFinally, we compare the MAP estimate (mode of the posterior) with prior and likelihood terms visually:\n\n\nCode\n# combine likelihood and prior\ncomp_tibble &lt;- rbind(\n  prior_tibble,\n  likelihood_tibble,\n  posterior_tibble\n) |&gt; \n  mutate(source = factor(source, levels = c(\"prior\", \"posterior\", \"likelihood\")))\n\n# define linewidth\nlinewidth &lt;- 0.8\n\n# plot both distributions with different colors and fill\ncomp_tibble |&gt;  \n  ggplot(aes(x=eta_i, y=exp(p), fill = source, color = source))+\n  scale_x_continuous(breaks = c(-2,-1,0, signif(map_est$par,2), signif(eta_i_max_likelihood,2), 2))+\n  geom_line(linewidth = linewidth)+\n  geom_vline(\n    data = data.frame(source = factor(\"prior\", levels = c(\"prior\", \"posterior\", \"likelihood\"))),\n    aes(xintercept = 0),\n    color = \"darkblue\", linetype = \"dashed\", linewidth = linewidth\n  ) +\n  geom_vline(\n    data = data.frame(source = factor(\"posterior\", levels = c(\"prior\", \"posterior\", \"likelihood\"))),\n    aes(xintercept = map_est$par),\n    color = \"darkgreen\", linetype = \"dashed\", linewidth = linewidth\n  ) +\n  geom_vline(\n    data = data.frame(source = factor(\"likelihood\", levels = c(\"prior\", \"posterior\", \"likelihood\"))),\n    aes(xintercept = eta_i_max_likelihood),\n    color = \"darkred\", linetype = \"dashed\", linewidth = linewidth\n  ) +\n  scale_color_manual(values = c(prior = \"darkblue\", \n                                posterior = \"darkgreen\", \n                                likelihood = \"darkred\")) +\n  facet_wrap(~source, scales = \"free\", ncol = 1)+\n  labs(\n    x = expression(eta[i]),\n    y = \"(Unnormalized) probability density\",\n    title = \"Comparison of prior, likelihood, and posterior\",\n    color = \"Source\"\n  ) +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nFigure 10: Comparison of prior, likelihood, and posterior.\n\n\n\n\n\nThis clearly demonstrates how the posterior is influenced by both the prior and the likelihood, with the MAP estimate (mode of the posterior, green dashed line) reflecting a balance between these two sources of information."
  },
  {
    "objectID": "posts/bayes_map_estimation_in_r/index.html#footnotes",
    "href": "posts/bayes_map_estimation_in_r/index.html#footnotes",
    "title": "Reproducing NONMEM’s MAP estimation in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMAP estimates are also known as Empirical Bayes Estimates (EBE).↩︎\nDepending on the estimation algorithm.↩︎\nThe dense sampling in combination with a simple additive error model for RUV led to a scenario where the posterior was nearly equal to the likelihood. Therefore, I have increased the additive error and introduced a proportional term as well. This places the posterior in the middle and I can recycle the old dataset I have simulated before.↩︎\nActually, this took me quite some time to figure out. I was using the model from the previous blogpost, which only had an additive error and was missing the INTERACTION option (which doesn’t matter if you are just using a constant additive error). With the newly introduced proportional error, I then had a mismatch between my calculations and the reference and I couldn’t figure out why. The missing INTERACTION was the solution, as with the introduction of the proportional error the RUV does not remain constant (which NONMEM is assuming when INTERACTION is missing). Lesson learned!↩︎"
  },
  {
    "objectID": "posts/expressing_ruv_as_theta/index.html",
    "href": "posts/expressing_ruv_as_theta/index.html",
    "title": "Expressing RUV as $THETA in NONMEM",
    "section": "",
    "text": "Code\n# load packages\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(kableExtra)\nWhen I started my PhD in pharmacometrics, I wanted to try something fancy1: specifying a combined proportional and additive error model in NONMEM for one of my projects. A colleague kindly sent me a reference model, and to my confusion, the code included a novel way (at least to me) of defining residual unexplained variability (RUV):\nalternative way (combined)\n\n$THETA\n0.15        ; RUV_PROP\n0.23        ; RUV_ADD\n\n$ERROR\nIPRED = F\nSD_PROP = THETA(1)*IPRED\nSD_ADD = THETA(2)\nSD = SQRT(SD_PROP**2 + SD_ADD**2)\nY = IPRED + SD * EPS(1)\n\n$SIGMA\n1 FIX\nIt wasn’t immediately clear why it was set up this way, and I was left with some questions:\nIt seemed a bit odd to me. I was more familiar with defining RUV directly in the $SIGMA block, something like:\nclassical way v1\n\n$ERROR\nIPRED = F\nY = IPRED + IPRED * EPS(1) + EPS(2)\n\n$SIGMA\n0.0225\n0.0529\nor maybe in a slightly more elegant form:\nclassical way v2\n\nY = IPRED * (1 + EPS(1)) + EPS(2)\nSo, why use this “alternative”2 way of defining the error? Before we try to explain this way of writing a combined error model to ourselves, let’s break down the additive and proportional error model separately to understand what’s going on. Please note: most of this content can also be found elsewhere (Proost 2017)."
  },
  {
    "objectID": "posts/expressing_ruv_as_theta/index.html#additive-error-models",
    "href": "posts/expressing_ruv_as_theta/index.html#additive-error-models",
    "title": "Expressing RUV as $THETA in NONMEM",
    "section": "1 Additive error models",
    "text": "1 Additive error models\nThe “classical” way (if I can call it that) of specifying an additive error model in NONMEM is as follows:\n\n\n\nclassical way (additive)\n\n$ERROR\nIPRED = F\nY = IPRED + EPS(1)\n\n$SIGMA\n0.0529\n\n\nIn this approach, RUV is defined directly in the $SIGMA block, where EPS(1) is assumed to be normally distributed with a mean of 0 and variance of 0.0529:\n\\[EPS(1) \\sim \\mathcal{N}(0,0.0529)\\]\nIt is quite important to note that we are specifying variances in $SIGMA. Now the alternative way (my colleague called it the Uppsala way3) of coding the additive error model looks like this:\n\n\n\nalternative way (additive)\n\n$THETA\n0.23        ; RUV_ADD\n\n$ERROR\nIPRED = F\nSD_ADD = THETA(1)\nY = IPRED + SD_ADD * EPS(1)\n\n$SIGMA\n1 FIX\n\n\nHere, $SIGMA is fixed so EPS(1) has a variance of 1, effectively making it a standard normal distribution:\n\\[EPS(1) \\sim \\mathcal{N}(0,1)\\]\nBut we then multiply this random variable EPS(1) by a scaling factor SD_ADD (which is being estimated as a THETA parameter) before the product is being added to the individual predicted IPRED value:\n\n\n\nalternative way (additive)\n\nY = IPRED + SD_ADD * EPS(1)\n\n\nI am not super familiar what happens if we multiply a random variable with a scaling factor. So maybe it is a good idea to visualize what happens when we fix $SIGMA to 1 and multiply it by SD = 0.23. Let’s start with plotting a standard normal distribution ($SIGMA 1 FIX):\n\n\nCode\n# sample from standard normal distribution\nx &lt;- rnorm(100000, mean = 0, sd = 1)\nstd_norm &lt;- tibble(x = x, source = \"unscaled\")\n\n# plot\nstd_norm |&gt; \n  ggplot(aes(x = x, fill = source)) +\n  geom_density(alpha=0.2)+\n  labs(title = \"Standard normal distribution\", x = \"\", y = \"Density\")+\n  scale_fill_manual(\n    \"Source\",\n    values = c(\n      \"unscaled\" = \"#003049\"\n    )\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThe resulting standard deviation should be 1, and since \\(1^2 = 1\\), the resulting variance should also be 1. Let’s be sure and check our empirical estimates (it is a simulation, after all) to confirm this:\n\n\nCode\n# summarize data and calculate sd and variance\nstd_norm |&gt; \n  group_by(source) |&gt;   \n  summarize(\n    sd = sd(x) |&gt; signif(digits = 3),\n    var = var(x) |&gt; signif(digits = 3)\n  ) |&gt; \n  rename(\n    \"Source\" = source,\n    \"Standard Deviation\" = sd,\n    \"Variance\" = var\n  ) |&gt;\n  kbl() |&gt; kable_styling()\n\n\n\n\n\nSource\nStandard Deviation\nVariance\n\n\n\n\nunscaled\n1\n1\n\n\n\n\n\nGood. But what happens now to this standard normal distribution if we multiply the random variable with some scaling parameter \\(SD = 0.23\\)? Let’s find out:\n\n\nCode\n# set a seed\nset.seed(123)\n\n# multiply with W\nSD &lt;- 0.23\nx_scaled &lt;- x * SD\nstd_norm_scaled &lt;- tibble(x = x_scaled, source = \"scaled\")\n\n# combine both\nstd_norm_combined &lt;- bind_rows(std_norm, std_norm_scaled)\n\n# plot\nstd_norm_combined |&gt; \n  ggplot(aes(x = x, fill = source)) +\n  geom_density(alpha = 0.2)+\n  labs(title = \"Normal distributions: Impact of scaling factor SD\", x = \"\", y = \"Density\")+\n  scale_fill_manual(\n    \"Source\",\n    values = c(\n      \"unscaled\" = \"#003049\",  # Blue color for original\n      \"scaled\" = \"#c1121f\"     # Orange color for scaled\n    )\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nLet’s compare the standard deviation and variance of both distributions:\n\n\nCode\n# summarize data and calculate sd and variance\nstd_norm_combined |&gt; \n  group_by(source) |&gt;   \n  summarize(\n    sd = sd(x) |&gt; signif(digits = 2),\n    var = var(x) |&gt; signif(digits = 2)\n  ) |&gt; \n  rename(\n    \"Source\" = source,\n    \"Standard Deviation\" = sd,\n    \"Variance\" = var\n  ) |&gt; \n  kbl() |&gt; \n  kable_styling()\n\n\n\n\n\nSource\nStandard Deviation\nVariance\n\n\n\n\nscaled\n0.23\n0.053\n\n\nunscaled\n1.00\n1.000\n\n\n\n\n\nFor the scaled distribution, we can see that the resulting standard deviation \\(\\sigma\\) is approximately equal to our scaling factor SD_ADD (which is 0.23) and the variance is \\(0.23^2 \\approx 0.053\\). This means that in our model code\n\n\n\nalternative way (additive)\n\nSD_ADD * EPS(1)\n\n\nthe SD_ADD parameter (specified via $THETA) is representing a standard deviation. Cool thing! Probably it’s not too surprising given my naming scheme, but anyways.4 Overall, both of these models should be equivalent:\n\n\n\nclassical way (additive)\n\n$SIGMA\n0.0529   ; variance\n\n\nand\n\n\n\nalternative way (additive)\n\n$THETA\n0.23   ; standard deviation\n\n$SIGMA\n1 FIX\n\n\nTo sum it up: We need to be careful with the units. If we use the classical way, we are estimating a variance via $SIGMA, but if we use the alternative way, we are estimating a standard deviation via $THETA and fix the $SIGMA to a standard normal. Typically, we would report the standard deviation (rather than the variance) if we use an additive model, and I think one of the advantages of the alternative way is that we directly read out the standard deviation from the parameter estimates (without the need to transform anything). Some also say that the estimation becomes more stable if we model the stochastic parts via $THETA, but I cannot judge if this is true or not.\n\n\n\n\n\n\nTipSpecifying additive RUV via $THETA gives us a standard deviation\n\n\n\nWhenever we have an additive error model and we specify the RUV in the $THETA block (the alternative way), the resulting estimate is a standard deviation."
  },
  {
    "objectID": "posts/expressing_ruv_as_theta/index.html#proportional-error-models",
    "href": "posts/expressing_ruv_as_theta/index.html#proportional-error-models",
    "title": "Expressing RUV as $THETA in NONMEM",
    "section": "2 Proportional error models",
    "text": "2 Proportional error models\nNow, let’s look at proportional error models. The classical way of specifying the proportional error model looks like this:\n\n\n\nclassical way (proportional)\n\n$ERROR\nIPRED = F\nY = IPRED + IPRED * EPS(1)\n\n$SIGMA\n0.0225\n\n\nAnd the alternative way is:\n\n\n\nalternative way (proportional)\n\n$THETA\n0.15        ; RUV_PROP\n\n$ERROR\nIPRED = F\nSD_PROP = IPRED * THETA(1)\nY = IPRED + SD_PROP * EPS(1)\n\n$SIGMA\n1 FIX\n\n\nThe structure is similar to the additive model we discussed earlier, except that the standard deviation of the random noise around our prediction depends on the prediction itself. This is why we first calculate the standard deviation SD_PROP at the given prediction as:\n\n\n\nalternative way (proportional)\n\nSD_PROP = IPRED * THETA(1)\n\n\nThis already gives us an understanding of the units of THETA(1): it represents the coefficient of variation (CV) of the prediction IPRED. Why? A coefficient of variation represents the ratio of the standard deviation to the mean. This is why we end up with a standard deviation (SD_PROP) if we multiply the prediction (IPRED) with the CV (THETA(1)). So we always have a fraction of the prediction representing our standard deviation at that point.\n\n2.1 An example\nSuppose we have a prediction (IPRED) of 10 mg/L and we want to show the resulting distribution. For the classical approach, we would specify a variance (EPS(1)) of 0.0225, and for the alternative way, we would specify a CV (THETA(1)) of 0.15. What do you think? Will this be equivalent or not? Let’s find out!\n\n\nCode\n# Set seed for reproducibility\nset.seed(123)\n\n# Parameters\nIPRED &lt;- 10         \nCV_percent &lt;- 0.15    \nSD_prop &lt;- CV_percent * IPRED  \nsd_classical &lt;- IPRED * sqrt(0.0225)  \n\n# Number of samples\nn &lt;- 100000\n\n# Classical way: Specify variance directly\neps_classical &lt;- rnorm(n, mean = 10, sd = sd_classical)  \n\n# Alternative way: Specify CV%\neps_alternative &lt;- rnorm(n, mean = 10, sd = 1 * SD_prop) \n\n# Create a tibble combining both distributions\nprop_models &lt;- tibble(\n  value = c(eps_classical, eps_alternative),\n  source = rep(c(\"Classical (Variance = 0.0225)\", \"Alternative (CV = 0.15)\"), each = n)\n)\n\n# Plot the density of both distributions\nprop_models |&gt; \n  ggplot(aes(x = value, fill = source)) +\n  geom_density(alpha = 0.4) +\n  labs(\n    title = \"Classical vs. alternative specification\",\n    x = \"Concentration [mg/L]\",\n    y = \"Density\"\n  ) +\n  scale_fill_manual(\n    \"Model Specification\",\n    values = c(\n      \"Classical (Variance = 0.0225)\" = \"#003049\",  # Blue\n      \"Alternative (CV = 0.15)\" = \"#c1121f\"      # Red\n    )\n  ) +\n  scale_x_continuous(breaks=c(4,6,8,10,12,14,16))+\n  theme_bw()\n\n\n\n\n\n\n\n\n\nBoth models end up with the same distribution. In the classical way, we are given a variance of 0.0225. To get the standard deviation, we take the square root of the variance:\n\\[\n\\sigma_{EPS} = \\sqrt{0.0225} = 0.15\n\\] This means, that our random variable EPS(1) has a standard deviation of 0.15 mg/L in our classical model:\n\n\n\nclassical way (proportional)\n\nY = IPRED + IPRED * EPS(1)\n\n\nBy multiplying this EPS(1) by the prediction (IPRED) of 10 mg/L, we are scaling this random variable to have the (desired) standard deviation of the prediction distribution (PRED):\n\\[\n\\sigma_{Y} = 0.15 \\times 10 = 1.5 \\, \\text{mg/L}\n\\]\nIn the alternative way, we are directly estimating the coefficient of variation (CV) as 0.15.\n\n\n\nalternative way (proportional)\n\nSD_PROP = IPRED * THETA(1)\nY = IPRED + SD_PROP * EPS(1)\n\n\nWe are first calculating the respective standard deviation (SD_PROP) by multiplying CV with IPRED. We then turn this standard deviation into a random variable with this standard deviation by multiplying it with a random variable from a standard normal (EPS(1)). Also here, the respective standard deviation of the prediction distribution (PRED) is 1.5 mg/L:\n\\[\n\\sigma_{Y} = 0.15 \\times 10 = 1.5 \\, \\text{mg/L}\n\\]\nIn both cases, the resulting variability is the same, meaning both approaches lead to the same standard deviation of 1.5 mg/L. Again, it is a bit more convenient to specify the CV directly, as it is more intuitive and easier to interpret. And if the stability argument is true (see above), we would also make our estimation more robust this way.\n\n\n\n\n\n\nTipSpecifying proportional RUV in $THETA gives us a coefficient of variation\n\n\n\nWhenever we have a proportional error model and we specify the RUV in the $THETA block, the resulting estimate is a coefficient of variation."
  },
  {
    "objectID": "posts/expressing_ruv_as_theta/index.html#combined-proportional-and-additive-error-models",
    "href": "posts/expressing_ruv_as_theta/index.html#combined-proportional-and-additive-error-models",
    "title": "Expressing RUV as $THETA in NONMEM",
    "section": "3 Combined proportional and additive error models",
    "text": "3 Combined proportional and additive error models\nFinally, let’s combine our knowledge to understand the alternative way of specifying a combined proportional and additive error model:\n\n\n\nalternative way (combined)\n\n$THETA\n0.15        ; RUV_PROP\n0.23        ; RUV_ADD\n\n$ERROR\nIPRED = F\nSD_PROP = THETA(1)*IPRED\nSD_ADD = THETA(2)\nSD = SQRT(SD_PROP**2 + SD_ADD**2)\nY = IPRED + SD * EPS(1)\n\n$SIGMA\n1 FIX\n\n\nTwo parts should already be familiar:\n\n\n\nalternative way (combined)\n\nSD_PROP = THETA(1)*IPRED\nSD_ADD = THETA(2)\n\n\nIn the first part, we calculate SD_PROP, representing the resulting standard deviation of the proportional part (as THETA(1) is a CV). The second part, SD_ADD, gives us the standard deviation of the additive part. Now we want to find the joint standard deviation SD at the given concentration. But how do we combine these components?\n\n\n\nalternative way (combined)\n\nSD = SQRT(SD_PROP**2 + SD_ADD**2)\n\n\nWe can see that we first square both terms, then add them together, then take the square root. Sounds complicated - why not just add them directly together? This is because variances are additive when combining independent random variables, while standard deviations are not (Soch 2020). Written a bit more formally for two independent random variables (we typically assume the covariance to be 0 when modelling RUV):\n\\[\\mathrm{Var}(X + Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y)\\] In our case, SD_PROP and SD_ADD are standard deviations, so we must first square them to get the variances and then add them. However, we want to go back to a standard deviation before we multiply SD with EPS(1) (being fixed to 1). Therefore, we take the square root in the end.\nThis operation has always confused me a bit, but once I understood that I can sum up variances, but not standard deviations 5 it made more sense to me.\n\n\n\n\n\n\nTipCombined error models\n\n\n\nWhen specifying a combined error model, the estimates in the $THETA block represent a standard deviation for the additive part and a coefficient of variation for the proportional part."
  },
  {
    "objectID": "posts/expressing_ruv_as_theta/index.html#conclusion",
    "href": "posts/expressing_ruv_as_theta/index.html#conclusion",
    "title": "Expressing RUV as $THETA in NONMEM",
    "section": "4 Conclusion",
    "text": "4 Conclusion\nThis is a somewhat lengthy explanation of why and how we code the alternative approach in NONMEM. Personally, I wasn’t very familiar with how distributions behave when its random variable is being multiplying by a factor, and I didn’t realize that while variances are additive when combining two random processes, standard deviations are not. If you have a stronger background in statistics, this might have been obvious, but I hope this explanation was still helpful for some others."
  },
  {
    "objectID": "posts/expressing_ruv_as_theta/index.html#footnotes",
    "href": "posts/expressing_ruv_as_theta/index.html#footnotes",
    "title": "Expressing RUV as $THETA in NONMEM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYeah, I know, not really fancy. But that’s how it feels when you touch a combined error model for the first time.↩︎\nFor many of you, this is likely quite standard. The naming reflects my perspective.↩︎\nI’m not sure if this was initially introduced by one of the Uppsala groups or if this is just some hearsay.↩︎\nSome people also code it with W instead of SD but it’s always a good idea to find descriptive variable names.↩︎\nProbably something you would tackle in the first semester of your statistics studies. But not if you study pharmacy ;)↩︎"
  },
  {
    "objectID": "publications/abstracts_docs/03-03-2025_tld_1_final_results_sakk6516.html",
    "href": "publications/abstracts_docs/03-03-2025_tld_1_final_results_sakk6516.html",
    "title": "TLD-1 – A novel liposomal doxorubicin formulation, in patients with advanced solid tumors: Final results of a multicenter, open-label, cross-over phase I study (SAKK 65/16)",
    "section": "",
    "text": "https://oncologypro.esmo.org/meeting-resources/esmo-targeted-anticancer-therapies-congress-2025/tld-1-a-novel-liposomal-doxorubicin-formulation-in-patients-with-advanced-solid-tumors-final-results-of-a-multicenter-open-label-cross-over-p"
  },
  {
    "objectID": "publications/abstracts_docs/03-03-2025_tld_1_final_results_sakk6516.html#url",
    "href": "publications/abstracts_docs/03-03-2025_tld_1_final_results_sakk6516.html#url",
    "title": "TLD-1 – A novel liposomal doxorubicin formulation, in patients with advanced solid tumors: Final results of a multicenter, open-label, cross-over phase I study (SAKK 65/16)",
    "section": "",
    "text": "https://oncologypro.esmo.org/meeting-resources/esmo-targeted-anticancer-therapies-congress-2025/tld-1-a-novel-liposomal-doxorubicin-formulation-in-patients-with-advanced-solid-tumors-final-results-of-a-multicenter-open-label-cross-over-p"
  },
  {
    "objectID": "publications/abstracts_docs/03-03-2025_tld_1_final_results_sakk6516.html#abstract",
    "href": "publications/abstracts_docs/03-03-2025_tld_1_final_results_sakk6516.html#abstract",
    "title": "TLD-1 – A novel liposomal doxorubicin formulation, in patients with advanced solid tumors: Final results of a multicenter, open-label, cross-over phase I study (SAKK 65/16)",
    "section": "Abstract",
    "text": "Abstract\n\nBackground\nTLD-1 is a novel liposomal doxorubicin formulation with a small hydrodynamic liposome diameter of 60 nm (90 nm for Caelyx) and a high lipid-to-drug ratio. We present the final results of SAKK 65/16, a phase 1 first-in-human study to define TLD-1’s safety, recommended phase 2 dose (RP2D), preliminary activity and pharmacokinetics (PK) in patients with advanced solid tumors.\n\n\nMethods\nWe included patients with advanced or recurrent solid tumors who failed standard treatment. TLD-1 was administered intravenously every 3 weeks up to a maximum of 9 cycles (6 cycles for anthracycline-pretreated patients) starting at 10 mg/m2, using accelerated dose escalation and a modified continual reassessment design. A subsequent comparative PK part with a randomized cross-over design used TLD-1 at the RP2D and assigned patients to a single cycle of Caelyx (first or second cycle), comparing the PK of TLD-1 and Caelyx.\n\n\nResults\n43 patients were enrolled between November 2018 and March 2023, incuding 30 patients in the dose-escalation part and 13 patients in the comparative PK part. Predominant tumor entities included breast cancer in 23 patients and gynecological cancers in 12 patients. TLD-1 RP2D was defined as 40 mg/m2 administered every 3 weeks. No treatment-related adverse events (TRAE) G4 or G5 were reported. TRAE G3 included cumulative hand-foot syndrome in 5 (12%) patients, stomatitis in 3 (7%) patients and anemia in 2 (5%) patients. Partial tumor response was documented in 4 (9%) patients. In the comparative PK part, median terminal half-life of encapsulated doxorubicin was 118 hours (TLD-1) and 70 hours (Caelyx), respectively, while median terminal half-life was 122 hours (TLD-1) and 81 hours (Caelyx) for unencapsulated doxorubicin, driven by the rate-limiting release from the liposomes.\n\n\nConclusions\nTLD-1 at 40 mg/m2 every 3 weeks was safe and showed preliminary activity in patients with heavily pretreated solid malignancies. TLD-1 exhibits a lower clearance and prolonged plasma half-life compared to conventional liposomal doxorubicin formulations such as Caelyx, potentially resulting from decreased degradation by the mononuclear phagocyte system.\n\n\nClinical trial identification\nNCT03387917.\n\n\nEditorial acknowledgement\nLegal entity responsible for the study Swiss Group for Clinical Cancer Research (SAKK).\n\n\nFunding\nInnomedica.\n\n\nDisclosure\nM. Joerger: Financial Interests, Institutional, Invited Speaker, Clinical study activity: Basilea, Bayer, BMS, Immunophotonics, Innomedica, MSD, Novartis, Roche; Financial Interests, Institutional, Other, Clinical study activity: DaiichySankyo; Financial Interests, Institutional, Invited Speaker: Anaveon; Non-Financial Interests, Personal, Advisory Role: Novartis, AstraZeneca, Basilea, Bayer, BMS, Debiopharm, MSD, Roche, Sanofi. I. Colombo: Financial Interests, Institutional, Expert Testimony: GSK, AstraZeneca, AbbVie; Financial Interests, Institutional, Advisory Board: GSK, MSD, AstraZeneca, Incyte; Financial Interests, Personal, Other, Travel grants: GSK; Financial Interests, Personal, Invited Speaker: GSK, MSD, Bayer, Vivesto, Incyte, AstraZeneca, Orion Pharma, Tolremo; Financial Interests, Personal, Other, Health Care Professional Consultancy: BionTech; Financial Interests, Personal, Full or part-time Employment, My husband is an employee of this biomedical company since 02.2022. His role is ‘Marketing Manager’ for the endoscopy division and he is in charge of the launchig program in the field of artificial intelligence applied to colonoscpy. I confirm that his job does not have any conflict of interest with my job and my role at ESMO: Medtronic; Financial Interests, Personal, Stocks/Shares: Medtronic; Non-Financial Interests, Personal, Leadership Role: Swiss Group for Clinical Cancer Research (SAKK); Non-Financial Interests, Personal, Advisory Role: European School of Oncology (ESO). S. Halbherr: Financial Interests, Personal, Stocks/Shares: Innomedica; Other, Personal, Full or part-time Employment: Innomedica. C. Sessa: Financial Interests, Personal, Other, ESO Consultant for gynaecological cancer: ESO (European School of Oncology); Financial Interests, Personal, Other, DMC member of the MK-3475-C93 studyDMC member of the MK-2870-005 study: Merck; Non-Financial Interests, Personal, Advisory Role, ESMO extended member women for oncology: ESMO; Non-Financial Interests, Personal, Advisory Role, Member of the compliance committee: ESMO. A. Mc Laughlin: Other, Personal, Full or part-time Employment: Pharmetheus AB. A. Stathis: Financial Interests, Institutional, Expert Testimony: Bayer, Eli Lilly; Financial Interests, Institutional, Advisory Board: Janssen, Roche, Incyte, Beigene; Financial Interests, Institutional, Other, Travel grant: AstraZeneca, Incyte; Financial Interests, Institutional, Other, Consultancy: Debiopharm, AstraZeneca, MSD; Financial Interests, Institutional, Invited Speaker: Janseen, Pfizer, Merck MSD, Roche, Novartis, ADC Therapeutics, AbbVie, Bayer, Philogen, Cellestia, AstraZeneca, Incyte, Amgen, Loxo Oncology, Debiopharm, BMS. All other authors have declared no conflicts of interest."
  },
  {
    "objectID": "publications/abstracts_docs/06-26-2024_ml_driven_flattening_model_priors.html",
    "href": "publications/abstracts_docs/06-26-2024_ml_driven_flattening_model_priors.html",
    "title": "Machine learning-driven flattening of model priors: A comparative simulation study across multiple compounds",
    "section": "",
    "text": "https://www.page-meeting.org/default.asp?abstract=10858"
  },
  {
    "objectID": "publications/abstracts_docs/06-26-2024_ml_driven_flattening_model_priors.html#url",
    "href": "publications/abstracts_docs/06-26-2024_ml_driven_flattening_model_priors.html#url",
    "title": "Machine learning-driven flattening of model priors: A comparative simulation study across multiple compounds",
    "section": "",
    "text": "https://www.page-meeting.org/default.asp?abstract=10858"
  },
  {
    "objectID": "publications/abstracts_docs/06-26-2024_ml_driven_flattening_model_priors.html#abstract",
    "href": "publications/abstracts_docs/06-26-2024_ml_driven_flattening_model_priors.html#abstract",
    "title": "Machine learning-driven flattening of model priors: A comparative simulation study across multiple compounds",
    "section": "Abstract",
    "text": "Abstract\n\nIntroduction\nModel-informed precision dosing (MIPD) supports clinical decision-making by leveraging mathematical models and individual drug or biomarker measurements [1]. A typical MIPD workflow involves estimating individual maximum a posteriori (MAP) values for parameters which include interindividual variability (IIV). MAP values represent the mode of the posterior distribution and are obtained by maximising the product of the likelihood of the data and the parameter’s prior distribution [2]. The selected prior distribution needs to properly reflect the uncertainty about the parameter and is often defined by a distribution around the covariate-adjusted typical parameter value with a variance given by the model’s IIV estimate. However, this representation of uncertainty is only accurate when the model closely represents the clinical population (CP). In reality, deviations between model and CP are expected [3]. This deviation gave rise to the method of machine learning (ML)-assisted flattening of model priors [4], where the variance of the prior is selectively increased to account for these deviations. While this method has shown considerable improvement in predictive performance for vancomycin [4], its potential benefit across other drugs has not been studied.\n\n\nObjectives\nTo assess the impact of ML-driven flattening of priors on the predictive performance for five compounds which are commonly applied in Therapeutic Drug Monitoring (TDM) programs by conducting a simulation study.\n\n\nMethods\nFive compounds were investigated: vancomycin (VAN), meropenem (MEM), methotrexate (MTX), infliximab (IFX), and tacrolimus (TAC). To have a plausible representation of the differences between model and CP to which the model is applied, two published models per drug [5-14] were selected and split into a data-generating model (DM) and an applied model (AM) used for Bayesian forecasting.\nA plausible TDM dataset was simulated with the DM (n=5000) using mrgsolve [15]. The PK-Sim® [16] population builder was used to generate demographically realistic virtual patients. All other covariates were drawn from lognormal distributions based on reported point estimates. Two administrations, each followed by a single drug concentration measurement, were simulated for each individual. RUV was included to account for the expected noise in clinical readouts.\nMAP estimation using the AM was performed with prior weights λ=1 and λ=1/8 for standard priors (SP) and flattened priors (FP), respectively. While the first concentration was included for MAP estimation, the second was used as a reference to evaluate the predictive performance. For each patient and λ, the residual between prediction and withheld datapoint was calculated.\nThe labeled dataset was then split into a training/test cohort (75%/25%) to build a ML model which predicts whether FP or SP should be applied for a given individual. All ML steps were conducted using the tidymodels [17] framework in R. The XGBoost (XGB) algorithm [18] was employed, using a 5-fold cross-validation with 5 repeats and a grid search for hyperparameter tuning. Optimal hyperparameters were selected based on precision.\nTo assess the impact of the method on the predictive performance of the model, the test dataset was bootstrapped (n=1000). For each sample, RMSE and MPE along with their relative improvement compared to SP were calculated across patients.\n\n\nResults\nResults from the simulation study, presented as the median relative improvement (-) or deterioration (+) compared to SP, were:\n\nVAN\n\nRMSE: -16.0% (-11 to -22% [4])\nMPE: -40.8% (-42 to -74% [4]) -MEM\nRMSE: -0.12%\nMPE: -2.89%\n\nMTX\n\nRMSE: +1.06%\nMPE: -4.15%\n\nIFX\n\nRMSE: -11.8%\nMPE: +1.69%\n\nTAC\n\nRMSE: -32.6%\nMPE: +18.0%\n\n\nOverall, the simulated impact ranged from -40.8% to +1.06% (RMSE) and -16.0% to +18.0% (MPE). Precision for the test dataset was 72.1% (VAN), 25.8% (MEM), 32.8% (MTX), 37.9% (IFX), and 71.3% (TAC).\n\n\nConclusion\nThe predicted improvements in RMSE and MPE for VAN are in agreement with the reported values. However, the impact of ML driven-flattening of priors on the predictive performance for other drugs varied substantially: The largest reduction in RMSE/MPE was indeed found for VAN with heterogenous improvements and even deteriorations for the other compounds. Missing impact of modified priors was also previously reported [19]. Moving forward, we plan to extend the investigation to identify variables which potentially explain the observed differences."
  },
  {
    "objectID": "publications/abstracts_docs/10-30-2022_oxycodone_pbpk_ddi_gdi.html",
    "href": "publications/abstracts_docs/10-30-2022_oxycodone_pbpk_ddi_gdi.html",
    "title": "The Impact of UGT2B7 and CYP2D6 Gene-Drug- and CYP-mediated Drug-Drug-Interactions on Oxycodone and Oxymorphone Pharmacokinetics using PBPK Modelling.",
    "section": "",
    "text": "https://www.go-acop.org/?abstract=413"
  },
  {
    "objectID": "publications/abstracts_docs/10-30-2022_oxycodone_pbpk_ddi_gdi.html#url",
    "href": "publications/abstracts_docs/10-30-2022_oxycodone_pbpk_ddi_gdi.html#url",
    "title": "The Impact of UGT2B7 and CYP2D6 Gene-Drug- and CYP-mediated Drug-Drug-Interactions on Oxycodone and Oxymorphone Pharmacokinetics using PBPK Modelling.",
    "section": "",
    "text": "https://www.go-acop.org/?abstract=413"
  },
  {
    "objectID": "publications/abstracts_docs/10-30-2022_oxycodone_pbpk_ddi_gdi.html#abstract",
    "href": "publications/abstracts_docs/10-30-2022_oxycodone_pbpk_ddi_gdi.html#abstract",
    "title": "The Impact of UGT2B7 and CYP2D6 Gene-Drug- and CYP-mediated Drug-Drug-Interactions on Oxycodone and Oxymorphone Pharmacokinetics using PBPK Modelling.",
    "section": "Abstract",
    "text": "Abstract\n\nObjectives\nOxycodone, a widely used opioid to treat moderate to severe pain, is subject of pharmacogenomic debate. After CYP2D6 mediated formation of oxymorphone (60 times higher μ-opioid receptor affinity than the parent [1]) it undergoes conjugation via UGT2B7. Two polymorphic enzymes are involved in this path, which may lead to high variability in oxymorphone exposure and response. In addition, 45% of the dose is metabolized to noroxycodone via CYP3A4. A recent publication by the Clinical Pharmacogenetics Implementation Consortium stated that there is ‘insufficient evidence’ to guide clinical practice at the moment [2]. Hence, the objective of our work was to investigate the impact of CYP2D6 and UGT2B7 gene-drug-interactions (GDI) and CYP-mediated drug-drug-interactions (DDI) on the PK of oxycodone using PBPK modelling.\n\n\nMethods\nGastroPlus 9.8.2 was used to build a PBPK model in a bottom-up approach. Drug-specific parameters were derived from reported in vitro or in vivo data; otherwise, in silico predictions were used. Vmax values were optimized to match reported mass balance and urinary excretion data. For the wild type UGT2B7H and the mutated allele UGT2B7Y, Vmax and Km values from literature [3] were weighted by the prevalence of the genotype. The verified PBPK models were applied to interrogate the impact of combined GDI and DDI on oxycodone and its metabolites exposures. The impact of such interactions was assessed by comparing drug exposure (AUC) in each simulated scenario versus oxycodone and metabolites AUCs in a CYP2D6 and UGT2B7 normal metabolizer without comedication.\n\n\nResults\n97% of predicted AUC values for oxycodone and its metabolites were within a 1.5-fold error range. In clinical DDI studies 95% of changes in AUC metabolite-to-parent-ratios were predicted within a 2.0-fold error range. Simulation results indicate that CYP2D6 ultrarapid metabolizers (UM) / UGT2B7 poor metabolizers (PM) have a 2.1-fold higher AUC for oxymorphone compared with normal metabolizers (NM). In CYP2D6 PM / UGT2B7 NM the AUC for oxymorphone was decreased by 3.1-fold and the AUC for oxycodone was increased by 1.2-fold. The highest exposure was predicted for CYP2D6 UM / UGT2B7 PM receiving a single dose of 400 mg ketoconazole. A 2.6-fold AUC increase for oxycodone and a 4.3-fold AUC increase for oxymorphone was predicted. The lowest exposure is expected for CYP2D6 PM / UGT2B7 NM receiving 600 mg rifampicin QD. The AUC for oxycodone decreased 5.8-fold and the AUC of oxymorphone decreased 21.1-fold.\n\n\nConclusions\nSo far, variability in oxycodone PKPD was attempted to be explained by CYP2D6 GDIs or DDIs alone. However, our research shows that not only the formation of oxymorphone but also the metabolism of oxymorphone via UGT2B7 should be considered. Some groups – especially in combined GDI and DDI scenarios – are expected to have significant over- or underexposure. For example, a CYP2D6 UM and UGT2B7 PM has high formation of oxymorphone and low clearance, which may lead to accumulation. These effects are amplified in DDI settings and should be considered in practice.\n\n\nCitations\n[1] Volpe et al. (2011) Regulatory Toxicology and Pharmacology (59)\n[2] Crews et al. (2021) Clinical Pharmacology & Therapeutics (110)\n[3] Coffman et.al. (1998) Drug Metab Dispos (26)"
  },
  {
    "objectID": "publications/publications.html",
    "href": "publications/publications.html",
    "title": "Publications",
    "section": "",
    "text": "Clinical trial simulation\n\nRShiny\n\nDuchenne muscular dystrophy\n\nMiddle author\n\n\n\n\n\n\nOct 3, 2024\n\n\nKim J, Morales JF, Kang S, Klose M, Willcocks RJ, Daniels MJ, Belfiore-Oshan R, Walter GA, Rooney WD, Vandenborne K, Kim S\n\n\nCPT: Pharmacometrics & Systems Pharmacology\n\n\n\n\n\n\n\n\n\nClinical trial simulation\n\nRShiny\n\nType 1 diabetes\n\nSecond author\n\n\n\n\n\n\nJul 3, 2024\n\n\nMorales JF, Klose M, Hoffert Y, Podichetty JT, Burton J, Schmidt S, Romero K, O’Doherty I, Martin F, Campbell-Thompson M, Haller MJ, Atkinson MA, Kim S.\n\n\nCPT: Pharmacometrics & Systems Pharmacology\n\n\n\n\n\n\n\n\n\nDoxorubicin\n\nTLD-1\n\nPEGylated liposomal doxorubicin\n\nNLME PK\n\nMiddle author\n\n\n\n\n\n\nJun 15, 2024\n\n\nMc Laughlin AM, Hess D, Michelet R, Colombo I, Haefliger S, Bastian S, Rabaglio M, Schwitter M, Fischer S, Eckhardt K, Hayoz S, Kopp C, Klose M, Sessa C, Stathis A, Halbherr S, Huisinga W, Joerger M, Kloft C.\n\n\nCancer Chemotherapy and Pharmacology\n\n\n\n\n\n\n\n\n\nPBPK\n\nOxycodone\n\nGDI\n\nDDI\n\nUGT2B7\n\nCYP2D6\n\nFirst author\n\n\n\n\n\n\nMar 1, 2024\n\n\nKlose M, Cristofoletti R, Silva CM, Mangal N, Turgeon J, Michaud V, Lesko LJ, Schmidt S.\n\n\nEuropean Journal of Pharmaceutical Sciences\n\n\n\n\n\n\n\n\n\nTLD-1\n\nPEGylated liposomal doxorubicin\n\nDoxorubicin\n\nSafety\n\nEfficacy\n\nNCA\n\nMiddle author\n\n\n\n\n\n\nFeb 2, 2024\n\n\nColombo I, Koster KL, Holer L, Haefliger S, Rabaglio M, Bastian S, Schwitter M, Eckhardt K, Hayoz S, Mc Laughlin AM, Kloft C, Klose M, Halbherr S, Baumgartner C, Sessa C, Stathis A, Hess D, Joerger M.\n\n\nEuropean Journal of Cancer\n\n\n\n\n\n\n\n\n\nMeropenem\n\nAntiinfectives\n\nNLME PK\n\nSecond author\n\n\n\n\n\n\nApr 20, 2021\n\n\nLiebchen U, Klose M, Paal M, Vogeser M, Zoller M, Schroeder I, Schmitt L, Huisinga W, Michelet R, Zander J, Scharf C, Weinelt FA, Kloft C.\n\n\nAntibiotics\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/publications.html#peer-reviewed-publications",
    "href": "publications/publications.html#peer-reviewed-publications",
    "title": "Publications",
    "section": "",
    "text": "Clinical trial simulation\n\nRShiny\n\nDuchenne muscular dystrophy\n\nMiddle author\n\n\n\n\n\n\nOct 3, 2024\n\n\nKim J, Morales JF, Kang S, Klose M, Willcocks RJ, Daniels MJ, Belfiore-Oshan R, Walter GA, Rooney WD, Vandenborne K, Kim S\n\n\nCPT: Pharmacometrics & Systems Pharmacology\n\n\n\n\n\n\n\n\n\nClinical trial simulation\n\nRShiny\n\nType 1 diabetes\n\nSecond author\n\n\n\n\n\n\nJul 3, 2024\n\n\nMorales JF, Klose M, Hoffert Y, Podichetty JT, Burton J, Schmidt S, Romero K, O’Doherty I, Martin F, Campbell-Thompson M, Haller MJ, Atkinson MA, Kim S.\n\n\nCPT: Pharmacometrics & Systems Pharmacology\n\n\n\n\n\n\n\n\n\nDoxorubicin\n\nTLD-1\n\nPEGylated liposomal doxorubicin\n\nNLME PK\n\nMiddle author\n\n\n\n\n\n\nJun 15, 2024\n\n\nMc Laughlin AM, Hess D, Michelet R, Colombo I, Haefliger S, Bastian S, Rabaglio M, Schwitter M, Fischer S, Eckhardt K, Hayoz S, Kopp C, Klose M, Sessa C, Stathis A, Halbherr S, Huisinga W, Joerger M, Kloft C.\n\n\nCancer Chemotherapy and Pharmacology\n\n\n\n\n\n\n\n\n\nPBPK\n\nOxycodone\n\nGDI\n\nDDI\n\nUGT2B7\n\nCYP2D6\n\nFirst author\n\n\n\n\n\n\nMar 1, 2024\n\n\nKlose M, Cristofoletti R, Silva CM, Mangal N, Turgeon J, Michaud V, Lesko LJ, Schmidt S.\n\n\nEuropean Journal of Pharmaceutical Sciences\n\n\n\n\n\n\n\n\n\nTLD-1\n\nPEGylated liposomal doxorubicin\n\nDoxorubicin\n\nSafety\n\nEfficacy\n\nNCA\n\nMiddle author\n\n\n\n\n\n\nFeb 2, 2024\n\n\nColombo I, Koster KL, Holer L, Haefliger S, Rabaglio M, Bastian S, Schwitter M, Eckhardt K, Hayoz S, Mc Laughlin AM, Kloft C, Klose M, Halbherr S, Baumgartner C, Sessa C, Stathis A, Hess D, Joerger M.\n\n\nEuropean Journal of Cancer\n\n\n\n\n\n\n\n\n\nMeropenem\n\nAntiinfectives\n\nNLME PK\n\nSecond author\n\n\n\n\n\n\nApr 20, 2021\n\n\nLiebchen U, Klose M, Paal M, Vogeser M, Zoller M, Schroeder I, Schmitt L, Huisinga W, Michelet R, Zander J, Scharf C, Weinelt FA, Kloft C.\n\n\nAntibiotics\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/publications.html#abstractsposters",
    "href": "publications/publications.html#abstractsposters",
    "title": "Publications",
    "section": "Abstracts/Posters",
    "text": "Abstracts/Posters\n\n\n\n\n\nA Bayesian-NLME approach identifies patients at risk of delayed MTX elimination if informative TDM data is provided\n\n\n\nMethotrexate\n\nMTX\n\nBayes\n\nNLME PK\n\nFirst author\n\n\n\n\n\n\nJun 4, 2025\n\n\nM. Klose, D. Marschner, M. Knott, J. Wendler, J. Müller-Kühnle, L. Kovar, W. Huisinga, C. Nyhoegen, R. Michelet, A. M. Mc Laughlin, G. Illerhaus, C. Kloft\n\n\nPAGE 2025\n\n\n\n\n\n\n\nTLD-1 – A novel liposomal doxorubicin formulation, in patients with advanced solid tumors: Final results of a multicenter, open-label, cross-over phase I study (SAKK 65/16)\n\n\n\nDoxorubicin\n\nTLD-1\n\nPegylated Liposomal Doxorubicin\n\nSecond author\n\n\n\n\n\n\nMar 3, 2025\n\n\nM. Joerger, M. Klose, I. Colombo, K. Koster, K. Gobat, S. Haefliger, M. Rabaglio, S. Bastian, M. Schwitter, K. Eckhardt, S. Hayoz, S. Halbherr, C. Sessa, R. Michelet, A. Mc Laughlin, D. Hess, C. Kloft, A. Stathis\n\n\nESMO TAT 2025\n\n\n\n\n\n\n\nMachine learning-driven flattening of model priors: A comparative simulation study across multiple compounds\n\n\n\nMachine learning\n\nBayes\n\nPrior Modifications\n\nSimulation study\n\nFirst author\n\n\n\n\n\n\nJun 26, 2024\n\n\nM. Klose, F. Thoma, L. Kovar, W. Huisinga, R. Michelet, C. Kloft\n\n\nPAGE 2024\n\n\n\n\n\n\n\nThe Impact of UGT2B7 and CYP2D6 Gene-Drug- and CYP-mediated Drug-Drug-Interactions on Oxycodone and Oxymorphone Pharmacokinetics using PBPK Modelling.\n\n\n\nPBPK\n\nOxycodone\n\nGDI\n\nDDI\n\nUGT2B7\n\nCYP2D6\n\nFirst author\n\n\n\n\n\n\nOct 30, 2022\n\n\nKlose M, Schmidt S, Cristofoletti R.\n\n\nACoP13\n\n\n\n\n\n\n\nUsing microdose-based activity measurement to individualise dosing of cytochrome P450 metabolised drugs: a case study with yohimbine and tamoxifen\n\n\n\nYohimbine\n\nTamoxifen\n\nCYP2D6\n\nMicrodosing\n\nMiddle author\n\n\n\n\n\n\nSep 2, 2021\n\n\nR. Michelet, F. Weinelt, M. Klose, A. M. Mc Laughlin, F. Kluwe, C. Montefusco-Pereira, M. Van Dyk, M. Vay, W. Huisinga, C. Kloft & G. Mikus\n\n\nPAGE 2021\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/publications.html#presentations",
    "href": "publications/publications.html#presentations",
    "title": "Publications",
    "section": "Presentations",
    "text": "Presentations\n\n\n\n\n\nA Bayesian-NLME approach identifies patients at risk of delayed MTX elimination if informative TDM data is provided\n\n\n\nMethotrexate\n\nMTX\n\nBayes\n\nNLME PK\n\nFirst author\n\n\n\n\n\n\nMay 16, 2025\n\n\nM. Klose, D. Marschner, M. Knott, J. Wendler, J. Müller-Kühnle, L. Kovar, W. Huisinga, C. Nyhoegen, R. Michelet, A. M. Mc Laughlin, G. Illerhaus, C. Kloft\n\n\nPK/PD Expert Meeting 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/publications_docs/03-01-2024_oxycodone_pbpk_ddi_gdi.html",
    "href": "publications/publications_docs/03-01-2024_oxycodone_pbpk_ddi_gdi.html",
    "title": "Exploring the impact of CYP2D6 and UGT2B7 gene-drug interactions, and CYP-mediated DDI on oxycodone and oxymorphone pharmacokinetics using physiologically-based pharmacokinetic modeling and simulation",
    "section": "",
    "text": "Oxycodone is one of the most commonly used opioids to treat moderate to severe pain. It is metabolized mainly by CYP3A4 and CYP2D6, while only a small fraction of the dose is excreted unchanged into the urine. Oxymorphone, the metabolite primarily formed by CYP2D6, has a 40- to 60-fold higher mu-opioid receptor affinity than the parent compound. While CYP2D6-mediated gene-drug-interactions (GDIs) and drug-drug interactions (DDIs) are well-studied, they only account for a portion of the variability in oxycodone and oxymorphone exposure. The combined impact of CYP2D6-mediated GDIs and DDIs, CYP3A4-mediated DDIs, and UGT2B7 GDIs is not fully understood yet and hard to study in head-to-head clinical trials given the relatively large number of scenarios. Instead, we propose the use of a physiologically-based pharmacokinetic model that integrates available information on oxycodone’s metabolism to characterize and predict the impact of DDIs and GDIs on the exposure of oxycodone and its major, pharmacologically-active metabolite oxymorphone. To this end, we first developed and verified a PBPK model for oxycodone and its metabolites using published clinical data. The verified model was then applied to determine the dose-exposure relationship of oxycodone and oxymorphone stratified by CYP2D6 and UGT2B7 phenotypes respectively, and administered perpetrators of CYP-based drug interactions. Our simulations demonstrate that the combination of CYP2D6 UM and a UGT2B7Y (268) mutation may lead to a 2.3-fold increase in oxymorphone exposure compared to individuals who are phenotyped as CYP2D6 NM / UGT2B7 NM. The extent of oxymorphone exposure increases up to 3.2-fold in individuals concurrently taking CYP3A4 inhibitors, such as ketoconazole. Inhibition of the CYP3A4 pathway results in a relative increase in the partial metabolic clearance of oxycodone to oxymorphone. Oxymorphone is impacted to a higher extent by GDIs and DDIs than oxycodone. We predict oxymorphone exposure to be highest in CYP2D6 UMs/UGT2B7 PMs in the presence of ketoconazole (strong CYP3A4 index inhibitor) and lowest in CYP2D6 PMs/UGT2B7 NMs in the presence of rifampicin (strong CYP3A4 index inducer) covering a 55-fold exposure range."
  },
  {
    "objectID": "publications/publications_docs/03-01-2024_oxycodone_pbpk_ddi_gdi.html#abstract",
    "href": "publications/publications_docs/03-01-2024_oxycodone_pbpk_ddi_gdi.html#abstract",
    "title": "Exploring the impact of CYP2D6 and UGT2B7 gene-drug interactions, and CYP-mediated DDI on oxycodone and oxymorphone pharmacokinetics using physiologically-based pharmacokinetic modeling and simulation",
    "section": "",
    "text": "Oxycodone is one of the most commonly used opioids to treat moderate to severe pain. It is metabolized mainly by CYP3A4 and CYP2D6, while only a small fraction of the dose is excreted unchanged into the urine. Oxymorphone, the metabolite primarily formed by CYP2D6, has a 40- to 60-fold higher mu-opioid receptor affinity than the parent compound. While CYP2D6-mediated gene-drug-interactions (GDIs) and drug-drug interactions (DDIs) are well-studied, they only account for a portion of the variability in oxycodone and oxymorphone exposure. The combined impact of CYP2D6-mediated GDIs and DDIs, CYP3A4-mediated DDIs, and UGT2B7 GDIs is not fully understood yet and hard to study in head-to-head clinical trials given the relatively large number of scenarios. Instead, we propose the use of a physiologically-based pharmacokinetic model that integrates available information on oxycodone’s metabolism to characterize and predict the impact of DDIs and GDIs on the exposure of oxycodone and its major, pharmacologically-active metabolite oxymorphone. To this end, we first developed and verified a PBPK model for oxycodone and its metabolites using published clinical data. The verified model was then applied to determine the dose-exposure relationship of oxycodone and oxymorphone stratified by CYP2D6 and UGT2B7 phenotypes respectively, and administered perpetrators of CYP-based drug interactions. Our simulations demonstrate that the combination of CYP2D6 UM and a UGT2B7Y (268) mutation may lead to a 2.3-fold increase in oxymorphone exposure compared to individuals who are phenotyped as CYP2D6 NM / UGT2B7 NM. The extent of oxymorphone exposure increases up to 3.2-fold in individuals concurrently taking CYP3A4 inhibitors, such as ketoconazole. Inhibition of the CYP3A4 pathway results in a relative increase in the partial metabolic clearance of oxycodone to oxymorphone. Oxymorphone is impacted to a higher extent by GDIs and DDIs than oxycodone. We predict oxymorphone exposure to be highest in CYP2D6 UMs/UGT2B7 PMs in the presence of ketoconazole (strong CYP3A4 index inhibitor) and lowest in CYP2D6 PMs/UGT2B7 NMs in the presence of rifampicin (strong CYP3A4 index inducer) covering a 55-fold exposure range."
  },
  {
    "objectID": "publications/publications_docs/06-15-2024_nlme_pk_tld1.html",
    "href": "publications/publications_docs/06-15-2024_nlme_pk_tld1.html",
    "title": "Population pharmacokinetics of TLD-1, a novel liposomal doxorubicin, in a phase I trial",
    "section": "",
    "text": "TLD-1 is a novel pegylated liposomal doxorubicin (PLD) formulation aiming to optimise the PLD efficacy-toxicity ratio. We aimed to characterise TLD-1’s population pharmacokinetics using non-compartmental analysis and nonlinear mixed-effects modelling.\n\n\n\nThe PK of TLD-1 was analysed by performing a non-compartmental analysis of longitudinal doxorubicin plasma concentration measurements obtained from a clinical trial in 30 patients with advanced solid tumours across a 4.5-fold dose range. Furthermore, a joint parent-metabolite PK model of doxorubicinentrapped, doxorubicinfree, and metabolite doxorubicinol was developed. Interindividual and interoccasion variability around the typical PK parameters and potential covariates to explain parts of this variability were explored.\n\n\n\nMedians \\(\\pm\\) standard deviations of dose-normalised doxorubicinentrapped+free Cmax and AUC0−∞ were 0.342 \\(\\pm\\) 0.134 mg/L and 40.1 \\(\\pm\\) 18.9 mg·h/L, respectively. The median half-life (95 h) was 23.5 h longer than the half-life of currently marketed PLD. The novel joint parent-metabolite model comprised a one-compartment model with linear release (doxorubicinentrapped), a two-compartment model with linear elimination (doxorubicinfree), and a one-compartment model with linear elimination for doxorubicinol. Body surface area on the volumes of distribution for free doxorubicin was the only significant covariate.\n\n\n\nThe population PK of TLD-1, including its release and main metabolite, were successfully characterised using non-compartmental and compartmental analyses. Based on its long half-life, TLD-1 presents a promising candidate for further clinical development. The PK characteristics form the basis to investigate TLD-1 exposure-response (i.e., clinical efficacy) and exposure-toxicity relationships in the future. Once such relationships have been established, the developed population PK model can be further used in model-informed precision dosing strategies.\n\n\n\nClinicalTrials.gov–NCT03387917–January 2, 2018"
  },
  {
    "objectID": "publications/publications_docs/06-15-2024_nlme_pk_tld1.html#abstract",
    "href": "publications/publications_docs/06-15-2024_nlme_pk_tld1.html#abstract",
    "title": "Population pharmacokinetics of TLD-1, a novel liposomal doxorubicin, in a phase I trial",
    "section": "",
    "text": "TLD-1 is a novel pegylated liposomal doxorubicin (PLD) formulation aiming to optimise the PLD efficacy-toxicity ratio. We aimed to characterise TLD-1’s population pharmacokinetics using non-compartmental analysis and nonlinear mixed-effects modelling.\n\n\n\nThe PK of TLD-1 was analysed by performing a non-compartmental analysis of longitudinal doxorubicin plasma concentration measurements obtained from a clinical trial in 30 patients with advanced solid tumours across a 4.5-fold dose range. Furthermore, a joint parent-metabolite PK model of doxorubicinentrapped, doxorubicinfree, and metabolite doxorubicinol was developed. Interindividual and interoccasion variability around the typical PK parameters and potential covariates to explain parts of this variability were explored.\n\n\n\nMedians \\(\\pm\\) standard deviations of dose-normalised doxorubicinentrapped+free Cmax and AUC0−∞ were 0.342 \\(\\pm\\) 0.134 mg/L and 40.1 \\(\\pm\\) 18.9 mg·h/L, respectively. The median half-life (95 h) was 23.5 h longer than the half-life of currently marketed PLD. The novel joint parent-metabolite model comprised a one-compartment model with linear release (doxorubicinentrapped), a two-compartment model with linear elimination (doxorubicinfree), and a one-compartment model with linear elimination for doxorubicinol. Body surface area on the volumes of distribution for free doxorubicin was the only significant covariate.\n\n\n\nThe population PK of TLD-1, including its release and main metabolite, were successfully characterised using non-compartmental and compartmental analyses. Based on its long half-life, TLD-1 presents a promising candidate for further clinical development. The PK characteristics form the basis to investigate TLD-1 exposure-response (i.e., clinical efficacy) and exposure-toxicity relationships in the future. Once such relationships have been established, the developed population PK model can be further used in model-informed precision dosing strategies.\n\n\n\nClinicalTrials.gov–NCT03387917–January 2, 2018"
  },
  {
    "objectID": "publications/publications_docs/10-03-2024_cts_duchenne_muscular_dystrophy.html",
    "href": "publications/publications_docs/10-03-2024_cts_duchenne_muscular_dystrophy.html",
    "title": "A model-informed clinical trial simulation tool with a graphical user interface for Duchenne muscular dystrophy",
    "section": "",
    "text": "Quantitative model-based clinical trial simulation tools play a critical role in informing study designs through simulation before actual execution. These tools help drug developers explore various trial scenarios in silico to select a clinical trial design to detect therapeutic effects more efficiently, therefore reducing time, expense, and participants’ burden. To increase the usability of the tools, user-friendly and interactive platforms should be developed to navigate various simulation scenarios. However, developing such tools challenges researchers, requiring expertise in modeling and interface development. This tutorial aims to address this gap by guiding developers in creating tailored R Shiny apps, using an example of a model-based clinical trial simulation tool that we developed for Duchenne muscular dystrophy (DMD). In this tutorial, the structural framework, essential controllers, and visualization techniques for analysis are described, along with key code examples such as criteria selection and power calculation. A virtual population was created using a machine learning algorithm to enlarge the available sample size to simulate clinical trial scenarios in the presented tool. In addition, external validation of the simulated outputs was conducted using a placebo arm of a recently published DMD trial. This tutorial will be particularly useful for developing clinical trial simulation tools based on DMD progression models for other end points and biomarkers. The presented strategies can also be applied to other diseases."
  },
  {
    "objectID": "publications/publications_docs/10-03-2024_cts_duchenne_muscular_dystrophy.html#abstract",
    "href": "publications/publications_docs/10-03-2024_cts_duchenne_muscular_dystrophy.html#abstract",
    "title": "A model-informed clinical trial simulation tool with a graphical user interface for Duchenne muscular dystrophy",
    "section": "",
    "text": "Quantitative model-based clinical trial simulation tools play a critical role in informing study designs through simulation before actual execution. These tools help drug developers explore various trial scenarios in silico to select a clinical trial design to detect therapeutic effects more efficiently, therefore reducing time, expense, and participants’ burden. To increase the usability of the tools, user-friendly and interactive platforms should be developed to navigate various simulation scenarios. However, developing such tools challenges researchers, requiring expertise in modeling and interface development. This tutorial aims to address this gap by guiding developers in creating tailored R Shiny apps, using an example of a model-based clinical trial simulation tool that we developed for Duchenne muscular dystrophy (DMD). In this tutorial, the structural framework, essential controllers, and visualization techniques for analysis are described, along with key code examples such as criteria selection and power calculation. A virtual population was created using a machine learning algorithm to enlarge the available sample size to simulate clinical trial scenarios in the presented tool. In addition, external validation of the simulated outputs was conducted using a placebo arm of a recently published DMD trial. This tutorial will be particularly useful for developing clinical trial simulation tools based on DMD progression models for other end points and biomarkers. The presented strategies can also be applied to other diseases."
  }
]